{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9F7vFmK-VoR",
        "outputId": "55ed8860-fdd4-4a3c-a472-482e24c4c924"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/cedar/signatures\n"
          ]
        }
      ],
      "source": [
        "# Mounting google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd  /content/drive/'My Drive'/cedar/signatures/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "nZqiGMMl-hNx"
      },
      "outputs": [],
      "source": [
        "# Import statements\n",
        "from torch.optim import RMSprop, Adam\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "from torch import save\n",
        "from torch import load\n",
        "from torch.nn import Linear, Conv2d, MaxPool2d, LocalResponseNorm, Dropout\n",
        "from torch.nn.functional import relu\n",
        "from torch.nn import Module\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "from torch import Tensor\n",
        "import torchsummary\n",
        "import torch\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "from PIL import Image\n",
        "from PIL.ImageOps import invert\n",
        "import pickle\n",
        "from random import randrange"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zC8SZs9u-loC"
      },
      "outputs": [],
      "source": [
        "# Inverts an input image\n",
        "def invert_image_(path):\n",
        "    image_file = Image.open(path) \n",
        "    image_file = image_file.convert('L').resize([220, 155])\n",
        "    image_file = invert(image_file)\n",
        "    image_array = np.array(image_file)\n",
        "    for i in range(image_array.shape[0]):\n",
        "        for j in range(image_array.shape[1]):\n",
        "            if image_array[i][j] <= 50:\n",
        "                image_array[i][j] = 0\n",
        "            else:\n",
        "                image_array[i][j] = 255\n",
        "    return image_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_QTqpK1y-qhE"
      },
      "outputs": [],
      "source": [
        "base_path_org = 'signatures/full_org/original_%d_%d.png'\n",
        "base_path_forg = 'signatures/full_forg/forgeries_%d_%d.png'\n",
        "\n",
        "data = []\n",
        "n_samples_of_each_class = 13500\n",
        "\n",
        "for _ in range(n_samples_of_each_class):\n",
        "    anchor_person = randrange(1, 55)\n",
        "    pos_sign = randrange(1, 24)\n",
        "    neg_sign = randrange(1, 24)\n",
        "\n",
        "    positive = [base_path_org % (anchor_person, pos_sign), 1]\n",
        "    negative = [base_path_forg % (anchor_person, neg_sign), 0]\n",
        "\n",
        "    data.append(positive)\n",
        "    data.append(negative)\n",
        "\n",
        "train, test = train_test_split(data, test_size=0.20)\n",
        "with open('train_index_binary.pkl', 'wb') as train_index_file:\n",
        "    pickle.dump(train, train_index_file)\n",
        "\n",
        "with open('test_index_binary.pkl', 'wb') as test_index_file:\n",
        "    pickle.dump(test, test_index_file)\n",
        "\n",
        "\n",
        "class Train(Dataset):\n",
        "\n",
        "    def __init__(self):\n",
        "        with open('train_index_binary.pkl', 'rb') as train_index_file:\n",
        "            self.pairs = pickle.load(train_index_file)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        item = self.pairs[index]\n",
        "        X = Tensor(invert_image_(item[0])/255.0).view(1, 220, 155)\n",
        "        return [X, item[1]]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "\n",
        "class Test(Dataset):\n",
        "\n",
        "    def __init__(self):\n",
        "        with open('test_index_binary.pkl', 'rb') as test_index_file:\n",
        "            self.pairs = pickle.load(test_index_file)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        item = self.pairs[index]\n",
        "        X = Tensor(invert_image_(item[0])/255.0).view(1, 220, 155)\n",
        "        return [X, item[1]]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "HNxI5I3KAqgV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import math\n",
        "\n",
        "# Defining a custom model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "          \n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p = 0.5),\n",
        "            nn.Linear(133760, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p = 0.5),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p = 0.5),\n",
        "            nn.Linear(512, 128),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layers(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "    \n",
        "        return F.log_softmax(x,dim=1) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nt-XrmTEAvf1",
        "outputId": "57d3805e-b26f-4660-87e8-e485882a19be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n",
            "Train Epoch: 1 [0/21600 (0%)]\tLoss: 5.014287\n",
            "Train Epoch: 1 [96/21600 (0%)]\tLoss: 3.825138\n",
            "Train Epoch: 1 [192/21600 (1%)]\tLoss: 2.875783\n",
            "Train Epoch: 1 [288/21600 (1%)]\tLoss: 2.351973\n",
            "Train Epoch: 1 [384/21600 (2%)]\tLoss: 1.513813\n",
            "Train Epoch: 1 [480/21600 (2%)]\tLoss: 1.306931\n",
            "Train Epoch: 1 [576/21600 (3%)]\tLoss: 0.984637\n",
            "Train Epoch: 1 [672/21600 (3%)]\tLoss: 0.668337\n",
            "Train Epoch: 1 [768/21600 (4%)]\tLoss: 0.623043\n",
            "Train Epoch: 1 [864/21600 (4%)]\tLoss: 0.725847\n",
            "Train Epoch: 1 [960/21600 (4%)]\tLoss: 0.769949\n",
            "Train Epoch: 1 [1056/21600 (5%)]\tLoss: 0.691719\n",
            "Train Epoch: 1 [1152/21600 (5%)]\tLoss: 0.747567\n",
            "Train Epoch: 1 [1248/21600 (6%)]\tLoss: 0.411536\n",
            "Train Epoch: 1 [1344/21600 (6%)]\tLoss: 0.529067\n",
            "Train Epoch: 1 [1440/21600 (7%)]\tLoss: 0.458953\n",
            "Train Epoch: 1 [1536/21600 (7%)]\tLoss: 0.488704\n",
            "Train Epoch: 1 [1632/21600 (8%)]\tLoss: 0.401880\n",
            "Train Epoch: 1 [1728/21600 (8%)]\tLoss: 0.420006\n",
            "Train Epoch: 1 [1824/21600 (8%)]\tLoss: 0.272656\n",
            "Train Epoch: 1 [1920/21600 (9%)]\tLoss: 0.409668\n",
            "Train Epoch: 1 [2016/21600 (9%)]\tLoss: 0.203608\n",
            "Train Epoch: 1 [2112/21600 (10%)]\tLoss: 0.529065\n",
            "Train Epoch: 1 [2208/21600 (10%)]\tLoss: 0.331349\n",
            "Train Epoch: 1 [2304/21600 (11%)]\tLoss: 0.358207\n",
            "Train Epoch: 1 [2400/21600 (11%)]\tLoss: 0.355535\n",
            "Train Epoch: 1 [2496/21600 (12%)]\tLoss: 0.356392\n",
            "Train Epoch: 1 [2592/21600 (12%)]\tLoss: 0.485028\n",
            "Train Epoch: 1 [2688/21600 (12%)]\tLoss: 0.258526\n",
            "Train Epoch: 1 [2784/21600 (13%)]\tLoss: 0.308491\n",
            "Train Epoch: 1 [2880/21600 (13%)]\tLoss: 0.302695\n",
            "Train Epoch: 1 [2976/21600 (14%)]\tLoss: 0.588303\n",
            "Train Epoch: 1 [3072/21600 (14%)]\tLoss: 0.288694\n",
            "Train Epoch: 1 [3168/21600 (15%)]\tLoss: 0.251564\n",
            "Train Epoch: 1 [3264/21600 (15%)]\tLoss: 0.294088\n",
            "Train Epoch: 1 [3360/21600 (16%)]\tLoss: 0.240710\n",
            "Train Epoch: 1 [3456/21600 (16%)]\tLoss: 0.271386\n",
            "Train Epoch: 1 [3552/21600 (16%)]\tLoss: 0.390069\n",
            "Train Epoch: 1 [3648/21600 (17%)]\tLoss: 0.391123\n",
            "Train Epoch: 1 [3744/21600 (17%)]\tLoss: 0.268783\n",
            "Train Epoch: 1 [3840/21600 (18%)]\tLoss: 0.382076\n",
            "Train Epoch: 1 [3936/21600 (18%)]\tLoss: 0.404430\n",
            "Train Epoch: 1 [4032/21600 (19%)]\tLoss: 0.333556\n",
            "Train Epoch: 1 [4128/21600 (19%)]\tLoss: 0.345508\n",
            "Train Epoch: 1 [4224/21600 (20%)]\tLoss: 0.295452\n",
            "Train Epoch: 1 [4320/21600 (20%)]\tLoss: 0.251829\n",
            "Train Epoch: 1 [4416/21600 (20%)]\tLoss: 0.213314\n",
            "Train Epoch: 1 [4512/21600 (21%)]\tLoss: 0.325794\n",
            "Train Epoch: 1 [4608/21600 (21%)]\tLoss: 0.301509\n",
            "Train Epoch: 1 [4704/21600 (22%)]\tLoss: 0.275355\n",
            "Train Epoch: 1 [4800/21600 (22%)]\tLoss: 0.165847\n",
            "Train Epoch: 1 [4896/21600 (23%)]\tLoss: 0.231791\n",
            "Train Epoch: 1 [4992/21600 (23%)]\tLoss: 0.171008\n",
            "Train Epoch: 1 [5088/21600 (24%)]\tLoss: 0.358848\n",
            "Train Epoch: 1 [5184/21600 (24%)]\tLoss: 0.381828\n",
            "Train Epoch: 1 [5280/21600 (24%)]\tLoss: 0.132449\n",
            "Train Epoch: 1 [5376/21600 (25%)]\tLoss: 0.542191\n",
            "Train Epoch: 1 [5472/21600 (25%)]\tLoss: 0.464004\n",
            "Train Epoch: 1 [5568/21600 (26%)]\tLoss: 0.367214\n",
            "Train Epoch: 1 [5664/21600 (26%)]\tLoss: 0.238959\n",
            "Train Epoch: 1 [5760/21600 (27%)]\tLoss: 0.215769\n",
            "Train Epoch: 1 [5856/21600 (27%)]\tLoss: 0.300908\n",
            "Train Epoch: 1 [5952/21600 (28%)]\tLoss: 0.148296\n",
            "Train Epoch: 1 [6048/21600 (28%)]\tLoss: 0.194885\n",
            "Train Epoch: 1 [6144/21600 (28%)]\tLoss: 0.139499\n",
            "Train Epoch: 1 [6240/21600 (29%)]\tLoss: 0.127083\n",
            "Train Epoch: 1 [6336/21600 (29%)]\tLoss: 0.359675\n",
            "Train Epoch: 1 [6432/21600 (30%)]\tLoss: 0.321225\n",
            "Train Epoch: 1 [6528/21600 (30%)]\tLoss: 0.287420\n",
            "Train Epoch: 1 [6624/21600 (31%)]\tLoss: 0.294524\n",
            "Train Epoch: 1 [6720/21600 (31%)]\tLoss: 0.157141\n",
            "Train Epoch: 1 [6816/21600 (32%)]\tLoss: 0.242176\n",
            "Train Epoch: 1 [6912/21600 (32%)]\tLoss: 0.337141\n",
            "Train Epoch: 1 [7008/21600 (32%)]\tLoss: 0.140884\n",
            "Train Epoch: 1 [7104/21600 (33%)]\tLoss: 0.224566\n",
            "Train Epoch: 1 [7200/21600 (33%)]\tLoss: 0.211415\n",
            "Train Epoch: 1 [7296/21600 (34%)]\tLoss: 0.215108\n",
            "Train Epoch: 1 [7392/21600 (34%)]\tLoss: 0.205359\n",
            "Train Epoch: 1 [7488/21600 (35%)]\tLoss: 0.218897\n",
            "Train Epoch: 1 [7584/21600 (35%)]\tLoss: 0.131675\n",
            "Train Epoch: 1 [7680/21600 (36%)]\tLoss: 0.102385\n",
            "Train Epoch: 1 [7776/21600 (36%)]\tLoss: 0.123465\n",
            "Train Epoch: 1 [7872/21600 (36%)]\tLoss: 0.179162\n",
            "Train Epoch: 1 [7968/21600 (37%)]\tLoss: 0.180689\n",
            "Train Epoch: 1 [8064/21600 (37%)]\tLoss: 0.450251\n",
            "Train Epoch: 1 [8160/21600 (38%)]\tLoss: 0.134622\n",
            "Train Epoch: 1 [8256/21600 (38%)]\tLoss: 0.132438\n",
            "Train Epoch: 1 [8352/21600 (39%)]\tLoss: 0.161315\n",
            "Train Epoch: 1 [8448/21600 (39%)]\tLoss: 0.140031\n",
            "Train Epoch: 1 [8544/21600 (40%)]\tLoss: 0.219888\n",
            "Train Epoch: 1 [8640/21600 (40%)]\tLoss: 0.085563\n",
            "Train Epoch: 1 [8736/21600 (40%)]\tLoss: 0.150743\n",
            "Train Epoch: 1 [8832/21600 (41%)]\tLoss: 0.076108\n",
            "Train Epoch: 1 [8928/21600 (41%)]\tLoss: 0.154082\n",
            "Train Epoch: 1 [9024/21600 (42%)]\tLoss: 0.204375\n",
            "Train Epoch: 1 [9120/21600 (42%)]\tLoss: 0.126560\n",
            "Train Epoch: 1 [9216/21600 (43%)]\tLoss: 0.233247\n",
            "Train Epoch: 1 [9312/21600 (43%)]\tLoss: 0.350904\n",
            "Train Epoch: 1 [9408/21600 (44%)]\tLoss: 0.131506\n",
            "Train Epoch: 1 [9504/21600 (44%)]\tLoss: 0.103017\n",
            "Train Epoch: 1 [9600/21600 (44%)]\tLoss: 0.269159\n",
            "Train Epoch: 1 [9696/21600 (45%)]\tLoss: 0.098275\n",
            "Train Epoch: 1 [9792/21600 (45%)]\tLoss: 0.147965\n",
            "Train Epoch: 1 [9888/21600 (46%)]\tLoss: 0.082698\n",
            "Train Epoch: 1 [9984/21600 (46%)]\tLoss: 0.130180\n",
            "Train Epoch: 1 [10080/21600 (47%)]\tLoss: 0.129391\n",
            "Train Epoch: 1 [10176/21600 (47%)]\tLoss: 0.137880\n",
            "Train Epoch: 1 [10272/21600 (48%)]\tLoss: 0.073631\n",
            "Train Epoch: 1 [10368/21600 (48%)]\tLoss: 0.164162\n",
            "Train Epoch: 1 [10464/21600 (48%)]\tLoss: 0.123999\n",
            "Train Epoch: 1 [10560/21600 (49%)]\tLoss: 0.160791\n",
            "Train Epoch: 1 [10656/21600 (49%)]\tLoss: 0.096902\n",
            "Train Epoch: 1 [10752/21600 (50%)]\tLoss: 0.100271\n",
            "Train Epoch: 1 [10848/21600 (50%)]\tLoss: 0.055124\n",
            "Train Epoch: 1 [10944/21600 (51%)]\tLoss: 0.098375\n",
            "Train Epoch: 1 [11040/21600 (51%)]\tLoss: 0.061514\n",
            "Train Epoch: 1 [11136/21600 (52%)]\tLoss: 0.080945\n",
            "Train Epoch: 1 [11232/21600 (52%)]\tLoss: 0.044895\n",
            "Train Epoch: 1 [11328/21600 (52%)]\tLoss: 0.177056\n",
            "Train Epoch: 1 [11424/21600 (53%)]\tLoss: 0.080830\n",
            "Train Epoch: 1 [11520/21600 (53%)]\tLoss: 0.135386\n",
            "Train Epoch: 1 [11616/21600 (54%)]\tLoss: 0.119095\n",
            "Train Epoch: 1 [11712/21600 (54%)]\tLoss: 0.139639\n",
            "Train Epoch: 1 [11808/21600 (55%)]\tLoss: 0.055005\n",
            "Train Epoch: 1 [11904/21600 (55%)]\tLoss: 0.108802\n",
            "Train Epoch: 1 [12000/21600 (56%)]\tLoss: 0.072605\n",
            "Train Epoch: 1 [12096/21600 (56%)]\tLoss: 0.139962\n",
            "Train Epoch: 1 [12192/21600 (56%)]\tLoss: 0.048456\n",
            "Train Epoch: 1 [12288/21600 (57%)]\tLoss: 0.119842\n",
            "Train Epoch: 1 [12384/21600 (57%)]\tLoss: 0.096049\n",
            "Train Epoch: 1 [12480/21600 (58%)]\tLoss: 0.123156\n",
            "Train Epoch: 1 [12576/21600 (58%)]\tLoss: 0.092769\n",
            "Train Epoch: 1 [12672/21600 (59%)]\tLoss: 0.099271\n",
            "Train Epoch: 1 [12768/21600 (59%)]\tLoss: 0.179412\n",
            "Train Epoch: 1 [12864/21600 (60%)]\tLoss: 0.052277\n",
            "Train Epoch: 1 [12960/21600 (60%)]\tLoss: 0.118512\n",
            "Train Epoch: 1 [13056/21600 (60%)]\tLoss: 0.048010\n",
            "Train Epoch: 1 [13152/21600 (61%)]\tLoss: 0.071923\n",
            "Train Epoch: 1 [13248/21600 (61%)]\tLoss: 0.045885\n",
            "Train Epoch: 1 [13344/21600 (62%)]\tLoss: 0.149812\n",
            "Train Epoch: 1 [13440/21600 (62%)]\tLoss: 0.083458\n",
            "Train Epoch: 1 [13536/21600 (63%)]\tLoss: 0.120424\n",
            "Train Epoch: 1 [13632/21600 (63%)]\tLoss: 0.083296\n",
            "Train Epoch: 1 [13728/21600 (64%)]\tLoss: 0.083336\n",
            "Train Epoch: 1 [13824/21600 (64%)]\tLoss: 0.063828\n",
            "Train Epoch: 1 [13920/21600 (64%)]\tLoss: 0.091932\n",
            "Train Epoch: 1 [14016/21600 (65%)]\tLoss: 0.141888\n",
            "Train Epoch: 1 [14112/21600 (65%)]\tLoss: 0.037135\n",
            "Train Epoch: 1 [14208/21600 (66%)]\tLoss: 0.182299\n",
            "Train Epoch: 1 [14304/21600 (66%)]\tLoss: 0.024909\n",
            "Train Epoch: 1 [14400/21600 (67%)]\tLoss: 0.095252\n",
            "Train Epoch: 1 [14496/21600 (67%)]\tLoss: 0.096233\n",
            "Train Epoch: 1 [14592/21600 (68%)]\tLoss: 0.060680\n",
            "Train Epoch: 1 [14688/21600 (68%)]\tLoss: 0.119711\n",
            "Train Epoch: 1 [14784/21600 (68%)]\tLoss: 0.091241\n",
            "Train Epoch: 1 [14880/21600 (69%)]\tLoss: 0.356404\n",
            "Train Epoch: 1 [14976/21600 (69%)]\tLoss: 0.227152\n",
            "Train Epoch: 1 [15072/21600 (70%)]\tLoss: 0.138125\n",
            "Train Epoch: 1 [15168/21600 (70%)]\tLoss: 0.137576\n",
            "Train Epoch: 1 [15264/21600 (71%)]\tLoss: 0.065997\n",
            "Train Epoch: 1 [15360/21600 (71%)]\tLoss: 0.039009\n",
            "Train Epoch: 1 [15456/21600 (72%)]\tLoss: 0.132496\n",
            "Train Epoch: 1 [15552/21600 (72%)]\tLoss: 0.097948\n",
            "Train Epoch: 1 [15648/21600 (72%)]\tLoss: 0.253397\n",
            "Train Epoch: 1 [15744/21600 (73%)]\tLoss: 0.077135\n",
            "Train Epoch: 1 [15840/21600 (73%)]\tLoss: 0.262337\n",
            "Train Epoch: 1 [15936/21600 (74%)]\tLoss: 0.026779\n",
            "Train Epoch: 1 [16032/21600 (74%)]\tLoss: 0.068680\n",
            "Train Epoch: 1 [16128/21600 (75%)]\tLoss: 0.132972\n",
            "Train Epoch: 1 [16224/21600 (75%)]\tLoss: 0.031101\n",
            "Train Epoch: 1 [16320/21600 (76%)]\tLoss: 0.050094\n",
            "Train Epoch: 1 [16416/21600 (76%)]\tLoss: 0.140156\n",
            "Train Epoch: 1 [16512/21600 (76%)]\tLoss: 0.029988\n",
            "Train Epoch: 1 [16608/21600 (77%)]\tLoss: 0.118699\n",
            "Train Epoch: 1 [16704/21600 (77%)]\tLoss: 0.030458\n",
            "Train Epoch: 1 [16800/21600 (78%)]\tLoss: 0.109682\n",
            "Train Epoch: 1 [16896/21600 (78%)]\tLoss: 0.033019\n",
            "Train Epoch: 1 [16992/21600 (79%)]\tLoss: 0.112494\n",
            "Train Epoch: 1 [17088/21600 (79%)]\tLoss: 0.066023\n",
            "Train Epoch: 1 [17184/21600 (80%)]\tLoss: 0.037967\n",
            "Train Epoch: 1 [17280/21600 (80%)]\tLoss: 0.038813\n",
            "Train Epoch: 1 [17376/21600 (80%)]\tLoss: 0.066847\n",
            "Train Epoch: 1 [17472/21600 (81%)]\tLoss: 0.120468\n",
            "Train Epoch: 1 [17568/21600 (81%)]\tLoss: 0.041205\n",
            "Train Epoch: 1 [17664/21600 (82%)]\tLoss: 0.040205\n",
            "Train Epoch: 1 [17760/21600 (82%)]\tLoss: 0.024693\n",
            "Train Epoch: 1 [17856/21600 (83%)]\tLoss: 0.111526\n",
            "Train Epoch: 1 [17952/21600 (83%)]\tLoss: 0.021388\n",
            "Train Epoch: 1 [18048/21600 (84%)]\tLoss: 0.042554\n",
            "Train Epoch: 1 [18144/21600 (84%)]\tLoss: 0.185621\n",
            "Train Epoch: 1 [18240/21600 (84%)]\tLoss: 0.052513\n",
            "Train Epoch: 1 [18336/21600 (85%)]\tLoss: 0.018514\n",
            "Train Epoch: 1 [18432/21600 (85%)]\tLoss: 0.071650\n",
            "Train Epoch: 1 [18528/21600 (86%)]\tLoss: 0.026434\n",
            "Train Epoch: 1 [18624/21600 (86%)]\tLoss: 0.035263\n",
            "Train Epoch: 1 [18720/21600 (87%)]\tLoss: 0.054425\n",
            "Train Epoch: 1 [18816/21600 (87%)]\tLoss: 0.053407\n",
            "Train Epoch: 1 [18912/21600 (88%)]\tLoss: 0.021423\n",
            "Train Epoch: 1 [19008/21600 (88%)]\tLoss: 0.106152\n",
            "Train Epoch: 1 [19104/21600 (88%)]\tLoss: 0.055364\n",
            "Train Epoch: 1 [19200/21600 (89%)]\tLoss: 0.152033\n",
            "Train Epoch: 1 [19296/21600 (89%)]\tLoss: 0.027315\n",
            "Train Epoch: 1 [19392/21600 (90%)]\tLoss: 0.016205\n",
            "Train Epoch: 1 [19488/21600 (90%)]\tLoss: 0.143463\n",
            "Train Epoch: 1 [19584/21600 (91%)]\tLoss: 0.020421\n",
            "Train Epoch: 1 [19680/21600 (91%)]\tLoss: 0.089271\n",
            "Train Epoch: 1 [19776/21600 (92%)]\tLoss: 0.057139\n",
            "Train Epoch: 1 [19872/21600 (92%)]\tLoss: 0.129940\n",
            "Train Epoch: 1 [19968/21600 (92%)]\tLoss: 0.098443\n",
            "Train Epoch: 1 [20064/21600 (93%)]\tLoss: 0.134808\n",
            "Train Epoch: 1 [20160/21600 (93%)]\tLoss: 0.106756\n",
            "Train Epoch: 1 [20256/21600 (94%)]\tLoss: 0.032987\n",
            "Train Epoch: 1 [20352/21600 (94%)]\tLoss: 0.044437\n",
            "Train Epoch: 1 [20448/21600 (95%)]\tLoss: 0.032849\n",
            "Train Epoch: 1 [20544/21600 (95%)]\tLoss: 0.027305\n",
            "Train Epoch: 1 [20640/21600 (96%)]\tLoss: 0.020719\n",
            "Train Epoch: 1 [20736/21600 (96%)]\tLoss: 0.087995\n",
            "Train Epoch: 1 [20832/21600 (96%)]\tLoss: 0.069303\n",
            "Train Epoch: 1 [20928/21600 (97%)]\tLoss: 0.077255\n",
            "Train Epoch: 1 [21024/21600 (97%)]\tLoss: 0.012437\n",
            "Train Epoch: 1 [21120/21600 (98%)]\tLoss: 0.022152\n",
            "Train Epoch: 1 [21216/21600 (98%)]\tLoss: 0.048921\n",
            "Train Epoch: 1 [21312/21600 (99%)]\tLoss: 0.084982\n",
            "Train Epoch: 1 [21408/21600 (99%)]\tLoss: 0.079609\n",
            "Train Epoch: 1 [21504/21600 (100%)]\tLoss: 0.015829\n",
            "\n",
            "Validation set: Average loss: 0.1065, Accuracy: 5199/5400 (96%)\n",
            "\n",
            "\n",
            "Saved model to model_binary_1.pth.\n",
            "Train Epoch: 2 [0/21600 (0%)]\tLoss: 0.071078\n",
            "Train Epoch: 2 [96/21600 (0%)]\tLoss: 0.011135\n",
            "Train Epoch: 2 [192/21600 (1%)]\tLoss: 0.099666\n",
            "Train Epoch: 2 [288/21600 (1%)]\tLoss: 0.069816\n",
            "Train Epoch: 2 [384/21600 (2%)]\tLoss: 0.146430\n",
            "Train Epoch: 2 [480/21600 (2%)]\tLoss: 0.249733\n",
            "Train Epoch: 2 [576/21600 (3%)]\tLoss: 0.176408\n",
            "Train Epoch: 2 [672/21600 (3%)]\tLoss: 0.005580\n",
            "Train Epoch: 2 [768/21600 (4%)]\tLoss: 0.055743\n",
            "Train Epoch: 2 [864/21600 (4%)]\tLoss: 0.062537\n",
            "Train Epoch: 2 [960/21600 (4%)]\tLoss: 0.017036\n",
            "Train Epoch: 2 [1056/21600 (5%)]\tLoss: 0.036353\n",
            "Train Epoch: 2 [1152/21600 (5%)]\tLoss: 0.037926\n",
            "Train Epoch: 2 [1248/21600 (6%)]\tLoss: 0.008364\n",
            "Train Epoch: 2 [1344/21600 (6%)]\tLoss: 0.011169\n",
            "Train Epoch: 2 [1440/21600 (7%)]\tLoss: 0.084091\n",
            "Train Epoch: 2 [1536/21600 (7%)]\tLoss: 0.018656\n",
            "Train Epoch: 2 [1632/21600 (8%)]\tLoss: 0.117339\n",
            "Train Epoch: 2 [1728/21600 (8%)]\tLoss: 0.079272\n",
            "Train Epoch: 2 [1824/21600 (8%)]\tLoss: 0.019644\n",
            "Train Epoch: 2 [1920/21600 (9%)]\tLoss: 0.027125\n",
            "Train Epoch: 2 [2016/21600 (9%)]\tLoss: 0.009810\n",
            "Train Epoch: 2 [2112/21600 (10%)]\tLoss: 0.134746\n",
            "Train Epoch: 2 [2208/21600 (10%)]\tLoss: 0.196018\n",
            "Train Epoch: 2 [2304/21600 (11%)]\tLoss: 0.062585\n",
            "Train Epoch: 2 [2400/21600 (11%)]\tLoss: 0.063745\n",
            "Train Epoch: 2 [2496/21600 (12%)]\tLoss: 0.039753\n",
            "Train Epoch: 2 [2592/21600 (12%)]\tLoss: 0.076351\n",
            "Train Epoch: 2 [2688/21600 (12%)]\tLoss: 0.021614\n",
            "Train Epoch: 2 [2784/21600 (13%)]\tLoss: 0.010853\n",
            "Train Epoch: 2 [2880/21600 (13%)]\tLoss: 0.085081\n",
            "Train Epoch: 2 [2976/21600 (14%)]\tLoss: 0.078497\n",
            "Train Epoch: 2 [3072/21600 (14%)]\tLoss: 0.053804\n",
            "Train Epoch: 2 [3168/21600 (15%)]\tLoss: 0.019923\n",
            "Train Epoch: 2 [3264/21600 (15%)]\tLoss: 0.006642\n",
            "Train Epoch: 2 [3360/21600 (16%)]\tLoss: 0.035379\n",
            "Train Epoch: 2 [3456/21600 (16%)]\tLoss: 0.012432\n",
            "Train Epoch: 2 [3552/21600 (16%)]\tLoss: 0.031157\n",
            "Train Epoch: 2 [3648/21600 (17%)]\tLoss: 0.047830\n",
            "Train Epoch: 2 [3744/21600 (17%)]\tLoss: 0.125185\n",
            "Train Epoch: 2 [3840/21600 (18%)]\tLoss: 0.047082\n",
            "Train Epoch: 2 [3936/21600 (18%)]\tLoss: 0.052133\n",
            "Train Epoch: 2 [4032/21600 (19%)]\tLoss: 0.022363\n",
            "Train Epoch: 2 [4128/21600 (19%)]\tLoss: 0.053520\n",
            "Train Epoch: 2 [4224/21600 (20%)]\tLoss: 0.011334\n",
            "Train Epoch: 2 [4320/21600 (20%)]\tLoss: 0.012065\n",
            "Train Epoch: 2 [4416/21600 (20%)]\tLoss: 0.016257\n",
            "Train Epoch: 2 [4512/21600 (21%)]\tLoss: 0.036335\n",
            "Train Epoch: 2 [4608/21600 (21%)]\tLoss: 0.019013\n",
            "Train Epoch: 2 [4704/21600 (22%)]\tLoss: 0.035456\n",
            "Train Epoch: 2 [4800/21600 (22%)]\tLoss: 0.004291\n",
            "Train Epoch: 2 [4896/21600 (23%)]\tLoss: 0.014154\n",
            "Train Epoch: 2 [4992/21600 (23%)]\tLoss: 0.087617\n",
            "Train Epoch: 2 [5088/21600 (24%)]\tLoss: 0.116717\n",
            "Train Epoch: 2 [5184/21600 (24%)]\tLoss: 0.056405\n",
            "Train Epoch: 2 [5280/21600 (24%)]\tLoss: 0.068914\n",
            "Train Epoch: 2 [5376/21600 (25%)]\tLoss: 0.010240\n",
            "Train Epoch: 2 [5472/21600 (25%)]\tLoss: 0.007451\n",
            "Train Epoch: 2 [5568/21600 (26%)]\tLoss: 0.023361\n",
            "Train Epoch: 2 [5664/21600 (26%)]\tLoss: 0.016140\n",
            "Train Epoch: 2 [5760/21600 (27%)]\tLoss: 0.006833\n",
            "Train Epoch: 2 [5856/21600 (27%)]\tLoss: 0.153397\n",
            "Train Epoch: 2 [5952/21600 (28%)]\tLoss: 0.041381\n",
            "Train Epoch: 2 [6048/21600 (28%)]\tLoss: 0.104015\n",
            "Train Epoch: 2 [6144/21600 (28%)]\tLoss: 0.078738\n",
            "Train Epoch: 2 [6240/21600 (29%)]\tLoss: 0.028069\n",
            "Train Epoch: 2 [6336/21600 (29%)]\tLoss: 0.004724\n",
            "Train Epoch: 2 [6432/21600 (30%)]\tLoss: 0.003975\n",
            "Train Epoch: 2 [6528/21600 (30%)]\tLoss: 0.024006\n",
            "Train Epoch: 2 [6624/21600 (31%)]\tLoss: 0.025434\n",
            "Train Epoch: 2 [6720/21600 (31%)]\tLoss: 0.039589\n",
            "Train Epoch: 2 [6816/21600 (32%)]\tLoss: 0.034735\n",
            "Train Epoch: 2 [6912/21600 (32%)]\tLoss: 0.026975\n",
            "Train Epoch: 2 [7008/21600 (32%)]\tLoss: 0.051822\n",
            "Train Epoch: 2 [7104/21600 (33%)]\tLoss: 0.004577\n",
            "Train Epoch: 2 [7200/21600 (33%)]\tLoss: 0.036931\n",
            "Train Epoch: 2 [7296/21600 (34%)]\tLoss: 0.012561\n",
            "Train Epoch: 2 [7392/21600 (34%)]\tLoss: 0.022615\n",
            "Train Epoch: 2 [7488/21600 (35%)]\tLoss: 0.013196\n",
            "Train Epoch: 2 [7584/21600 (35%)]\tLoss: 0.030347\n",
            "Train Epoch: 2 [7680/21600 (36%)]\tLoss: 0.087898\n",
            "Train Epoch: 2 [7776/21600 (36%)]\tLoss: 0.052271\n",
            "Train Epoch: 2 [7872/21600 (36%)]\tLoss: 0.004421\n",
            "Train Epoch: 2 [7968/21600 (37%)]\tLoss: 0.044549\n",
            "Train Epoch: 2 [8064/21600 (37%)]\tLoss: 0.046042\n",
            "Train Epoch: 2 [8160/21600 (38%)]\tLoss: 0.053974\n",
            "Train Epoch: 2 [8256/21600 (38%)]\tLoss: 0.023502\n",
            "Train Epoch: 2 [8352/21600 (39%)]\tLoss: 0.026657\n",
            "Train Epoch: 2 [8448/21600 (39%)]\tLoss: 0.007069\n",
            "Train Epoch: 2 [8544/21600 (40%)]\tLoss: 0.035459\n",
            "Train Epoch: 2 [8640/21600 (40%)]\tLoss: 0.085893\n",
            "Train Epoch: 2 [8736/21600 (40%)]\tLoss: 0.068008\n",
            "Train Epoch: 2 [8832/21600 (41%)]\tLoss: 0.205799\n",
            "Train Epoch: 2 [8928/21600 (41%)]\tLoss: 0.039086\n",
            "Train Epoch: 2 [9024/21600 (42%)]\tLoss: 0.008565\n",
            "Train Epoch: 2 [9120/21600 (42%)]\tLoss: 0.032395\n",
            "Train Epoch: 2 [9216/21600 (43%)]\tLoss: 0.017175\n",
            "Train Epoch: 2 [9312/21600 (43%)]\tLoss: 0.050860\n",
            "Train Epoch: 2 [9408/21600 (44%)]\tLoss: 0.080221\n",
            "Train Epoch: 2 [9504/21600 (44%)]\tLoss: 0.009879\n",
            "Train Epoch: 2 [9600/21600 (44%)]\tLoss: 0.150155\n",
            "Train Epoch: 2 [9696/21600 (45%)]\tLoss: 0.012027\n",
            "Train Epoch: 2 [9792/21600 (45%)]\tLoss: 0.019396\n",
            "Train Epoch: 2 [9888/21600 (46%)]\tLoss: 0.015068\n",
            "Train Epoch: 2 [9984/21600 (46%)]\tLoss: 0.072435\n",
            "Train Epoch: 2 [10080/21600 (47%)]\tLoss: 0.005085\n",
            "Train Epoch: 2 [10176/21600 (47%)]\tLoss: 0.027762\n",
            "Train Epoch: 2 [10272/21600 (48%)]\tLoss: 0.018606\n",
            "Train Epoch: 2 [10368/21600 (48%)]\tLoss: 0.064443\n",
            "Train Epoch: 2 [10464/21600 (48%)]\tLoss: 0.062242\n",
            "Train Epoch: 2 [10560/21600 (49%)]\tLoss: 0.030864\n",
            "Train Epoch: 2 [10656/21600 (49%)]\tLoss: 0.018521\n",
            "Train Epoch: 2 [10752/21600 (50%)]\tLoss: 0.016634\n",
            "Train Epoch: 2 [10848/21600 (50%)]\tLoss: 0.007679\n",
            "Train Epoch: 2 [10944/21600 (51%)]\tLoss: 0.057309\n",
            "Train Epoch: 2 [11040/21600 (51%)]\tLoss: 0.077822\n",
            "Train Epoch: 2 [11136/21600 (52%)]\tLoss: 0.061158\n",
            "Train Epoch: 2 [11232/21600 (52%)]\tLoss: 0.016772\n",
            "Train Epoch: 2 [11328/21600 (52%)]\tLoss: 0.015740\n",
            "Train Epoch: 2 [11424/21600 (53%)]\tLoss: 0.014642\n",
            "Train Epoch: 2 [11520/21600 (53%)]\tLoss: 0.009764\n",
            "Train Epoch: 2 [11616/21600 (54%)]\tLoss: 0.082040\n",
            "Train Epoch: 2 [11712/21600 (54%)]\tLoss: 0.015585\n",
            "Train Epoch: 2 [11808/21600 (55%)]\tLoss: 0.040874\n",
            "Train Epoch: 2 [11904/21600 (55%)]\tLoss: 0.007619\n",
            "Train Epoch: 2 [12000/21600 (56%)]\tLoss: 0.026531\n",
            "Train Epoch: 2 [12096/21600 (56%)]\tLoss: 0.060573\n",
            "Train Epoch: 2 [12192/21600 (56%)]\tLoss: 0.020538\n",
            "Train Epoch: 2 [12288/21600 (57%)]\tLoss: 0.005523\n",
            "Train Epoch: 2 [12384/21600 (57%)]\tLoss: 0.041489\n",
            "Train Epoch: 2 [12480/21600 (58%)]\tLoss: 0.003227\n",
            "Train Epoch: 2 [12576/21600 (58%)]\tLoss: 0.060952\n",
            "Train Epoch: 2 [12672/21600 (59%)]\tLoss: 0.014495\n",
            "Train Epoch: 2 [12768/21600 (59%)]\tLoss: 0.029331\n",
            "Train Epoch: 2 [12864/21600 (60%)]\tLoss: 0.020591\n",
            "Train Epoch: 2 [12960/21600 (60%)]\tLoss: 0.129401\n",
            "Train Epoch: 2 [13056/21600 (60%)]\tLoss: 0.053963\n",
            "Train Epoch: 2 [13152/21600 (61%)]\tLoss: 0.180283\n",
            "Train Epoch: 2 [13248/21600 (61%)]\tLoss: 0.007263\n",
            "Train Epoch: 2 [13344/21600 (62%)]\tLoss: 0.163441\n",
            "Train Epoch: 2 [13440/21600 (62%)]\tLoss: 0.018617\n",
            "Train Epoch: 2 [13536/21600 (63%)]\tLoss: 0.029215\n",
            "Train Epoch: 2 [13632/21600 (63%)]\tLoss: 0.017908\n",
            "Train Epoch: 2 [13728/21600 (64%)]\tLoss: 0.095230\n",
            "Train Epoch: 2 [13824/21600 (64%)]\tLoss: 0.006735\n",
            "Train Epoch: 2 [13920/21600 (64%)]\tLoss: 0.021349\n",
            "Train Epoch: 2 [14016/21600 (65%)]\tLoss: 0.020489\n",
            "Train Epoch: 2 [14112/21600 (65%)]\tLoss: 0.001241\n",
            "Train Epoch: 2 [14208/21600 (66%)]\tLoss: 0.023023\n",
            "Train Epoch: 2 [14304/21600 (66%)]\tLoss: 0.048296\n",
            "Train Epoch: 2 [14400/21600 (67%)]\tLoss: 0.047828\n",
            "Train Epoch: 2 [14496/21600 (67%)]\tLoss: 0.034758\n",
            "Train Epoch: 2 [14592/21600 (68%)]\tLoss: 0.033641\n",
            "Train Epoch: 2 [14688/21600 (68%)]\tLoss: 0.021240\n",
            "Train Epoch: 2 [14784/21600 (68%)]\tLoss: 0.139585\n",
            "Train Epoch: 2 [14880/21600 (69%)]\tLoss: 0.008884\n",
            "Train Epoch: 2 [14976/21600 (69%)]\tLoss: 0.012259\n",
            "Train Epoch: 2 [15072/21600 (70%)]\tLoss: 0.022090\n",
            "Train Epoch: 2 [15168/21600 (70%)]\tLoss: 0.027007\n",
            "Train Epoch: 2 [15264/21600 (71%)]\tLoss: 0.007239\n",
            "Train Epoch: 2 [15360/21600 (71%)]\tLoss: 0.034367\n",
            "Train Epoch: 2 [15456/21600 (72%)]\tLoss: 0.010829\n",
            "Train Epoch: 2 [15552/21600 (72%)]\tLoss: 0.049600\n",
            "Train Epoch: 2 [15648/21600 (72%)]\tLoss: 0.126270\n",
            "Train Epoch: 2 [15744/21600 (73%)]\tLoss: 0.028097\n",
            "Train Epoch: 2 [15840/21600 (73%)]\tLoss: 0.020626\n",
            "Train Epoch: 2 [15936/21600 (74%)]\tLoss: 0.022041\n",
            "Train Epoch: 2 [16032/21600 (74%)]\tLoss: 0.013198\n",
            "Train Epoch: 2 [16128/21600 (75%)]\tLoss: 0.003519\n",
            "Train Epoch: 2 [16224/21600 (75%)]\tLoss: 0.002935\n",
            "Train Epoch: 2 [16320/21600 (76%)]\tLoss: 0.013265\n",
            "Train Epoch: 2 [16416/21600 (76%)]\tLoss: 0.001859\n",
            "Train Epoch: 2 [16512/21600 (76%)]\tLoss: 0.005219\n",
            "Train Epoch: 2 [16608/21600 (77%)]\tLoss: 0.025337\n",
            "Train Epoch: 2 [16704/21600 (77%)]\tLoss: 0.025252\n",
            "Train Epoch: 2 [16800/21600 (78%)]\tLoss: 0.006771\n",
            "Train Epoch: 2 [16896/21600 (78%)]\tLoss: 0.001910\n",
            "Train Epoch: 2 [16992/21600 (79%)]\tLoss: 0.061115\n",
            "Train Epoch: 2 [17088/21600 (79%)]\tLoss: 0.161944\n",
            "Train Epoch: 2 [17184/21600 (80%)]\tLoss: 0.014248\n",
            "Train Epoch: 2 [17280/21600 (80%)]\tLoss: 0.002896\n",
            "Train Epoch: 2 [17376/21600 (80%)]\tLoss: 0.081120\n",
            "Train Epoch: 2 [17472/21600 (81%)]\tLoss: 0.045149\n",
            "Train Epoch: 2 [17568/21600 (81%)]\tLoss: 0.005686\n",
            "Train Epoch: 2 [17664/21600 (82%)]\tLoss: 0.026592\n",
            "Train Epoch: 2 [17760/21600 (82%)]\tLoss: 0.272385\n",
            "Train Epoch: 2 [17856/21600 (83%)]\tLoss: 0.023690\n",
            "Train Epoch: 2 [17952/21600 (83%)]\tLoss: 0.077851\n",
            "Train Epoch: 2 [18048/21600 (84%)]\tLoss: 0.032084\n",
            "Train Epoch: 2 [18144/21600 (84%)]\tLoss: 0.007802\n",
            "Train Epoch: 2 [18240/21600 (84%)]\tLoss: 0.007548\n",
            "Train Epoch: 2 [18336/21600 (85%)]\tLoss: 0.027264\n",
            "Train Epoch: 2 [18432/21600 (85%)]\tLoss: 0.132278\n",
            "Train Epoch: 2 [18528/21600 (86%)]\tLoss: 0.014023\n",
            "Train Epoch: 2 [18624/21600 (86%)]\tLoss: 0.133708\n",
            "Train Epoch: 2 [18720/21600 (87%)]\tLoss: 0.004833\n",
            "Train Epoch: 2 [18816/21600 (87%)]\tLoss: 0.026868\n",
            "Train Epoch: 2 [18912/21600 (88%)]\tLoss: 0.032943\n",
            "Train Epoch: 2 [19008/21600 (88%)]\tLoss: 0.036269\n",
            "Train Epoch: 2 [19104/21600 (88%)]\tLoss: 0.033086\n",
            "Train Epoch: 2 [19200/21600 (89%)]\tLoss: 0.052733\n",
            "Train Epoch: 2 [19296/21600 (89%)]\tLoss: 0.007130\n",
            "Train Epoch: 2 [19392/21600 (90%)]\tLoss: 0.005158\n",
            "Train Epoch: 2 [19488/21600 (90%)]\tLoss: 0.015372\n",
            "Train Epoch: 2 [19584/21600 (91%)]\tLoss: 0.002856\n",
            "Train Epoch: 2 [19680/21600 (91%)]\tLoss: 0.006252\n",
            "Train Epoch: 2 [19776/21600 (92%)]\tLoss: 0.001228\n",
            "Train Epoch: 2 [19872/21600 (92%)]\tLoss: 0.016467\n",
            "Train Epoch: 2 [19968/21600 (92%)]\tLoss: 0.011755\n",
            "Train Epoch: 2 [20064/21600 (93%)]\tLoss: 0.025702\n",
            "Train Epoch: 2 [20160/21600 (93%)]\tLoss: 0.012952\n",
            "Train Epoch: 2 [20256/21600 (94%)]\tLoss: 0.053919\n",
            "Train Epoch: 2 [20352/21600 (94%)]\tLoss: 0.010972\n",
            "Train Epoch: 2 [20448/21600 (95%)]\tLoss: 0.012487\n",
            "Train Epoch: 2 [20544/21600 (95%)]\tLoss: 0.024149\n",
            "Train Epoch: 2 [20640/21600 (96%)]\tLoss: 0.031258\n",
            "Train Epoch: 2 [20736/21600 (96%)]\tLoss: 0.051514\n",
            "Train Epoch: 2 [20832/21600 (96%)]\tLoss: 0.048141\n",
            "Train Epoch: 2 [20928/21600 (97%)]\tLoss: 0.002186\n",
            "Train Epoch: 2 [21024/21600 (97%)]\tLoss: 0.008995\n",
            "Train Epoch: 2 [21120/21600 (98%)]\tLoss: 0.001235\n",
            "Train Epoch: 2 [21216/21600 (98%)]\tLoss: 0.016641\n",
            "Train Epoch: 2 [21312/21600 (99%)]\tLoss: 0.032309\n",
            "Train Epoch: 2 [21408/21600 (99%)]\tLoss: 0.005495\n",
            "Train Epoch: 2 [21504/21600 (100%)]\tLoss: 0.013434\n",
            "\n",
            "Validation set: Average loss: 0.0091, Accuracy: 5384/5400 (100%)\n",
            "\n",
            "\n",
            "Saved model to model_binary_2.pth.\n",
            "Train Epoch: 3 [0/21600 (0%)]\tLoss: 0.049853\n",
            "Train Epoch: 3 [96/21600 (0%)]\tLoss: 0.002493\n",
            "Train Epoch: 3 [192/21600 (1%)]\tLoss: 0.038270\n",
            "Train Epoch: 3 [288/21600 (1%)]\tLoss: 0.002918\n",
            "Train Epoch: 3 [384/21600 (2%)]\tLoss: 0.003865\n",
            "Train Epoch: 3 [480/21600 (2%)]\tLoss: 0.003283\n",
            "Train Epoch: 3 [576/21600 (3%)]\tLoss: 0.008239\n",
            "Train Epoch: 3 [672/21600 (3%)]\tLoss: 0.007375\n",
            "Train Epoch: 3 [768/21600 (4%)]\tLoss: 0.016319\n",
            "Train Epoch: 3 [864/21600 (4%)]\tLoss: 0.054225\n",
            "Train Epoch: 3 [960/21600 (4%)]\tLoss: 0.011335\n",
            "Train Epoch: 3 [1056/21600 (5%)]\tLoss: 0.008226\n",
            "Train Epoch: 3 [1152/21600 (5%)]\tLoss: 0.004506\n",
            "Train Epoch: 3 [1248/21600 (6%)]\tLoss: 0.044999\n",
            "Train Epoch: 3 [1344/21600 (6%)]\tLoss: 0.053229\n",
            "Train Epoch: 3 [1440/21600 (7%)]\tLoss: 0.018668\n",
            "Train Epoch: 3 [1536/21600 (7%)]\tLoss: 0.089626\n",
            "Train Epoch: 3 [1632/21600 (8%)]\tLoss: 0.123096\n",
            "Train Epoch: 3 [1728/21600 (8%)]\tLoss: 0.019755\n",
            "Train Epoch: 3 [1824/21600 (8%)]\tLoss: 0.078169\n",
            "Train Epoch: 3 [1920/21600 (9%)]\tLoss: 0.004870\n",
            "Train Epoch: 3 [2016/21600 (9%)]\tLoss: 0.032963\n",
            "Train Epoch: 3 [2112/21600 (10%)]\tLoss: 0.001525\n",
            "Train Epoch: 3 [2208/21600 (10%)]\tLoss: 0.062144\n",
            "Train Epoch: 3 [2304/21600 (11%)]\tLoss: 0.005574\n",
            "Train Epoch: 3 [2400/21600 (11%)]\tLoss: 0.002415\n",
            "Train Epoch: 3 [2496/21600 (12%)]\tLoss: 0.003377\n",
            "Train Epoch: 3 [2592/21600 (12%)]\tLoss: 0.007150\n",
            "Train Epoch: 3 [2688/21600 (12%)]\tLoss: 0.054352\n",
            "Train Epoch: 3 [2784/21600 (13%)]\tLoss: 0.018915\n",
            "Train Epoch: 3 [2880/21600 (13%)]\tLoss: 0.018252\n",
            "Train Epoch: 3 [2976/21600 (14%)]\tLoss: 0.026941\n",
            "Train Epoch: 3 [3072/21600 (14%)]\tLoss: 0.094674\n",
            "Train Epoch: 3 [3168/21600 (15%)]\tLoss: 0.041368\n",
            "Train Epoch: 3 [3264/21600 (15%)]\tLoss: 0.023272\n",
            "Train Epoch: 3 [3360/21600 (16%)]\tLoss: 0.007555\n",
            "Train Epoch: 3 [3456/21600 (16%)]\tLoss: 0.016243\n",
            "Train Epoch: 3 [3552/21600 (16%)]\tLoss: 0.046945\n",
            "Train Epoch: 3 [3648/21600 (17%)]\tLoss: 0.009137\n",
            "Train Epoch: 3 [3744/21600 (17%)]\tLoss: 0.012879\n",
            "Train Epoch: 3 [3840/21600 (18%)]\tLoss: 0.167146\n",
            "Train Epoch: 3 [3936/21600 (18%)]\tLoss: 0.020569\n",
            "Train Epoch: 3 [4032/21600 (19%)]\tLoss: 0.016540\n",
            "Train Epoch: 3 [4128/21600 (19%)]\tLoss: 0.008405\n",
            "Train Epoch: 3 [4224/21600 (20%)]\tLoss: 0.165475\n",
            "Train Epoch: 3 [4320/21600 (20%)]\tLoss: 0.003748\n",
            "Train Epoch: 3 [4416/21600 (20%)]\tLoss: 0.005593\n",
            "Train Epoch: 3 [4512/21600 (21%)]\tLoss: 0.005194\n",
            "Train Epoch: 3 [4608/21600 (21%)]\tLoss: 0.095997\n",
            "Train Epoch: 3 [4704/21600 (22%)]\tLoss: 0.029502\n",
            "Train Epoch: 3 [4800/21600 (22%)]\tLoss: 0.001182\n",
            "Train Epoch: 3 [4896/21600 (23%)]\tLoss: 0.014019\n",
            "Train Epoch: 3 [4992/21600 (23%)]\tLoss: 0.008893\n",
            "Train Epoch: 3 [5088/21600 (24%)]\tLoss: 0.002028\n",
            "Train Epoch: 3 [5184/21600 (24%)]\tLoss: 0.000711\n",
            "Train Epoch: 3 [5280/21600 (24%)]\tLoss: 0.037466\n",
            "Train Epoch: 3 [5376/21600 (25%)]\tLoss: 0.012498\n",
            "Train Epoch: 3 [5472/21600 (25%)]\tLoss: 0.003598\n",
            "Train Epoch: 3 [5568/21600 (26%)]\tLoss: 0.011064\n",
            "Train Epoch: 3 [5664/21600 (26%)]\tLoss: 0.004521\n",
            "Train Epoch: 3 [5760/21600 (27%)]\tLoss: 0.045019\n",
            "Train Epoch: 3 [5856/21600 (27%)]\tLoss: 0.007112\n",
            "Train Epoch: 3 [5952/21600 (28%)]\tLoss: 0.007118\n",
            "Train Epoch: 3 [6048/21600 (28%)]\tLoss: 0.036470\n",
            "Train Epoch: 3 [6144/21600 (28%)]\tLoss: 0.014896\n",
            "Train Epoch: 3 [6240/21600 (29%)]\tLoss: 0.098831\n",
            "Train Epoch: 3 [6336/21600 (29%)]\tLoss: 0.018039\n",
            "Train Epoch: 3 [6432/21600 (30%)]\tLoss: 0.004705\n",
            "Train Epoch: 3 [6528/21600 (30%)]\tLoss: 0.004763\n",
            "Train Epoch: 3 [6624/21600 (31%)]\tLoss: 0.001095\n",
            "Train Epoch: 3 [6720/21600 (31%)]\tLoss: 0.019595\n",
            "Train Epoch: 3 [6816/21600 (32%)]\tLoss: 0.002404\n",
            "Train Epoch: 3 [6912/21600 (32%)]\tLoss: 0.011457\n",
            "Train Epoch: 3 [7008/21600 (32%)]\tLoss: 0.008497\n",
            "Train Epoch: 3 [7104/21600 (33%)]\tLoss: 0.055813\n",
            "Train Epoch: 3 [7200/21600 (33%)]\tLoss: 0.086790\n",
            "Train Epoch: 3 [7296/21600 (34%)]\tLoss: 0.028136\n",
            "Train Epoch: 3 [7392/21600 (34%)]\tLoss: 0.004007\n",
            "Train Epoch: 3 [7488/21600 (35%)]\tLoss: 0.001342\n",
            "Train Epoch: 3 [7584/21600 (35%)]\tLoss: 0.016176\n",
            "Train Epoch: 3 [7680/21600 (36%)]\tLoss: 0.011714\n",
            "Train Epoch: 3 [7776/21600 (36%)]\tLoss: 0.005273\n",
            "Train Epoch: 3 [7872/21600 (36%)]\tLoss: 0.031375\n",
            "Train Epoch: 3 [7968/21600 (37%)]\tLoss: 0.004484\n",
            "Train Epoch: 3 [8064/21600 (37%)]\tLoss: 0.007588\n",
            "Train Epoch: 3 [8160/21600 (38%)]\tLoss: 0.009683\n",
            "Train Epoch: 3 [8256/21600 (38%)]\tLoss: 0.009542\n",
            "Train Epoch: 3 [8352/21600 (39%)]\tLoss: 0.086082\n",
            "Train Epoch: 3 [8448/21600 (39%)]\tLoss: 0.001993\n",
            "Train Epoch: 3 [8544/21600 (40%)]\tLoss: 0.000876\n",
            "Train Epoch: 3 [8640/21600 (40%)]\tLoss: 0.007332\n",
            "Train Epoch: 3 [8736/21600 (40%)]\tLoss: 0.072004\n",
            "Train Epoch: 3 [8832/21600 (41%)]\tLoss: 0.004159\n",
            "Train Epoch: 3 [8928/21600 (41%)]\tLoss: 0.061204\n",
            "Train Epoch: 3 [9024/21600 (42%)]\tLoss: 0.003094\n",
            "Train Epoch: 3 [9120/21600 (42%)]\tLoss: 0.004785\n",
            "Train Epoch: 3 [9216/21600 (43%)]\tLoss: 0.014660\n",
            "Train Epoch: 3 [9312/21600 (43%)]\tLoss: 0.001692\n",
            "Train Epoch: 3 [9408/21600 (44%)]\tLoss: 0.049650\n",
            "Train Epoch: 3 [9504/21600 (44%)]\tLoss: 0.010865\n",
            "Train Epoch: 3 [9600/21600 (44%)]\tLoss: 0.042891\n",
            "Train Epoch: 3 [9696/21600 (45%)]\tLoss: 0.009673\n",
            "Train Epoch: 3 [9792/21600 (45%)]\tLoss: 0.037465\n",
            "Train Epoch: 3 [9888/21600 (46%)]\tLoss: 0.008518\n",
            "Train Epoch: 3 [9984/21600 (46%)]\tLoss: 0.042222\n",
            "Train Epoch: 3 [10080/21600 (47%)]\tLoss: 0.013979\n",
            "Train Epoch: 3 [10176/21600 (47%)]\tLoss: 0.001233\n",
            "Train Epoch: 3 [10272/21600 (48%)]\tLoss: 0.035311\n",
            "Train Epoch: 3 [10368/21600 (48%)]\tLoss: 0.027013\n",
            "Train Epoch: 3 [10464/21600 (48%)]\tLoss: 0.005358\n",
            "Train Epoch: 3 [10560/21600 (49%)]\tLoss: 0.008775\n",
            "Train Epoch: 3 [10656/21600 (49%)]\tLoss: 0.006072\n",
            "Train Epoch: 3 [10752/21600 (50%)]\tLoss: 0.010290\n",
            "Train Epoch: 3 [10848/21600 (50%)]\tLoss: 0.120349\n",
            "Train Epoch: 3 [10944/21600 (51%)]\tLoss: 0.048004\n",
            "Train Epoch: 3 [11040/21600 (51%)]\tLoss: 0.002327\n",
            "Train Epoch: 3 [11136/21600 (52%)]\tLoss: 0.003853\n",
            "Train Epoch: 3 [11232/21600 (52%)]\tLoss: 0.061461\n",
            "Train Epoch: 3 [11328/21600 (52%)]\tLoss: 0.001077\n",
            "Train Epoch: 3 [11424/21600 (53%)]\tLoss: 0.325219\n",
            "Train Epoch: 3 [11520/21600 (53%)]\tLoss: 0.003338\n",
            "Train Epoch: 3 [11616/21600 (54%)]\tLoss: 0.001122\n",
            "Train Epoch: 3 [11712/21600 (54%)]\tLoss: 0.000550\n",
            "Train Epoch: 3 [11808/21600 (55%)]\tLoss: 0.001328\n",
            "Train Epoch: 3 [11904/21600 (55%)]\tLoss: 0.001835\n",
            "Train Epoch: 3 [12000/21600 (56%)]\tLoss: 0.089333\n",
            "Train Epoch: 3 [12096/21600 (56%)]\tLoss: 0.020804\n",
            "Train Epoch: 3 [12192/21600 (56%)]\tLoss: 0.010281\n",
            "Train Epoch: 3 [12288/21600 (57%)]\tLoss: 0.031043\n",
            "Train Epoch: 3 [12384/21600 (57%)]\tLoss: 0.012771\n",
            "Train Epoch: 3 [12480/21600 (58%)]\tLoss: 0.053460\n",
            "Train Epoch: 3 [12576/21600 (58%)]\tLoss: 0.000943\n",
            "Train Epoch: 3 [12672/21600 (59%)]\tLoss: 0.000559\n",
            "Train Epoch: 3 [12768/21600 (59%)]\tLoss: 0.001760\n",
            "Train Epoch: 3 [12864/21600 (60%)]\tLoss: 0.017674\n",
            "Train Epoch: 3 [12960/21600 (60%)]\tLoss: 0.098398\n",
            "Train Epoch: 3 [13056/21600 (60%)]\tLoss: 0.003638\n",
            "Train Epoch: 3 [13152/21600 (61%)]\tLoss: 0.021449\n",
            "Train Epoch: 3 [13248/21600 (61%)]\tLoss: 0.136892\n",
            "Train Epoch: 3 [13344/21600 (62%)]\tLoss: 0.002509\n",
            "Train Epoch: 3 [13440/21600 (62%)]\tLoss: 0.003456\n",
            "Train Epoch: 3 [13536/21600 (63%)]\tLoss: 0.017948\n",
            "Train Epoch: 3 [13632/21600 (63%)]\tLoss: 0.033727\n",
            "Train Epoch: 3 [13728/21600 (64%)]\tLoss: 0.016841\n",
            "Train Epoch: 3 [13824/21600 (64%)]\tLoss: 0.018484\n",
            "Train Epoch: 3 [13920/21600 (64%)]\tLoss: 0.001947\n",
            "Train Epoch: 3 [14016/21600 (65%)]\tLoss: 0.007464\n",
            "Train Epoch: 3 [14112/21600 (65%)]\tLoss: 0.015054\n",
            "Train Epoch: 3 [14208/21600 (66%)]\tLoss: 0.133607\n",
            "Train Epoch: 3 [14304/21600 (66%)]\tLoss: 0.027866\n",
            "Train Epoch: 3 [14400/21600 (67%)]\tLoss: 0.011747\n",
            "Train Epoch: 3 [14496/21600 (67%)]\tLoss: 0.015139\n",
            "Train Epoch: 3 [14592/21600 (68%)]\tLoss: 0.025074\n",
            "Train Epoch: 3 [14688/21600 (68%)]\tLoss: 0.062942\n",
            "Train Epoch: 3 [14784/21600 (68%)]\tLoss: 0.003158\n",
            "Train Epoch: 3 [14880/21600 (69%)]\tLoss: 0.005137\n",
            "Train Epoch: 3 [14976/21600 (69%)]\tLoss: 0.119888\n",
            "Train Epoch: 3 [15072/21600 (70%)]\tLoss: 0.007721\n",
            "Train Epoch: 3 [15168/21600 (70%)]\tLoss: 0.003664\n",
            "Train Epoch: 3 [15264/21600 (71%)]\tLoss: 0.012624\n",
            "Train Epoch: 3 [15360/21600 (71%)]\tLoss: 0.006115\n",
            "Train Epoch: 3 [15456/21600 (72%)]\tLoss: 0.013220\n",
            "Train Epoch: 3 [15552/21600 (72%)]\tLoss: 0.021995\n",
            "Train Epoch: 3 [15648/21600 (72%)]\tLoss: 0.007260\n",
            "Train Epoch: 3 [15744/21600 (73%)]\tLoss: 0.026542\n",
            "Train Epoch: 3 [15840/21600 (73%)]\tLoss: 0.112519\n",
            "Train Epoch: 3 [15936/21600 (74%)]\tLoss: 0.048296\n",
            "Train Epoch: 3 [16032/21600 (74%)]\tLoss: 0.003117\n",
            "Train Epoch: 3 [16128/21600 (75%)]\tLoss: 0.074054\n",
            "Train Epoch: 3 [16224/21600 (75%)]\tLoss: 0.001057\n",
            "Train Epoch: 3 [16320/21600 (76%)]\tLoss: 0.114687\n",
            "Train Epoch: 3 [16416/21600 (76%)]\tLoss: 0.013868\n",
            "Train Epoch: 3 [16512/21600 (76%)]\tLoss: 0.018774\n",
            "Train Epoch: 3 [16608/21600 (77%)]\tLoss: 0.003379\n",
            "Train Epoch: 3 [16704/21600 (77%)]\tLoss: 0.012121\n",
            "Train Epoch: 3 [16800/21600 (78%)]\tLoss: 0.038945\n",
            "Train Epoch: 3 [16896/21600 (78%)]\tLoss: 0.018762\n",
            "Train Epoch: 3 [16992/21600 (79%)]\tLoss: 0.002829\n",
            "Train Epoch: 3 [17088/21600 (79%)]\tLoss: 0.006260\n",
            "Train Epoch: 3 [17184/21600 (80%)]\tLoss: 0.011611\n",
            "Train Epoch: 3 [17280/21600 (80%)]\tLoss: 0.003211\n",
            "Train Epoch: 3 [17376/21600 (80%)]\tLoss: 0.005974\n",
            "Train Epoch: 3 [17472/21600 (81%)]\tLoss: 0.018187\n",
            "Train Epoch: 3 [17568/21600 (81%)]\tLoss: 0.035857\n",
            "Train Epoch: 3 [17664/21600 (82%)]\tLoss: 0.006934\n",
            "Train Epoch: 3 [17760/21600 (82%)]\tLoss: 0.029169\n",
            "Train Epoch: 3 [17856/21600 (83%)]\tLoss: 0.022935\n",
            "Train Epoch: 3 [17952/21600 (83%)]\tLoss: 0.071683\n",
            "Train Epoch: 3 [18048/21600 (84%)]\tLoss: 0.001979\n",
            "Train Epoch: 3 [18144/21600 (84%)]\tLoss: 0.008690\n",
            "Train Epoch: 3 [18240/21600 (84%)]\tLoss: 0.002904\n",
            "Train Epoch: 3 [18336/21600 (85%)]\tLoss: 0.011847\n",
            "Train Epoch: 3 [18432/21600 (85%)]\tLoss: 0.043783\n",
            "Train Epoch: 3 [18528/21600 (86%)]\tLoss: 0.004787\n",
            "Train Epoch: 3 [18624/21600 (86%)]\tLoss: 0.022559\n",
            "Train Epoch: 3 [18720/21600 (87%)]\tLoss: 0.007459\n",
            "Train Epoch: 3 [18816/21600 (87%)]\tLoss: 0.021364\n",
            "Train Epoch: 3 [18912/21600 (88%)]\tLoss: 0.001025\n",
            "Train Epoch: 3 [19008/21600 (88%)]\tLoss: 0.058360\n",
            "Train Epoch: 3 [19104/21600 (88%)]\tLoss: 0.140409\n",
            "Train Epoch: 3 [19200/21600 (89%)]\tLoss: 0.001923\n",
            "Train Epoch: 3 [19296/21600 (89%)]\tLoss: 0.005467\n",
            "Train Epoch: 3 [19392/21600 (90%)]\tLoss: 0.004872\n",
            "Train Epoch: 3 [19488/21600 (90%)]\tLoss: 0.002227\n",
            "Train Epoch: 3 [19584/21600 (91%)]\tLoss: 0.019580\n",
            "Train Epoch: 3 [19680/21600 (91%)]\tLoss: 0.000933\n",
            "Train Epoch: 3 [19776/21600 (92%)]\tLoss: 0.018590\n",
            "Train Epoch: 3 [19872/21600 (92%)]\tLoss: 0.024707\n",
            "Train Epoch: 3 [19968/21600 (92%)]\tLoss: 0.010512\n",
            "Train Epoch: 3 [20064/21600 (93%)]\tLoss: 0.002171\n",
            "Train Epoch: 3 [20160/21600 (93%)]\tLoss: 0.002190\n",
            "Train Epoch: 3 [20256/21600 (94%)]\tLoss: 0.019649\n",
            "Train Epoch: 3 [20352/21600 (94%)]\tLoss: 0.003798\n",
            "Train Epoch: 3 [20448/21600 (95%)]\tLoss: 0.005950\n",
            "Train Epoch: 3 [20544/21600 (95%)]\tLoss: 0.004859\n",
            "Train Epoch: 3 [20640/21600 (96%)]\tLoss: 0.017150\n",
            "Train Epoch: 3 [20736/21600 (96%)]\tLoss: 0.038465\n",
            "Train Epoch: 3 [20832/21600 (96%)]\tLoss: 0.005319\n",
            "Train Epoch: 3 [20928/21600 (97%)]\tLoss: 0.164546\n",
            "Train Epoch: 3 [21024/21600 (97%)]\tLoss: 0.015895\n",
            "Train Epoch: 3 [21120/21600 (98%)]\tLoss: 0.097993\n",
            "Train Epoch: 3 [21216/21600 (98%)]\tLoss: 0.004476\n",
            "Train Epoch: 3 [21312/21600 (99%)]\tLoss: 0.002130\n",
            "Train Epoch: 3 [21408/21600 (99%)]\tLoss: 0.004364\n",
            "Train Epoch: 3 [21504/21600 (100%)]\tLoss: 0.013983\n",
            "\n",
            "Validation set: Average loss: 0.1007, Accuracy: 5223/5400 (97%)\n",
            "\n",
            "\n",
            "Saved model to model_binary_3.pth.\n",
            "Train Epoch: 4 [0/21600 (0%)]\tLoss: 0.009738\n",
            "Train Epoch: 4 [96/21600 (0%)]\tLoss: 0.040488\n",
            "Train Epoch: 4 [192/21600 (1%)]\tLoss: 0.008804\n",
            "Train Epoch: 4 [288/21600 (1%)]\tLoss: 0.009883\n",
            "Train Epoch: 4 [384/21600 (2%)]\tLoss: 0.018659\n",
            "Train Epoch: 4 [480/21600 (2%)]\tLoss: 0.001290\n",
            "Train Epoch: 4 [576/21600 (3%)]\tLoss: 0.077037\n",
            "Train Epoch: 4 [672/21600 (3%)]\tLoss: 0.079463\n",
            "Train Epoch: 4 [768/21600 (4%)]\tLoss: 0.002140\n",
            "Train Epoch: 4 [864/21600 (4%)]\tLoss: 0.079291\n",
            "Train Epoch: 4 [960/21600 (4%)]\tLoss: 0.014948\n",
            "Train Epoch: 4 [1056/21600 (5%)]\tLoss: 0.001243\n",
            "Train Epoch: 4 [1152/21600 (5%)]\tLoss: 0.002304\n",
            "Train Epoch: 4 [1248/21600 (6%)]\tLoss: 0.004473\n",
            "Train Epoch: 4 [1344/21600 (6%)]\tLoss: 0.071600\n",
            "Train Epoch: 4 [1440/21600 (7%)]\tLoss: 0.016662\n",
            "Train Epoch: 4 [1536/21600 (7%)]\tLoss: 0.000940\n",
            "Train Epoch: 4 [1632/21600 (8%)]\tLoss: 0.014332\n",
            "Train Epoch: 4 [1728/21600 (8%)]\tLoss: 0.108113\n",
            "Train Epoch: 4 [1824/21600 (8%)]\tLoss: 0.000966\n",
            "Train Epoch: 4 [1920/21600 (9%)]\tLoss: 0.044328\n",
            "Train Epoch: 4 [2016/21600 (9%)]\tLoss: 0.013594\n",
            "Train Epoch: 4 [2112/21600 (10%)]\tLoss: 0.007116\n",
            "Train Epoch: 4 [2208/21600 (10%)]\tLoss: 0.003195\n",
            "Train Epoch: 4 [2304/21600 (11%)]\tLoss: 0.004532\n",
            "Train Epoch: 4 [2400/21600 (11%)]\tLoss: 0.016156\n",
            "Train Epoch: 4 [2496/21600 (12%)]\tLoss: 0.007058\n",
            "Train Epoch: 4 [2592/21600 (12%)]\tLoss: 0.001062\n",
            "Train Epoch: 4 [2688/21600 (12%)]\tLoss: 0.009136\n",
            "Train Epoch: 4 [2784/21600 (13%)]\tLoss: 0.002456\n",
            "Train Epoch: 4 [2880/21600 (13%)]\tLoss: 0.000978\n",
            "Train Epoch: 4 [2976/21600 (14%)]\tLoss: 0.010197\n",
            "Train Epoch: 4 [3072/21600 (14%)]\tLoss: 0.010702\n",
            "Train Epoch: 4 [3168/21600 (15%)]\tLoss: 0.001597\n",
            "Train Epoch: 4 [3264/21600 (15%)]\tLoss: 0.005929\n",
            "Train Epoch: 4 [3360/21600 (16%)]\tLoss: 0.099408\n",
            "Train Epoch: 4 [3456/21600 (16%)]\tLoss: 0.000801\n",
            "Train Epoch: 4 [3552/21600 (16%)]\tLoss: 0.001488\n",
            "Train Epoch: 4 [3648/21600 (17%)]\tLoss: 0.079170\n",
            "Train Epoch: 4 [3744/21600 (17%)]\tLoss: 0.009894\n",
            "Train Epoch: 4 [3840/21600 (18%)]\tLoss: 0.000879\n",
            "Train Epoch: 4 [3936/21600 (18%)]\tLoss: 0.002530\n",
            "Train Epoch: 4 [4032/21600 (19%)]\tLoss: 0.016489\n",
            "Train Epoch: 4 [4128/21600 (19%)]\tLoss: 0.006781\n",
            "Train Epoch: 4 [4224/21600 (20%)]\tLoss: 0.013938\n",
            "Train Epoch: 4 [4320/21600 (20%)]\tLoss: 0.021641\n",
            "Train Epoch: 4 [4416/21600 (20%)]\tLoss: 0.000790\n",
            "Train Epoch: 4 [4512/21600 (21%)]\tLoss: 0.017630\n",
            "Train Epoch: 4 [4608/21600 (21%)]\tLoss: 0.004096\n",
            "Train Epoch: 4 [4704/21600 (22%)]\tLoss: 0.007399\n",
            "Train Epoch: 4 [4800/21600 (22%)]\tLoss: 0.057476\n",
            "Train Epoch: 4 [4896/21600 (23%)]\tLoss: 0.006612\n",
            "Train Epoch: 4 [4992/21600 (23%)]\tLoss: 0.012661\n",
            "Train Epoch: 4 [5088/21600 (24%)]\tLoss: 0.007456\n",
            "Train Epoch: 4 [5184/21600 (24%)]\tLoss: 0.004524\n",
            "Train Epoch: 4 [5280/21600 (24%)]\tLoss: 0.008486\n",
            "Train Epoch: 4 [5376/21600 (25%)]\tLoss: 0.007992\n",
            "Train Epoch: 4 [5472/21600 (25%)]\tLoss: 0.001391\n",
            "Train Epoch: 4 [5568/21600 (26%)]\tLoss: 0.002591\n",
            "Train Epoch: 4 [5664/21600 (26%)]\tLoss: 0.014021\n",
            "Train Epoch: 4 [5760/21600 (27%)]\tLoss: 0.039365\n",
            "Train Epoch: 4 [5856/21600 (27%)]\tLoss: 0.003151\n",
            "Train Epoch: 4 [5952/21600 (28%)]\tLoss: 0.016012\n",
            "Train Epoch: 4 [6048/21600 (28%)]\tLoss: 0.009450\n",
            "Train Epoch: 4 [6144/21600 (28%)]\tLoss: 0.003348\n",
            "Train Epoch: 4 [6240/21600 (29%)]\tLoss: 0.206586\n",
            "Train Epoch: 4 [6336/21600 (29%)]\tLoss: 0.003480\n",
            "Train Epoch: 4 [6432/21600 (30%)]\tLoss: 0.011692\n",
            "Train Epoch: 4 [6528/21600 (30%)]\tLoss: 0.022087\n",
            "Train Epoch: 4 [6624/21600 (31%)]\tLoss: 0.036481\n",
            "Train Epoch: 4 [6720/21600 (31%)]\tLoss: 0.085381\n",
            "Train Epoch: 4 [6816/21600 (32%)]\tLoss: 0.008409\n",
            "Train Epoch: 4 [6912/21600 (32%)]\tLoss: 0.055929\n",
            "Train Epoch: 4 [7008/21600 (32%)]\tLoss: 0.044836\n",
            "Train Epoch: 4 [7104/21600 (33%)]\tLoss: 0.010062\n",
            "Train Epoch: 4 [7200/21600 (33%)]\tLoss: 0.006029\n",
            "Train Epoch: 4 [7296/21600 (34%)]\tLoss: 0.003240\n",
            "Train Epoch: 4 [7392/21600 (34%)]\tLoss: 0.020743\n",
            "Train Epoch: 4 [7488/21600 (35%)]\tLoss: 0.007233\n",
            "Train Epoch: 4 [7584/21600 (35%)]\tLoss: 0.007920\n",
            "Train Epoch: 4 [7680/21600 (36%)]\tLoss: 0.012178\n",
            "Train Epoch: 4 [7776/21600 (36%)]\tLoss: 0.008691\n",
            "Train Epoch: 4 [7872/21600 (36%)]\tLoss: 0.001181\n",
            "Train Epoch: 4 [7968/21600 (37%)]\tLoss: 0.038958\n",
            "Train Epoch: 4 [8064/21600 (37%)]\tLoss: 0.001988\n",
            "Train Epoch: 4 [8160/21600 (38%)]\tLoss: 0.002899\n",
            "Train Epoch: 4 [8256/21600 (38%)]\tLoss: 0.002354\n",
            "Train Epoch: 4 [8352/21600 (39%)]\tLoss: 0.006023\n",
            "Train Epoch: 4 [8448/21600 (39%)]\tLoss: 0.043846\n",
            "Train Epoch: 4 [8544/21600 (40%)]\tLoss: 0.019279\n",
            "Train Epoch: 4 [8640/21600 (40%)]\tLoss: 0.001024\n",
            "Train Epoch: 4 [8736/21600 (40%)]\tLoss: 0.022418\n",
            "Train Epoch: 4 [8832/21600 (41%)]\tLoss: 0.001816\n",
            "Train Epoch: 4 [8928/21600 (41%)]\tLoss: 0.001490\n",
            "Train Epoch: 4 [9024/21600 (42%)]\tLoss: 0.001805\n",
            "Train Epoch: 4 [9120/21600 (42%)]\tLoss: 0.004550\n",
            "Train Epoch: 4 [9216/21600 (43%)]\tLoss: 0.001336\n",
            "Train Epoch: 4 [9312/21600 (43%)]\tLoss: 0.012456\n",
            "Train Epoch: 4 [9408/21600 (44%)]\tLoss: 0.005973\n",
            "Train Epoch: 4 [9504/21600 (44%)]\tLoss: 0.000807\n",
            "Train Epoch: 4 [9600/21600 (44%)]\tLoss: 0.025873\n",
            "Train Epoch: 4 [9696/21600 (45%)]\tLoss: 0.001448\n",
            "Train Epoch: 4 [9792/21600 (45%)]\tLoss: 0.000677\n",
            "Train Epoch: 4 [9888/21600 (46%)]\tLoss: 0.000454\n",
            "Train Epoch: 4 [9984/21600 (46%)]\tLoss: 0.080952\n",
            "Train Epoch: 4 [10080/21600 (47%)]\tLoss: 0.028354\n",
            "Train Epoch: 4 [10176/21600 (47%)]\tLoss: 0.003075\n",
            "Train Epoch: 4 [10272/21600 (48%)]\tLoss: 0.017201\n",
            "Train Epoch: 4 [10368/21600 (48%)]\tLoss: 0.003519\n",
            "Train Epoch: 4 [10464/21600 (48%)]\tLoss: 0.012368\n",
            "Train Epoch: 4 [10560/21600 (49%)]\tLoss: 0.007684\n",
            "Train Epoch: 4 [10656/21600 (49%)]\tLoss: 0.141468\n",
            "Train Epoch: 4 [10752/21600 (50%)]\tLoss: 0.002079\n",
            "Train Epoch: 4 [10848/21600 (50%)]\tLoss: 0.057608\n",
            "Train Epoch: 4 [10944/21600 (51%)]\tLoss: 0.027016\n",
            "Train Epoch: 4 [11040/21600 (51%)]\tLoss: 0.001960\n",
            "Train Epoch: 4 [11136/21600 (52%)]\tLoss: 0.019215\n",
            "Train Epoch: 4 [11232/21600 (52%)]\tLoss: 0.020543\n",
            "Train Epoch: 4 [11328/21600 (52%)]\tLoss: 0.002835\n",
            "Train Epoch: 4 [11424/21600 (53%)]\tLoss: 0.026000\n",
            "Train Epoch: 4 [11520/21600 (53%)]\tLoss: 0.005189\n",
            "Train Epoch: 4 [11616/21600 (54%)]\tLoss: 0.000251\n",
            "Train Epoch: 4 [11712/21600 (54%)]\tLoss: 0.001921\n",
            "Train Epoch: 4 [11808/21600 (55%)]\tLoss: 0.000871\n",
            "Train Epoch: 4 [11904/21600 (55%)]\tLoss: 0.002555\n",
            "Train Epoch: 4 [12000/21600 (56%)]\tLoss: 0.018790\n",
            "Train Epoch: 4 [12096/21600 (56%)]\tLoss: 0.000108\n",
            "Train Epoch: 4 [12192/21600 (56%)]\tLoss: 0.017208\n",
            "Train Epoch: 4 [12288/21600 (57%)]\tLoss: 0.014969\n",
            "Train Epoch: 4 [12384/21600 (57%)]\tLoss: 0.002117\n",
            "Train Epoch: 4 [12480/21600 (58%)]\tLoss: 0.010748\n",
            "Train Epoch: 4 [12576/21600 (58%)]\tLoss: 0.013789\n",
            "Train Epoch: 4 [12672/21600 (59%)]\tLoss: 0.002365\n",
            "Train Epoch: 4 [12768/21600 (59%)]\tLoss: 0.004960\n",
            "Train Epoch: 4 [12864/21600 (60%)]\tLoss: 0.006719\n",
            "Train Epoch: 4 [12960/21600 (60%)]\tLoss: 0.002992\n",
            "Train Epoch: 4 [13056/21600 (60%)]\tLoss: 0.001393\n",
            "Train Epoch: 4 [13152/21600 (61%)]\tLoss: 0.004258\n",
            "Train Epoch: 4 [13248/21600 (61%)]\tLoss: 0.199683\n",
            "Train Epoch: 4 [13344/21600 (62%)]\tLoss: 0.007459\n",
            "Train Epoch: 4 [13440/21600 (62%)]\tLoss: 0.001541\n",
            "Train Epoch: 4 [13536/21600 (63%)]\tLoss: 0.001742\n",
            "Train Epoch: 4 [13632/21600 (63%)]\tLoss: 0.006629\n",
            "Train Epoch: 4 [13728/21600 (64%)]\tLoss: 0.004337\n",
            "Train Epoch: 4 [13824/21600 (64%)]\tLoss: 0.020989\n",
            "Train Epoch: 4 [13920/21600 (64%)]\tLoss: 0.006433\n",
            "Train Epoch: 4 [14016/21600 (65%)]\tLoss: 0.058022\n",
            "Train Epoch: 4 [14112/21600 (65%)]\tLoss: 0.008089\n",
            "Train Epoch: 4 [14208/21600 (66%)]\tLoss: 0.002682\n",
            "Train Epoch: 4 [14304/21600 (66%)]\tLoss: 0.029570\n",
            "Train Epoch: 4 [14400/21600 (67%)]\tLoss: 0.000677\n",
            "Train Epoch: 4 [14496/21600 (67%)]\tLoss: 0.033905\n",
            "Train Epoch: 4 [14592/21600 (68%)]\tLoss: 0.010524\n",
            "Train Epoch: 4 [14688/21600 (68%)]\tLoss: 0.004921\n",
            "Train Epoch: 4 [14784/21600 (68%)]\tLoss: 0.006958\n",
            "Train Epoch: 4 [14880/21600 (69%)]\tLoss: 0.001911\n",
            "Train Epoch: 4 [14976/21600 (69%)]\tLoss: 0.007795\n",
            "Train Epoch: 4 [15072/21600 (70%)]\tLoss: 0.002228\n",
            "Train Epoch: 4 [15168/21600 (70%)]\tLoss: 0.030856\n",
            "Train Epoch: 4 [15264/21600 (71%)]\tLoss: 0.058776\n",
            "Train Epoch: 4 [15360/21600 (71%)]\tLoss: 0.043261\n",
            "Train Epoch: 4 [15456/21600 (72%)]\tLoss: 0.000788\n",
            "Train Epoch: 4 [15552/21600 (72%)]\tLoss: 0.002150\n",
            "Train Epoch: 4 [15648/21600 (72%)]\tLoss: 0.060887\n",
            "Train Epoch: 4 [15744/21600 (73%)]\tLoss: 0.012897\n",
            "Train Epoch: 4 [15840/21600 (73%)]\tLoss: 0.001508\n",
            "Train Epoch: 4 [15936/21600 (74%)]\tLoss: 0.011565\n",
            "Train Epoch: 4 [16032/21600 (74%)]\tLoss: 0.004720\n",
            "Train Epoch: 4 [16128/21600 (75%)]\tLoss: 0.027928\n",
            "Train Epoch: 4 [16224/21600 (75%)]\tLoss: 0.010534\n",
            "Train Epoch: 4 [16320/21600 (76%)]\tLoss: 0.010042\n",
            "Train Epoch: 4 [16416/21600 (76%)]\tLoss: 0.001279\n",
            "Train Epoch: 4 [16512/21600 (76%)]\tLoss: 0.000737\n",
            "Train Epoch: 4 [16608/21600 (77%)]\tLoss: 0.004012\n",
            "Train Epoch: 4 [16704/21600 (77%)]\tLoss: 0.005753\n",
            "Train Epoch: 4 [16800/21600 (78%)]\tLoss: 0.172598\n",
            "Train Epoch: 4 [16896/21600 (78%)]\tLoss: 0.000653\n",
            "Train Epoch: 4 [16992/21600 (79%)]\tLoss: 0.002358\n",
            "Train Epoch: 4 [17088/21600 (79%)]\tLoss: 0.000905\n",
            "Train Epoch: 4 [17184/21600 (80%)]\tLoss: 0.001035\n",
            "Train Epoch: 4 [17280/21600 (80%)]\tLoss: 0.001390\n",
            "Train Epoch: 4 [17376/21600 (80%)]\tLoss: 0.006603\n",
            "Train Epoch: 4 [17472/21600 (81%)]\tLoss: 0.067302\n",
            "Train Epoch: 4 [17568/21600 (81%)]\tLoss: 0.030078\n",
            "Train Epoch: 4 [17664/21600 (82%)]\tLoss: 0.002875\n",
            "Train Epoch: 4 [17760/21600 (82%)]\tLoss: 0.091693\n",
            "Train Epoch: 4 [17856/21600 (83%)]\tLoss: 0.028084\n",
            "Train Epoch: 4 [17952/21600 (83%)]\tLoss: 0.002289\n",
            "Train Epoch: 4 [18048/21600 (84%)]\tLoss: 0.003222\n",
            "Train Epoch: 4 [18144/21600 (84%)]\tLoss: 0.057756\n",
            "Train Epoch: 4 [18240/21600 (84%)]\tLoss: 0.003279\n",
            "Train Epoch: 4 [18336/21600 (85%)]\tLoss: 0.004526\n",
            "Train Epoch: 4 [18432/21600 (85%)]\tLoss: 0.000251\n",
            "Train Epoch: 4 [18528/21600 (86%)]\tLoss: 0.053520\n",
            "Train Epoch: 4 [18624/21600 (86%)]\tLoss: 0.001347\n",
            "Train Epoch: 4 [18720/21600 (87%)]\tLoss: 0.004922\n",
            "Train Epoch: 4 [18816/21600 (87%)]\tLoss: 0.000243\n",
            "Train Epoch: 4 [18912/21600 (88%)]\tLoss: 0.017147\n",
            "Train Epoch: 4 [19008/21600 (88%)]\tLoss: 0.003149\n",
            "Train Epoch: 4 [19104/21600 (88%)]\tLoss: 0.001751\n",
            "Train Epoch: 4 [19200/21600 (89%)]\tLoss: 0.004212\n",
            "Train Epoch: 4 [19296/21600 (89%)]\tLoss: 0.142810\n",
            "Train Epoch: 4 [19392/21600 (90%)]\tLoss: 0.007158\n",
            "Train Epoch: 4 [19488/21600 (90%)]\tLoss: 0.001831\n",
            "Train Epoch: 4 [19584/21600 (91%)]\tLoss: 0.009449\n",
            "Train Epoch: 4 [19680/21600 (91%)]\tLoss: 0.001455\n",
            "Train Epoch: 4 [19776/21600 (92%)]\tLoss: 0.036938\n",
            "Train Epoch: 4 [19872/21600 (92%)]\tLoss: 0.010330\n",
            "Train Epoch: 4 [19968/21600 (92%)]\tLoss: 0.000911\n",
            "Train Epoch: 4 [20064/21600 (93%)]\tLoss: 0.000792\n",
            "Train Epoch: 4 [20160/21600 (93%)]\tLoss: 0.011680\n",
            "Train Epoch: 4 [20256/21600 (94%)]\tLoss: 0.002294\n",
            "Train Epoch: 4 [20352/21600 (94%)]\tLoss: 0.012534\n",
            "Train Epoch: 4 [20448/21600 (95%)]\tLoss: 0.005064\n",
            "Train Epoch: 4 [20544/21600 (95%)]\tLoss: 0.074544\n",
            "Train Epoch: 4 [20640/21600 (96%)]\tLoss: 0.000220\n",
            "Train Epoch: 4 [20736/21600 (96%)]\tLoss: 0.005251\n",
            "Train Epoch: 4 [20832/21600 (96%)]\tLoss: 0.000787\n",
            "Train Epoch: 4 [20928/21600 (97%)]\tLoss: 0.012514\n",
            "Train Epoch: 4 [21024/21600 (97%)]\tLoss: 0.097430\n",
            "Train Epoch: 4 [21120/21600 (98%)]\tLoss: 0.001220\n",
            "Train Epoch: 4 [21216/21600 (98%)]\tLoss: 0.000725\n",
            "Train Epoch: 4 [21312/21600 (99%)]\tLoss: 0.001210\n",
            "Train Epoch: 4 [21408/21600 (99%)]\tLoss: 0.009896\n",
            "Train Epoch: 4 [21504/21600 (100%)]\tLoss: 0.004150\n",
            "\n",
            "Validation set: Average loss: 0.0621, Accuracy: 5266/5400 (98%)\n",
            "\n",
            "\n",
            "Saved model to model_binary_4.pth.\n",
            "Train Epoch: 5 [0/21600 (0%)]\tLoss: 0.011519\n",
            "Train Epoch: 5 [96/21600 (0%)]\tLoss: 0.013846\n",
            "Train Epoch: 5 [192/21600 (1%)]\tLoss: 0.003011\n",
            "Train Epoch: 5 [288/21600 (1%)]\tLoss: 0.008385\n",
            "Train Epoch: 5 [384/21600 (2%)]\tLoss: 0.005522\n",
            "Train Epoch: 5 [480/21600 (2%)]\tLoss: 0.019230\n",
            "Train Epoch: 5 [576/21600 (3%)]\tLoss: 0.001294\n",
            "Train Epoch: 5 [672/21600 (3%)]\tLoss: 0.001047\n",
            "Train Epoch: 5 [768/21600 (4%)]\tLoss: 0.075547\n",
            "Train Epoch: 5 [864/21600 (4%)]\tLoss: 0.001043\n",
            "Train Epoch: 5 [960/21600 (4%)]\tLoss: 0.030800\n",
            "Train Epoch: 5 [1056/21600 (5%)]\tLoss: 0.022097\n",
            "Train Epoch: 5 [1152/21600 (5%)]\tLoss: 0.003765\n",
            "Train Epoch: 5 [1248/21600 (6%)]\tLoss: 0.002955\n",
            "Train Epoch: 5 [1344/21600 (6%)]\tLoss: 0.117987\n",
            "Train Epoch: 5 [1440/21600 (7%)]\tLoss: 0.000657\n",
            "Train Epoch: 5 [1536/21600 (7%)]\tLoss: 0.001429\n",
            "Train Epoch: 5 [1632/21600 (8%)]\tLoss: 0.001300\n",
            "Train Epoch: 5 [1728/21600 (8%)]\tLoss: 0.009756\n",
            "Train Epoch: 5 [1824/21600 (8%)]\tLoss: 0.049949\n",
            "Train Epoch: 5 [1920/21600 (9%)]\tLoss: 0.004646\n",
            "Train Epoch: 5 [2016/21600 (9%)]\tLoss: 0.005300\n",
            "Train Epoch: 5 [2112/21600 (10%)]\tLoss: 0.009064\n",
            "Train Epoch: 5 [2208/21600 (10%)]\tLoss: 0.013216\n",
            "Train Epoch: 5 [2304/21600 (11%)]\tLoss: 0.088755\n",
            "Train Epoch: 5 [2400/21600 (11%)]\tLoss: 0.002791\n",
            "Train Epoch: 5 [2496/21600 (12%)]\tLoss: 0.001450\n",
            "Train Epoch: 5 [2592/21600 (12%)]\tLoss: 0.003187\n",
            "Train Epoch: 5 [2688/21600 (12%)]\tLoss: 0.014365\n",
            "Train Epoch: 5 [2784/21600 (13%)]\tLoss: 0.005827\n",
            "Train Epoch: 5 [2880/21600 (13%)]\tLoss: 0.160493\n",
            "Train Epoch: 5 [2976/21600 (14%)]\tLoss: 0.005212\n",
            "Train Epoch: 5 [3072/21600 (14%)]\tLoss: 0.000542\n",
            "Train Epoch: 5 [3168/21600 (15%)]\tLoss: 0.000715\n",
            "Train Epoch: 5 [3264/21600 (15%)]\tLoss: 0.036331\n",
            "Train Epoch: 5 [3360/21600 (16%)]\tLoss: 0.005233\n",
            "Train Epoch: 5 [3456/21600 (16%)]\tLoss: 0.020885\n",
            "Train Epoch: 5 [3552/21600 (16%)]\tLoss: 0.018943\n",
            "Train Epoch: 5 [3648/21600 (17%)]\tLoss: 0.001466\n",
            "Train Epoch: 5 [3744/21600 (17%)]\tLoss: 0.153628\n",
            "Train Epoch: 5 [3840/21600 (18%)]\tLoss: 0.002986\n",
            "Train Epoch: 5 [3936/21600 (18%)]\tLoss: 0.028513\n",
            "Train Epoch: 5 [4032/21600 (19%)]\tLoss: 0.003675\n",
            "Train Epoch: 5 [4128/21600 (19%)]\tLoss: 0.001974\n",
            "Train Epoch: 5 [4224/21600 (20%)]\tLoss: 0.039192\n",
            "Train Epoch: 5 [4320/21600 (20%)]\tLoss: 0.001118\n",
            "Train Epoch: 5 [4416/21600 (20%)]\tLoss: 0.000384\n",
            "Train Epoch: 5 [4512/21600 (21%)]\tLoss: 0.003057\n",
            "Train Epoch: 5 [4608/21600 (21%)]\tLoss: 0.000864\n",
            "Train Epoch: 5 [4704/21600 (22%)]\tLoss: 0.001066\n",
            "Train Epoch: 5 [4800/21600 (22%)]\tLoss: 0.001216\n",
            "Train Epoch: 5 [4896/21600 (23%)]\tLoss: 0.004905\n",
            "Train Epoch: 5 [4992/21600 (23%)]\tLoss: 0.004856\n",
            "Train Epoch: 5 [5088/21600 (24%)]\tLoss: 0.002796\n",
            "Train Epoch: 5 [5184/21600 (24%)]\tLoss: 0.007117\n",
            "Train Epoch: 5 [5280/21600 (24%)]\tLoss: 0.000611\n",
            "Train Epoch: 5 [5376/21600 (25%)]\tLoss: 0.005320\n",
            "Train Epoch: 5 [5472/21600 (25%)]\tLoss: 0.051046\n",
            "Train Epoch: 5 [5568/21600 (26%)]\tLoss: 0.008373\n",
            "Train Epoch: 5 [5664/21600 (26%)]\tLoss: 0.228489\n",
            "Train Epoch: 5 [5760/21600 (27%)]\tLoss: 0.002109\n",
            "Train Epoch: 5 [5856/21600 (27%)]\tLoss: 0.010505\n",
            "Train Epoch: 5 [5952/21600 (28%)]\tLoss: 0.015674\n",
            "Train Epoch: 5 [6048/21600 (28%)]\tLoss: 0.001982\n",
            "Train Epoch: 5 [6144/21600 (28%)]\tLoss: 0.000619\n",
            "Train Epoch: 5 [6240/21600 (29%)]\tLoss: 0.000582\n",
            "Train Epoch: 5 [6336/21600 (29%)]\tLoss: 0.137439\n",
            "Train Epoch: 5 [6432/21600 (30%)]\tLoss: 0.021183\n",
            "Train Epoch: 5 [6528/21600 (30%)]\tLoss: 0.020299\n",
            "Train Epoch: 5 [6624/21600 (31%)]\tLoss: 0.027789\n",
            "Train Epoch: 5 [6720/21600 (31%)]\tLoss: 0.209054\n",
            "Train Epoch: 5 [6816/21600 (32%)]\tLoss: 0.000507\n",
            "Train Epoch: 5 [6912/21600 (32%)]\tLoss: 0.006056\n",
            "Train Epoch: 5 [7008/21600 (32%)]\tLoss: 0.119738\n",
            "Train Epoch: 5 [7104/21600 (33%)]\tLoss: 0.014783\n",
            "Train Epoch: 5 [7200/21600 (33%)]\tLoss: 0.106218\n",
            "Train Epoch: 5 [7296/21600 (34%)]\tLoss: 0.002970\n",
            "Train Epoch: 5 [7392/21600 (34%)]\tLoss: 0.039410\n",
            "Train Epoch: 5 [7488/21600 (35%)]\tLoss: 0.015093\n",
            "Train Epoch: 5 [7584/21600 (35%)]\tLoss: 0.015080\n",
            "Train Epoch: 5 [7680/21600 (36%)]\tLoss: 0.001664\n",
            "Train Epoch: 5 [7776/21600 (36%)]\tLoss: 0.001609\n",
            "Train Epoch: 5 [7872/21600 (36%)]\tLoss: 0.002618\n",
            "Train Epoch: 5 [7968/21600 (37%)]\tLoss: 0.046152\n",
            "Train Epoch: 5 [8064/21600 (37%)]\tLoss: 0.007968\n",
            "Train Epoch: 5 [8160/21600 (38%)]\tLoss: 0.033649\n",
            "Train Epoch: 5 [8256/21600 (38%)]\tLoss: 0.009561\n",
            "Train Epoch: 5 [8352/21600 (39%)]\tLoss: 0.002589\n",
            "Train Epoch: 5 [8448/21600 (39%)]\tLoss: 0.029524\n",
            "Train Epoch: 5 [8544/21600 (40%)]\tLoss: 0.001093\n",
            "Train Epoch: 5 [8640/21600 (40%)]\tLoss: 0.006608\n",
            "Train Epoch: 5 [8736/21600 (40%)]\tLoss: 0.000416\n",
            "Train Epoch: 5 [8832/21600 (41%)]\tLoss: 0.084866\n",
            "Train Epoch: 5 [8928/21600 (41%)]\tLoss: 0.129167\n",
            "Train Epoch: 5 [9024/21600 (42%)]\tLoss: 0.009557\n",
            "Train Epoch: 5 [9120/21600 (42%)]\tLoss: 0.014228\n",
            "Train Epoch: 5 [9216/21600 (43%)]\tLoss: 0.005314\n",
            "Train Epoch: 5 [9312/21600 (43%)]\tLoss: 0.004191\n",
            "Train Epoch: 5 [9408/21600 (44%)]\tLoss: 0.000512\n",
            "Train Epoch: 5 [9504/21600 (44%)]\tLoss: 0.001538\n",
            "Train Epoch: 5 [9600/21600 (44%)]\tLoss: 0.009268\n",
            "Train Epoch: 5 [9696/21600 (45%)]\tLoss: 0.000559\n",
            "Train Epoch: 5 [9792/21600 (45%)]\tLoss: 0.003220\n",
            "Train Epoch: 5 [9888/21600 (46%)]\tLoss: 0.238556\n",
            "Train Epoch: 5 [9984/21600 (46%)]\tLoss: 0.003633\n",
            "Train Epoch: 5 [10080/21600 (47%)]\tLoss: 0.008049\n",
            "Train Epoch: 5 [10176/21600 (47%)]\tLoss: 0.006712\n",
            "Train Epoch: 5 [10272/21600 (48%)]\tLoss: 0.001307\n",
            "Train Epoch: 5 [10368/21600 (48%)]\tLoss: 0.010315\n",
            "Train Epoch: 5 [10464/21600 (48%)]\tLoss: 0.001400\n",
            "Train Epoch: 5 [10560/21600 (49%)]\tLoss: 0.008013\n",
            "Train Epoch: 5 [10656/21600 (49%)]\tLoss: 0.002516\n",
            "Train Epoch: 5 [10752/21600 (50%)]\tLoss: 0.000338\n",
            "Train Epoch: 5 [10848/21600 (50%)]\tLoss: 0.000142\n",
            "Train Epoch: 5 [10944/21600 (51%)]\tLoss: 0.016125\n",
            "Train Epoch: 5 [11040/21600 (51%)]\tLoss: 0.019950\n",
            "Train Epoch: 5 [11136/21600 (52%)]\tLoss: 0.000692\n",
            "Train Epoch: 5 [11232/21600 (52%)]\tLoss: 0.033376\n",
            "Train Epoch: 5 [11328/21600 (52%)]\tLoss: 0.019071\n",
            "Train Epoch: 5 [11424/21600 (53%)]\tLoss: 0.000416\n",
            "Train Epoch: 5 [11520/21600 (53%)]\tLoss: 0.044076\n",
            "Train Epoch: 5 [11616/21600 (54%)]\tLoss: 0.017578\n",
            "Train Epoch: 5 [11712/21600 (54%)]\tLoss: 0.001536\n",
            "Train Epoch: 5 [11808/21600 (55%)]\tLoss: 0.000955\n",
            "Train Epoch: 5 [11904/21600 (55%)]\tLoss: 0.003059\n",
            "Train Epoch: 5 [12000/21600 (56%)]\tLoss: 0.001000\n",
            "Train Epoch: 5 [12096/21600 (56%)]\tLoss: 0.092061\n",
            "Train Epoch: 5 [12192/21600 (56%)]\tLoss: 0.000277\n",
            "Train Epoch: 5 [12288/21600 (57%)]\tLoss: 0.000835\n",
            "Train Epoch: 5 [12384/21600 (57%)]\tLoss: 0.203875\n",
            "Train Epoch: 5 [12480/21600 (58%)]\tLoss: 0.000207\n",
            "Train Epoch: 5 [12576/21600 (58%)]\tLoss: 0.001707\n",
            "Train Epoch: 5 [12672/21600 (59%)]\tLoss: 0.006348\n",
            "Train Epoch: 5 [12768/21600 (59%)]\tLoss: 0.004816\n",
            "Train Epoch: 5 [12864/21600 (60%)]\tLoss: 0.000798\n",
            "Train Epoch: 5 [12960/21600 (60%)]\tLoss: 0.001582\n",
            "Train Epoch: 5 [13056/21600 (60%)]\tLoss: 0.029003\n",
            "Train Epoch: 5 [13152/21600 (61%)]\tLoss: 0.007248\n",
            "Train Epoch: 5 [13248/21600 (61%)]\tLoss: 0.001217\n",
            "Train Epoch: 5 [13344/21600 (62%)]\tLoss: 0.005488\n",
            "Train Epoch: 5 [13440/21600 (62%)]\tLoss: 0.004402\n",
            "Train Epoch: 5 [13536/21600 (63%)]\tLoss: 0.002606\n",
            "Train Epoch: 5 [13632/21600 (63%)]\tLoss: 0.002683\n",
            "Train Epoch: 5 [13728/21600 (64%)]\tLoss: 0.012706\n",
            "Train Epoch: 5 [13824/21600 (64%)]\tLoss: 0.000546\n",
            "Train Epoch: 5 [13920/21600 (64%)]\tLoss: 0.004076\n",
            "Train Epoch: 5 [14016/21600 (65%)]\tLoss: 0.079840\n",
            "Train Epoch: 5 [14112/21600 (65%)]\tLoss: 0.005492\n",
            "Train Epoch: 5 [14208/21600 (66%)]\tLoss: 0.001191\n",
            "Train Epoch: 5 [14304/21600 (66%)]\tLoss: 0.000318\n",
            "Train Epoch: 5 [14400/21600 (67%)]\tLoss: 0.001568\n",
            "Train Epoch: 5 [14496/21600 (67%)]\tLoss: 0.001352\n",
            "Train Epoch: 5 [14592/21600 (68%)]\tLoss: 0.051767\n",
            "Train Epoch: 5 [14688/21600 (68%)]\tLoss: 0.002458\n",
            "Train Epoch: 5 [14784/21600 (68%)]\tLoss: 0.027029\n",
            "Train Epoch: 5 [14880/21600 (69%)]\tLoss: 0.000876\n",
            "Train Epoch: 5 [14976/21600 (69%)]\tLoss: 0.015119\n",
            "Train Epoch: 5 [15072/21600 (70%)]\tLoss: 0.001172\n",
            "Train Epoch: 5 [15168/21600 (70%)]\tLoss: 0.004941\n",
            "Train Epoch: 5 [15264/21600 (71%)]\tLoss: 0.001901\n",
            "Train Epoch: 5 [15360/21600 (71%)]\tLoss: 0.000276\n",
            "Train Epoch: 5 [15456/21600 (72%)]\tLoss: 0.006132\n",
            "Train Epoch: 5 [15552/21600 (72%)]\tLoss: 0.042076\n",
            "Train Epoch: 5 [15648/21600 (72%)]\tLoss: 0.000259\n",
            "Train Epoch: 5 [15744/21600 (73%)]\tLoss: 0.000399\n",
            "Train Epoch: 5 [15840/21600 (73%)]\tLoss: 0.018101\n",
            "Train Epoch: 5 [15936/21600 (74%)]\tLoss: 0.008560\n",
            "Train Epoch: 5 [16032/21600 (74%)]\tLoss: 0.015288\n",
            "Train Epoch: 5 [16128/21600 (75%)]\tLoss: 0.007873\n",
            "Train Epoch: 5 [16224/21600 (75%)]\tLoss: 0.002075\n",
            "Train Epoch: 5 [16320/21600 (76%)]\tLoss: 0.002632\n",
            "Train Epoch: 5 [16416/21600 (76%)]\tLoss: 0.013572\n",
            "Train Epoch: 5 [16512/21600 (76%)]\tLoss: 0.013301\n",
            "Train Epoch: 5 [16608/21600 (77%)]\tLoss: 0.000482\n",
            "Train Epoch: 5 [16704/21600 (77%)]\tLoss: 0.001141\n",
            "Train Epoch: 5 [16800/21600 (78%)]\tLoss: 0.000646\n",
            "Train Epoch: 5 [16896/21600 (78%)]\tLoss: 0.003581\n",
            "Train Epoch: 5 [16992/21600 (79%)]\tLoss: 0.018530\n",
            "Train Epoch: 5 [17088/21600 (79%)]\tLoss: 0.054408\n",
            "Train Epoch: 5 [17184/21600 (80%)]\tLoss: 0.001278\n",
            "Train Epoch: 5 [17280/21600 (80%)]\tLoss: 0.000441\n",
            "Train Epoch: 5 [17376/21600 (80%)]\tLoss: 0.005178\n",
            "Train Epoch: 5 [17472/21600 (81%)]\tLoss: 0.028632\n",
            "Train Epoch: 5 [17568/21600 (81%)]\tLoss: 0.044406\n",
            "Train Epoch: 5 [17664/21600 (82%)]\tLoss: 0.003007\n",
            "Train Epoch: 5 [17760/21600 (82%)]\tLoss: 0.001756\n",
            "Train Epoch: 5 [17856/21600 (83%)]\tLoss: 0.007369\n",
            "Train Epoch: 5 [17952/21600 (83%)]\tLoss: 0.001339\n",
            "Train Epoch: 5 [18048/21600 (84%)]\tLoss: 0.123557\n",
            "Train Epoch: 5 [18144/21600 (84%)]\tLoss: 0.136708\n",
            "Train Epoch: 5 [18240/21600 (84%)]\tLoss: 0.065518\n",
            "Train Epoch: 5 [18336/21600 (85%)]\tLoss: 0.012976\n",
            "Train Epoch: 5 [18432/21600 (85%)]\tLoss: 0.003220\n",
            "Train Epoch: 5 [18528/21600 (86%)]\tLoss: 0.002217\n",
            "Train Epoch: 5 [18624/21600 (86%)]\tLoss: 0.003402\n",
            "Train Epoch: 5 [18720/21600 (87%)]\tLoss: 0.000926\n",
            "Train Epoch: 5 [18816/21600 (87%)]\tLoss: 0.000247\n",
            "Train Epoch: 5 [18912/21600 (88%)]\tLoss: 0.020376\n",
            "Train Epoch: 5 [19008/21600 (88%)]\tLoss: 0.016247\n",
            "Train Epoch: 5 [19104/21600 (88%)]\tLoss: 0.000098\n",
            "Train Epoch: 5 [19200/21600 (89%)]\tLoss: 0.002135\n",
            "Train Epoch: 5 [19296/21600 (89%)]\tLoss: 0.006958\n",
            "Train Epoch: 5 [19392/21600 (90%)]\tLoss: 0.000679\n",
            "Train Epoch: 5 [19488/21600 (90%)]\tLoss: 0.004413\n",
            "Train Epoch: 5 [19584/21600 (91%)]\tLoss: 0.002568\n",
            "Train Epoch: 5 [19680/21600 (91%)]\tLoss: 0.012289\n",
            "Train Epoch: 5 [19776/21600 (92%)]\tLoss: 0.004508\n",
            "Train Epoch: 5 [19872/21600 (92%)]\tLoss: 0.001046\n",
            "Train Epoch: 5 [19968/21600 (92%)]\tLoss: 0.005092\n",
            "Train Epoch: 5 [20064/21600 (93%)]\tLoss: 0.000336\n",
            "Train Epoch: 5 [20160/21600 (93%)]\tLoss: 0.010246\n",
            "Train Epoch: 5 [20256/21600 (94%)]\tLoss: 0.004357\n",
            "Train Epoch: 5 [20352/21600 (94%)]\tLoss: 0.005148\n",
            "Train Epoch: 5 [20448/21600 (95%)]\tLoss: 0.000781\n",
            "Train Epoch: 5 [20544/21600 (95%)]\tLoss: 0.008028\n",
            "Train Epoch: 5 [20640/21600 (96%)]\tLoss: 0.000744\n",
            "Train Epoch: 5 [20736/21600 (96%)]\tLoss: 0.004451\n",
            "Train Epoch: 5 [20832/21600 (96%)]\tLoss: 0.000112\n",
            "Train Epoch: 5 [20928/21600 (97%)]\tLoss: 0.012206\n",
            "Train Epoch: 5 [21024/21600 (97%)]\tLoss: 0.005813\n",
            "Train Epoch: 5 [21120/21600 (98%)]\tLoss: 0.002010\n",
            "Train Epoch: 5 [21216/21600 (98%)]\tLoss: 0.000915\n",
            "Train Epoch: 5 [21312/21600 (99%)]\tLoss: 0.005029\n",
            "Train Epoch: 5 [21408/21600 (99%)]\tLoss: 0.000434\n",
            "Train Epoch: 5 [21504/21600 (100%)]\tLoss: 0.002083\n",
            "\n",
            "Validation set: Average loss: 0.0005, Accuracy: 5400/5400 (100%)\n",
            "\n",
            "\n",
            "Saved model to model_binary_5.pth.\n",
            "Train Epoch: 6 [0/21600 (0%)]\tLoss: 0.002462\n",
            "Train Epoch: 6 [96/21600 (0%)]\tLoss: 0.001616\n",
            "Train Epoch: 6 [192/21600 (1%)]\tLoss: 0.001578\n",
            "Train Epoch: 6 [288/21600 (1%)]\tLoss: 0.001137\n",
            "Train Epoch: 6 [384/21600 (2%)]\tLoss: 0.015527\n",
            "Train Epoch: 6 [480/21600 (2%)]\tLoss: 0.005302\n",
            "Train Epoch: 6 [576/21600 (3%)]\tLoss: 0.067329\n",
            "Train Epoch: 6 [672/21600 (3%)]\tLoss: 0.000453\n",
            "Train Epoch: 6 [768/21600 (4%)]\tLoss: 0.006437\n",
            "Train Epoch: 6 [864/21600 (4%)]\tLoss: 0.003126\n",
            "Train Epoch: 6 [960/21600 (4%)]\tLoss: 0.012170\n",
            "Train Epoch: 6 [1056/21600 (5%)]\tLoss: 0.004556\n",
            "Train Epoch: 6 [1152/21600 (5%)]\tLoss: 0.010849\n",
            "Train Epoch: 6 [1248/21600 (6%)]\tLoss: 0.020551\n",
            "Train Epoch: 6 [1344/21600 (6%)]\tLoss: 0.023939\n",
            "Train Epoch: 6 [1440/21600 (7%)]\tLoss: 0.001407\n",
            "Train Epoch: 6 [1536/21600 (7%)]\tLoss: 0.001291\n",
            "Train Epoch: 6 [1632/21600 (8%)]\tLoss: 0.012993\n",
            "Train Epoch: 6 [1728/21600 (8%)]\tLoss: 0.000566\n",
            "Train Epoch: 6 [1824/21600 (8%)]\tLoss: 0.087298\n",
            "Train Epoch: 6 [1920/21600 (9%)]\tLoss: 0.000811\n",
            "Train Epoch: 6 [2016/21600 (9%)]\tLoss: 0.000390\n",
            "Train Epoch: 6 [2112/21600 (10%)]\tLoss: 0.039876\n",
            "Train Epoch: 6 [2208/21600 (10%)]\tLoss: 0.008006\n",
            "Train Epoch: 6 [2304/21600 (11%)]\tLoss: 0.012258\n",
            "Train Epoch: 6 [2400/21600 (11%)]\tLoss: 0.001679\n",
            "Train Epoch: 6 [2496/21600 (12%)]\tLoss: 0.000559\n",
            "Train Epoch: 6 [2592/21600 (12%)]\tLoss: 0.013954\n",
            "Train Epoch: 6 [2688/21600 (12%)]\tLoss: 0.000054\n",
            "Train Epoch: 6 [2784/21600 (13%)]\tLoss: 0.004154\n",
            "Train Epoch: 6 [2880/21600 (13%)]\tLoss: 0.000795\n",
            "Train Epoch: 6 [2976/21600 (14%)]\tLoss: 0.009852\n",
            "Train Epoch: 6 [3072/21600 (14%)]\tLoss: 0.080863\n",
            "Train Epoch: 6 [3168/21600 (15%)]\tLoss: 0.001507\n",
            "Train Epoch: 6 [3264/21600 (15%)]\tLoss: 0.008506\n",
            "Train Epoch: 6 [3360/21600 (16%)]\tLoss: 0.005043\n",
            "Train Epoch: 6 [3456/21600 (16%)]\tLoss: 0.006175\n",
            "Train Epoch: 6 [3552/21600 (16%)]\tLoss: 0.013442\n",
            "Train Epoch: 6 [3648/21600 (17%)]\tLoss: 0.000927\n",
            "Train Epoch: 6 [3744/21600 (17%)]\tLoss: 0.108276\n",
            "Train Epoch: 6 [3840/21600 (18%)]\tLoss: 0.001152\n",
            "Train Epoch: 6 [3936/21600 (18%)]\tLoss: 0.003014\n",
            "Train Epoch: 6 [4032/21600 (19%)]\tLoss: 0.006297\n",
            "Train Epoch: 6 [4128/21600 (19%)]\tLoss: 0.004926\n",
            "Train Epoch: 6 [4224/21600 (20%)]\tLoss: 0.050841\n",
            "Train Epoch: 6 [4320/21600 (20%)]\tLoss: 0.014588\n",
            "Train Epoch: 6 [4416/21600 (20%)]\tLoss: 0.002115\n",
            "Train Epoch: 6 [4512/21600 (21%)]\tLoss: 0.001474\n",
            "Train Epoch: 6 [4608/21600 (21%)]\tLoss: 0.002687\n",
            "Train Epoch: 6 [4704/21600 (22%)]\tLoss: 0.005572\n",
            "Train Epoch: 6 [4800/21600 (22%)]\tLoss: 0.001614\n",
            "Train Epoch: 6 [4896/21600 (23%)]\tLoss: 0.076033\n",
            "Train Epoch: 6 [4992/21600 (23%)]\tLoss: 0.001114\n",
            "Train Epoch: 6 [5088/21600 (24%)]\tLoss: 0.000221\n",
            "Train Epoch: 6 [5184/21600 (24%)]\tLoss: 0.000122\n",
            "Train Epoch: 6 [5280/21600 (24%)]\tLoss: 0.006299\n",
            "Train Epoch: 6 [5376/21600 (25%)]\tLoss: 0.017291\n",
            "Train Epoch: 6 [5472/21600 (25%)]\tLoss: 0.004510\n",
            "Train Epoch: 6 [5568/21600 (26%)]\tLoss: 0.000719\n",
            "Train Epoch: 6 [5664/21600 (26%)]\tLoss: 0.000236\n",
            "Train Epoch: 6 [5760/21600 (27%)]\tLoss: 0.000560\n",
            "Train Epoch: 6 [5856/21600 (27%)]\tLoss: 0.104818\n",
            "Train Epoch: 6 [5952/21600 (28%)]\tLoss: 0.009476\n",
            "Train Epoch: 6 [6048/21600 (28%)]\tLoss: 0.001462\n",
            "Train Epoch: 6 [6144/21600 (28%)]\tLoss: 0.001149\n",
            "Train Epoch: 6 [6240/21600 (29%)]\tLoss: 0.011686\n",
            "Train Epoch: 6 [6336/21600 (29%)]\tLoss: 0.001207\n",
            "Train Epoch: 6 [6432/21600 (30%)]\tLoss: 0.004431\n",
            "Train Epoch: 6 [6528/21600 (30%)]\tLoss: 0.002768\n",
            "Train Epoch: 6 [6624/21600 (31%)]\tLoss: 0.026286\n",
            "Train Epoch: 6 [6720/21600 (31%)]\tLoss: 0.002208\n",
            "Train Epoch: 6 [6816/21600 (32%)]\tLoss: 0.000437\n",
            "Train Epoch: 6 [6912/21600 (32%)]\tLoss: 0.013434\n",
            "Train Epoch: 6 [7008/21600 (32%)]\tLoss: 0.003275\n",
            "Train Epoch: 6 [7104/21600 (33%)]\tLoss: 0.083202\n",
            "Train Epoch: 6 [7200/21600 (33%)]\tLoss: 0.001040\n",
            "Train Epoch: 6 [7296/21600 (34%)]\tLoss: 0.004489\n",
            "Train Epoch: 6 [7392/21600 (34%)]\tLoss: 0.061346\n",
            "Train Epoch: 6 [7488/21600 (35%)]\tLoss: 0.000740\n",
            "Train Epoch: 6 [7584/21600 (35%)]\tLoss: 0.009561\n",
            "Train Epoch: 6 [7680/21600 (36%)]\tLoss: 0.000731\n",
            "Train Epoch: 6 [7776/21600 (36%)]\tLoss: 0.002360\n",
            "Train Epoch: 6 [7872/21600 (36%)]\tLoss: 0.013676\n",
            "Train Epoch: 6 [7968/21600 (37%)]\tLoss: 0.003749\n",
            "Train Epoch: 6 [8064/21600 (37%)]\tLoss: 0.000473\n",
            "Train Epoch: 6 [8160/21600 (38%)]\tLoss: 0.231108\n",
            "Train Epoch: 6 [8256/21600 (38%)]\tLoss: 0.015340\n",
            "Train Epoch: 6 [8352/21600 (39%)]\tLoss: 0.003756\n",
            "Train Epoch: 6 [8448/21600 (39%)]\tLoss: 0.071571\n",
            "Train Epoch: 6 [8544/21600 (40%)]\tLoss: 0.000327\n",
            "Train Epoch: 6 [8640/21600 (40%)]\tLoss: 0.001344\n",
            "Train Epoch: 6 [8736/21600 (40%)]\tLoss: 0.043182\n",
            "Train Epoch: 6 [8832/21600 (41%)]\tLoss: 0.015227\n",
            "Train Epoch: 6 [8928/21600 (41%)]\tLoss: 0.001168\n",
            "Train Epoch: 6 [9024/21600 (42%)]\tLoss: 0.003456\n",
            "Train Epoch: 6 [9120/21600 (42%)]\tLoss: 0.012811\n",
            "Train Epoch: 6 [9216/21600 (43%)]\tLoss: 0.000226\n",
            "Train Epoch: 6 [9312/21600 (43%)]\tLoss: 0.000590\n",
            "Train Epoch: 6 [9408/21600 (44%)]\tLoss: 0.001994\n",
            "Train Epoch: 6 [9504/21600 (44%)]\tLoss: 0.001201\n",
            "Train Epoch: 6 [9600/21600 (44%)]\tLoss: 0.003753\n",
            "Train Epoch: 6 [9696/21600 (45%)]\tLoss: 0.003381\n",
            "Train Epoch: 6 [9792/21600 (45%)]\tLoss: 0.018631\n",
            "Train Epoch: 6 [9888/21600 (46%)]\tLoss: 0.002826\n",
            "Train Epoch: 6 [9984/21600 (46%)]\tLoss: 0.003372\n",
            "Train Epoch: 6 [10080/21600 (47%)]\tLoss: 0.005040\n",
            "Train Epoch: 6 [10176/21600 (47%)]\tLoss: 0.000309\n",
            "Train Epoch: 6 [10272/21600 (48%)]\tLoss: 0.006956\n",
            "Train Epoch: 6 [10368/21600 (48%)]\tLoss: 0.009622\n",
            "Train Epoch: 6 [10464/21600 (48%)]\tLoss: 0.067165\n",
            "Train Epoch: 6 [10560/21600 (49%)]\tLoss: 0.001226\n",
            "Train Epoch: 6 [10656/21600 (49%)]\tLoss: 0.000238\n",
            "Train Epoch: 6 [10752/21600 (50%)]\tLoss: 0.004406\n",
            "Train Epoch: 6 [10848/21600 (50%)]\tLoss: 0.003285\n",
            "Train Epoch: 6 [10944/21600 (51%)]\tLoss: 0.002446\n",
            "Train Epoch: 6 [11040/21600 (51%)]\tLoss: 0.010714\n",
            "Train Epoch: 6 [11136/21600 (52%)]\tLoss: 0.007791\n",
            "Train Epoch: 6 [11232/21600 (52%)]\tLoss: 0.011772\n",
            "Train Epoch: 6 [11328/21600 (52%)]\tLoss: 0.001842\n",
            "Train Epoch: 6 [11424/21600 (53%)]\tLoss: 0.036348\n",
            "Train Epoch: 6 [11520/21600 (53%)]\tLoss: 0.000925\n",
            "Train Epoch: 6 [11616/21600 (54%)]\tLoss: 0.001589\n",
            "Train Epoch: 6 [11712/21600 (54%)]\tLoss: 0.000364\n",
            "Train Epoch: 6 [11808/21600 (55%)]\tLoss: 0.039090\n",
            "Train Epoch: 6 [11904/21600 (55%)]\tLoss: 0.002166\n",
            "Train Epoch: 6 [12000/21600 (56%)]\tLoss: 0.000225\n",
            "Train Epoch: 6 [12096/21600 (56%)]\tLoss: 0.013166\n",
            "Train Epoch: 6 [12192/21600 (56%)]\tLoss: 0.006065\n",
            "Train Epoch: 6 [12288/21600 (57%)]\tLoss: 0.016556\n",
            "Train Epoch: 6 [12384/21600 (57%)]\tLoss: 0.002836\n",
            "Train Epoch: 6 [12480/21600 (58%)]\tLoss: 0.000278\n",
            "Train Epoch: 6 [12576/21600 (58%)]\tLoss: 0.000442\n",
            "Train Epoch: 6 [12672/21600 (59%)]\tLoss: 0.001317\n",
            "Train Epoch: 6 [12768/21600 (59%)]\tLoss: 0.045756\n",
            "Train Epoch: 6 [12864/21600 (60%)]\tLoss: 0.000233\n",
            "Train Epoch: 6 [12960/21600 (60%)]\tLoss: 0.018690\n",
            "Train Epoch: 6 [13056/21600 (60%)]\tLoss: 0.000218\n",
            "Train Epoch: 6 [13152/21600 (61%)]\tLoss: 0.004316\n",
            "Train Epoch: 6 [13248/21600 (61%)]\tLoss: 0.000796\n",
            "Train Epoch: 6 [13344/21600 (62%)]\tLoss: 0.000753\n",
            "Train Epoch: 6 [13440/21600 (62%)]\tLoss: 0.005070\n",
            "Train Epoch: 6 [13536/21600 (63%)]\tLoss: 0.001655\n",
            "Train Epoch: 6 [13632/21600 (63%)]\tLoss: 0.000754\n",
            "Train Epoch: 6 [13728/21600 (64%)]\tLoss: 0.000659\n",
            "Train Epoch: 6 [13824/21600 (64%)]\tLoss: 0.005122\n",
            "Train Epoch: 6 [13920/21600 (64%)]\tLoss: 0.000452\n",
            "Train Epoch: 6 [14016/21600 (65%)]\tLoss: 0.045571\n",
            "Train Epoch: 6 [14112/21600 (65%)]\tLoss: 0.013782\n",
            "Train Epoch: 6 [14208/21600 (66%)]\tLoss: 0.017222\n",
            "Train Epoch: 6 [14304/21600 (66%)]\tLoss: 0.000618\n",
            "Train Epoch: 6 [14400/21600 (67%)]\tLoss: 0.016256\n",
            "Train Epoch: 6 [14496/21600 (67%)]\tLoss: 0.000353\n",
            "Train Epoch: 6 [14592/21600 (68%)]\tLoss: 0.001282\n",
            "Train Epoch: 6 [14688/21600 (68%)]\tLoss: 0.000205\n",
            "Train Epoch: 6 [14784/21600 (68%)]\tLoss: 0.004323\n",
            "Train Epoch: 6 [14880/21600 (69%)]\tLoss: 0.002486\n",
            "Train Epoch: 6 [14976/21600 (69%)]\tLoss: 0.002212\n",
            "Train Epoch: 6 [15072/21600 (70%)]\tLoss: 0.000258\n",
            "Train Epoch: 6 [15168/21600 (70%)]\tLoss: 0.014148\n",
            "Train Epoch: 6 [15264/21600 (71%)]\tLoss: 0.001983\n",
            "Train Epoch: 6 [15360/21600 (71%)]\tLoss: 0.001235\n",
            "Train Epoch: 6 [15456/21600 (72%)]\tLoss: 0.002845\n",
            "Train Epoch: 6 [15552/21600 (72%)]\tLoss: 0.001792\n",
            "Train Epoch: 6 [15648/21600 (72%)]\tLoss: 0.028069\n",
            "Train Epoch: 6 [15744/21600 (73%)]\tLoss: 0.000983\n",
            "Train Epoch: 6 [15840/21600 (73%)]\tLoss: 0.007556\n",
            "Train Epoch: 6 [15936/21600 (74%)]\tLoss: 0.011350\n",
            "Train Epoch: 6 [16032/21600 (74%)]\tLoss: 0.015938\n",
            "Train Epoch: 6 [16128/21600 (75%)]\tLoss: 0.000873\n",
            "Train Epoch: 6 [16224/21600 (75%)]\tLoss: 0.005008\n",
            "Train Epoch: 6 [16320/21600 (76%)]\tLoss: 0.003571\n",
            "Train Epoch: 6 [16416/21600 (76%)]\tLoss: 0.000871\n",
            "Train Epoch: 6 [16512/21600 (76%)]\tLoss: 0.002472\n",
            "Train Epoch: 6 [16608/21600 (77%)]\tLoss: 0.003462\n",
            "Train Epoch: 6 [16704/21600 (77%)]\tLoss: 0.009615\n",
            "Train Epoch: 6 [16800/21600 (78%)]\tLoss: 0.000308\n",
            "Train Epoch: 6 [16896/21600 (78%)]\tLoss: 0.004332\n",
            "Train Epoch: 6 [16992/21600 (79%)]\tLoss: 0.003033\n",
            "Train Epoch: 6 [17088/21600 (79%)]\tLoss: 0.000236\n",
            "Train Epoch: 6 [17184/21600 (80%)]\tLoss: 0.012717\n",
            "Train Epoch: 6 [17280/21600 (80%)]\tLoss: 0.000385\n",
            "Train Epoch: 6 [17376/21600 (80%)]\tLoss: 0.007724\n",
            "Train Epoch: 6 [17472/21600 (81%)]\tLoss: 0.001506\n",
            "Train Epoch: 6 [17568/21600 (81%)]\tLoss: 0.000697\n",
            "Train Epoch: 6 [17664/21600 (82%)]\tLoss: 0.006117\n",
            "Train Epoch: 6 [17760/21600 (82%)]\tLoss: 0.000979\n",
            "Train Epoch: 6 [17856/21600 (83%)]\tLoss: 0.000457\n",
            "Train Epoch: 6 [17952/21600 (83%)]\tLoss: 0.000452\n",
            "Train Epoch: 6 [18048/21600 (84%)]\tLoss: 0.006118\n",
            "Train Epoch: 6 [18144/21600 (84%)]\tLoss: 0.002063\n",
            "Train Epoch: 6 [18240/21600 (84%)]\tLoss: 0.001892\n",
            "Train Epoch: 6 [18336/21600 (85%)]\tLoss: 0.003276\n",
            "Train Epoch: 6 [18432/21600 (85%)]\tLoss: 0.002134\n",
            "Train Epoch: 6 [18528/21600 (86%)]\tLoss: 0.005253\n",
            "Train Epoch: 6 [18624/21600 (86%)]\tLoss: 0.000156\n",
            "Train Epoch: 6 [18720/21600 (87%)]\tLoss: 0.017886\n",
            "Train Epoch: 6 [18816/21600 (87%)]\tLoss: 0.000122\n",
            "Train Epoch: 6 [18912/21600 (88%)]\tLoss: 0.000252\n",
            "Train Epoch: 6 [19008/21600 (88%)]\tLoss: 0.042013\n",
            "Train Epoch: 6 [19104/21600 (88%)]\tLoss: 0.038258\n",
            "Train Epoch: 6 [19200/21600 (89%)]\tLoss: 0.013076\n",
            "Train Epoch: 6 [19296/21600 (89%)]\tLoss: 0.000517\n",
            "Train Epoch: 6 [19392/21600 (90%)]\tLoss: 0.001826\n",
            "Train Epoch: 6 [19488/21600 (90%)]\tLoss: 0.000422\n",
            "Train Epoch: 6 [19584/21600 (91%)]\tLoss: 0.000061\n",
            "Train Epoch: 6 [19680/21600 (91%)]\tLoss: 0.003193\n",
            "Train Epoch: 6 [19776/21600 (92%)]\tLoss: 0.001927\n",
            "Train Epoch: 6 [19872/21600 (92%)]\tLoss: 0.000903\n",
            "Train Epoch: 6 [19968/21600 (92%)]\tLoss: 0.001128\n",
            "Train Epoch: 6 [20064/21600 (93%)]\tLoss: 0.017382\n",
            "Train Epoch: 6 [20160/21600 (93%)]\tLoss: 0.046098\n",
            "Train Epoch: 6 [20256/21600 (94%)]\tLoss: 0.000609\n",
            "Train Epoch: 6 [20352/21600 (94%)]\tLoss: 0.000964\n",
            "Train Epoch: 6 [20448/21600 (95%)]\tLoss: 0.021611\n",
            "Train Epoch: 6 [20544/21600 (95%)]\tLoss: 0.000448\n",
            "Train Epoch: 6 [20640/21600 (96%)]\tLoss: 0.007827\n",
            "Train Epoch: 6 [20736/21600 (96%)]\tLoss: 0.003420\n",
            "Train Epoch: 6 [20832/21600 (96%)]\tLoss: 0.000418\n",
            "Train Epoch: 6 [20928/21600 (97%)]\tLoss: 0.000757\n",
            "Train Epoch: 6 [21024/21600 (97%)]\tLoss: 0.004279\n",
            "Train Epoch: 6 [21120/21600 (98%)]\tLoss: 0.000399\n",
            "Train Epoch: 6 [21216/21600 (98%)]\tLoss: 0.004458\n",
            "Train Epoch: 6 [21312/21600 (99%)]\tLoss: 0.001448\n",
            "Train Epoch: 6 [21408/21600 (99%)]\tLoss: 0.001129\n",
            "Train Epoch: 6 [21504/21600 (100%)]\tLoss: 0.012672\n",
            "\n",
            "Validation set: Average loss: 0.0163, Accuracy: 5375/5400 (100%)\n",
            "\n",
            "\n",
            "Saved model to model_binary_6.pth.\n",
            "Train Epoch: 7 [0/21600 (0%)]\tLoss: 0.000424\n",
            "Train Epoch: 7 [96/21600 (0%)]\tLoss: 0.000199\n",
            "Train Epoch: 7 [192/21600 (1%)]\tLoss: 0.001293\n",
            "Train Epoch: 7 [288/21600 (1%)]\tLoss: 0.003110\n",
            "Train Epoch: 7 [384/21600 (2%)]\tLoss: 0.001616\n",
            "Train Epoch: 7 [480/21600 (2%)]\tLoss: 0.008605\n",
            "Train Epoch: 7 [576/21600 (3%)]\tLoss: 0.002035\n",
            "Train Epoch: 7 [672/21600 (3%)]\tLoss: 0.002451\n",
            "Train Epoch: 7 [768/21600 (4%)]\tLoss: 0.000434\n",
            "Train Epoch: 7 [864/21600 (4%)]\tLoss: 0.000272\n",
            "Train Epoch: 7 [960/21600 (4%)]\tLoss: 0.001363\n",
            "Train Epoch: 7 [1056/21600 (5%)]\tLoss: 0.000950\n",
            "Train Epoch: 7 [1152/21600 (5%)]\tLoss: 0.000093\n",
            "Train Epoch: 7 [1248/21600 (6%)]\tLoss: 0.036636\n",
            "Train Epoch: 7 [1344/21600 (6%)]\tLoss: 0.004357\n",
            "Train Epoch: 7 [1440/21600 (7%)]\tLoss: 0.000137\n",
            "Train Epoch: 7 [1536/21600 (7%)]\tLoss: 0.000027\n",
            "Train Epoch: 7 [1632/21600 (8%)]\tLoss: 0.000270\n",
            "Train Epoch: 7 [1728/21600 (8%)]\tLoss: 0.062420\n",
            "Train Epoch: 7 [1824/21600 (8%)]\tLoss: 0.000095\n",
            "Train Epoch: 7 [1920/21600 (9%)]\tLoss: 0.003618\n",
            "Train Epoch: 7 [2016/21600 (9%)]\tLoss: 0.001121\n",
            "Train Epoch: 7 [2112/21600 (10%)]\tLoss: 0.000650\n",
            "Train Epoch: 7 [2208/21600 (10%)]\tLoss: 0.000160\n",
            "Train Epoch: 7 [2304/21600 (11%)]\tLoss: 0.000433\n",
            "Train Epoch: 7 [2400/21600 (11%)]\tLoss: 0.000202\n",
            "Train Epoch: 7 [2496/21600 (12%)]\tLoss: 0.000472\n",
            "Train Epoch: 7 [2592/21600 (12%)]\tLoss: 0.000498\n",
            "Train Epoch: 7 [2688/21600 (12%)]\tLoss: 0.000530\n",
            "Train Epoch: 7 [2784/21600 (13%)]\tLoss: 0.004172\n",
            "Train Epoch: 7 [2880/21600 (13%)]\tLoss: 0.000992\n",
            "Train Epoch: 7 [2976/21600 (14%)]\tLoss: 0.000279\n",
            "Train Epoch: 7 [3072/21600 (14%)]\tLoss: 0.000166\n",
            "Train Epoch: 7 [3168/21600 (15%)]\tLoss: 0.000254\n",
            "Train Epoch: 7 [3264/21600 (15%)]\tLoss: 0.000381\n",
            "Train Epoch: 7 [3360/21600 (16%)]\tLoss: 0.032792\n",
            "Train Epoch: 7 [3456/21600 (16%)]\tLoss: 0.000253\n",
            "Train Epoch: 7 [3552/21600 (16%)]\tLoss: 0.000025\n",
            "Train Epoch: 7 [3648/21600 (17%)]\tLoss: 0.000611\n",
            "Train Epoch: 7 [3744/21600 (17%)]\tLoss: 0.000120\n",
            "Train Epoch: 7 [3840/21600 (18%)]\tLoss: 0.000144\n",
            "Train Epoch: 7 [3936/21600 (18%)]\tLoss: 0.047001\n",
            "Train Epoch: 7 [4032/21600 (19%)]\tLoss: 0.000480\n",
            "Train Epoch: 7 [4128/21600 (19%)]\tLoss: 0.010650\n",
            "Train Epoch: 7 [4224/21600 (20%)]\tLoss: 0.002668\n",
            "Train Epoch: 7 [4320/21600 (20%)]\tLoss: 0.000965\n",
            "Train Epoch: 7 [4416/21600 (20%)]\tLoss: 0.000096\n",
            "Train Epoch: 7 [4512/21600 (21%)]\tLoss: 0.005982\n",
            "Train Epoch: 7 [4608/21600 (21%)]\tLoss: 0.000461\n",
            "Train Epoch: 7 [4704/21600 (22%)]\tLoss: 0.040041\n",
            "Train Epoch: 7 [4800/21600 (22%)]\tLoss: 0.003782\n",
            "Train Epoch: 7 [4896/21600 (23%)]\tLoss: 0.001191\n",
            "Train Epoch: 7 [4992/21600 (23%)]\tLoss: 0.001752\n",
            "Train Epoch: 7 [5088/21600 (24%)]\tLoss: 0.000374\n",
            "Train Epoch: 7 [5184/21600 (24%)]\tLoss: 0.062846\n",
            "Train Epoch: 7 [5280/21600 (24%)]\tLoss: 0.000534\n",
            "Train Epoch: 7 [5376/21600 (25%)]\tLoss: 0.000424\n",
            "Train Epoch: 7 [5472/21600 (25%)]\tLoss: 0.000298\n",
            "Train Epoch: 7 [5568/21600 (26%)]\tLoss: 0.000297\n",
            "Train Epoch: 7 [5664/21600 (26%)]\tLoss: 0.000078\n",
            "Train Epoch: 7 [5760/21600 (27%)]\tLoss: 0.000669\n",
            "Train Epoch: 7 [5856/21600 (27%)]\tLoss: 0.000374\n",
            "Train Epoch: 7 [5952/21600 (28%)]\tLoss: 0.007570\n",
            "Train Epoch: 7 [6048/21600 (28%)]\tLoss: 0.000145\n",
            "Train Epoch: 7 [6144/21600 (28%)]\tLoss: 0.000075\n",
            "Train Epoch: 7 [6240/21600 (29%)]\tLoss: 0.059258\n",
            "Train Epoch: 7 [6336/21600 (29%)]\tLoss: 0.010892\n",
            "Train Epoch: 7 [6432/21600 (30%)]\tLoss: 0.000054\n",
            "Train Epoch: 7 [6528/21600 (30%)]\tLoss: 0.000209\n",
            "Train Epoch: 7 [6624/21600 (31%)]\tLoss: 0.002977\n",
            "Train Epoch: 7 [6720/21600 (31%)]\tLoss: 0.001639\n",
            "Train Epoch: 7 [6816/21600 (32%)]\tLoss: 0.000314\n",
            "Train Epoch: 7 [6912/21600 (32%)]\tLoss: 0.000433\n",
            "Train Epoch: 7 [7008/21600 (32%)]\tLoss: 0.000766\n",
            "Train Epoch: 7 [7104/21600 (33%)]\tLoss: 0.001810\n",
            "Train Epoch: 7 [7200/21600 (33%)]\tLoss: 0.023495\n",
            "Train Epoch: 7 [7296/21600 (34%)]\tLoss: 0.000454\n",
            "Train Epoch: 7 [7392/21600 (34%)]\tLoss: 0.007245\n",
            "Train Epoch: 7 [7488/21600 (35%)]\tLoss: 0.008917\n",
            "Train Epoch: 7 [7584/21600 (35%)]\tLoss: 0.040564\n",
            "Train Epoch: 7 [7680/21600 (36%)]\tLoss: 0.006115\n",
            "Train Epoch: 7 [7776/21600 (36%)]\tLoss: 0.111691\n",
            "Train Epoch: 7 [7872/21600 (36%)]\tLoss: 0.000614\n",
            "Train Epoch: 7 [7968/21600 (37%)]\tLoss: 0.000305\n",
            "Train Epoch: 7 [8064/21600 (37%)]\tLoss: 0.000608\n",
            "Train Epoch: 7 [8160/21600 (38%)]\tLoss: 0.000856\n",
            "Train Epoch: 7 [8256/21600 (38%)]\tLoss: 0.002058\n",
            "Train Epoch: 7 [8352/21600 (39%)]\tLoss: 0.019757\n",
            "Train Epoch: 7 [8448/21600 (39%)]\tLoss: 0.018590\n",
            "Train Epoch: 7 [8544/21600 (40%)]\tLoss: 0.001259\n",
            "Train Epoch: 7 [8640/21600 (40%)]\tLoss: 0.008279\n",
            "Train Epoch: 7 [8736/21600 (40%)]\tLoss: 0.092569\n",
            "Train Epoch: 7 [8832/21600 (41%)]\tLoss: 0.022122\n",
            "Train Epoch: 7 [8928/21600 (41%)]\tLoss: 0.009271\n",
            "Train Epoch: 7 [9024/21600 (42%)]\tLoss: 0.000145\n",
            "Train Epoch: 7 [9120/21600 (42%)]\tLoss: 0.008080\n",
            "Train Epoch: 7 [9216/21600 (43%)]\tLoss: 0.008562\n",
            "Train Epoch: 7 [9312/21600 (43%)]\tLoss: 0.009156\n",
            "Train Epoch: 7 [9408/21600 (44%)]\tLoss: 0.000182\n",
            "Train Epoch: 7 [9504/21600 (44%)]\tLoss: 0.065777\n",
            "Train Epoch: 7 [9600/21600 (44%)]\tLoss: 0.000509\n",
            "Train Epoch: 7 [9696/21600 (45%)]\tLoss: 0.000112\n",
            "Train Epoch: 7 [9792/21600 (45%)]\tLoss: 0.006083\n",
            "Train Epoch: 7 [9888/21600 (46%)]\tLoss: 0.000226\n",
            "Train Epoch: 7 [9984/21600 (46%)]\tLoss: 0.000533\n",
            "Train Epoch: 7 [10080/21600 (47%)]\tLoss: 0.009072\n",
            "Train Epoch: 7 [10176/21600 (47%)]\tLoss: 0.009856\n",
            "Train Epoch: 7 [10272/21600 (48%)]\tLoss: 0.004874\n",
            "Train Epoch: 7 [10368/21600 (48%)]\tLoss: 0.098325\n",
            "Train Epoch: 7 [10464/21600 (48%)]\tLoss: 0.185486\n",
            "Train Epoch: 7 [10560/21600 (49%)]\tLoss: 0.001621\n",
            "Train Epoch: 7 [10656/21600 (49%)]\tLoss: 0.003768\n",
            "Train Epoch: 7 [10752/21600 (50%)]\tLoss: 0.008622\n",
            "Train Epoch: 7 [10848/21600 (50%)]\tLoss: 0.000582\n",
            "Train Epoch: 7 [10944/21600 (51%)]\tLoss: 0.000199\n",
            "Train Epoch: 7 [11040/21600 (51%)]\tLoss: 0.002390\n",
            "Train Epoch: 7 [11136/21600 (52%)]\tLoss: 0.029129\n",
            "Train Epoch: 7 [11232/21600 (52%)]\tLoss: 0.067932\n",
            "Train Epoch: 7 [11328/21600 (52%)]\tLoss: 0.000981\n",
            "Train Epoch: 7 [11424/21600 (53%)]\tLoss: 0.000862\n",
            "Train Epoch: 7 [11520/21600 (53%)]\tLoss: 0.000216\n",
            "Train Epoch: 7 [11616/21600 (54%)]\tLoss: 0.002939\n",
            "Train Epoch: 7 [11712/21600 (54%)]\tLoss: 0.000388\n",
            "Train Epoch: 7 [11808/21600 (55%)]\tLoss: 0.000073\n",
            "Train Epoch: 7 [11904/21600 (55%)]\tLoss: 0.054912\n",
            "Train Epoch: 7 [12000/21600 (56%)]\tLoss: 0.001697\n",
            "Train Epoch: 7 [12096/21600 (56%)]\tLoss: 0.003484\n",
            "Train Epoch: 7 [12192/21600 (56%)]\tLoss: 0.000129\n",
            "Train Epoch: 7 [12288/21600 (57%)]\tLoss: 0.000671\n",
            "Train Epoch: 7 [12384/21600 (57%)]\tLoss: 0.000146\n",
            "Train Epoch: 7 [12480/21600 (58%)]\tLoss: 0.005219\n",
            "Train Epoch: 7 [12576/21600 (58%)]\tLoss: 0.000202\n",
            "Train Epoch: 7 [12672/21600 (59%)]\tLoss: 0.001192\n",
            "Train Epoch: 7 [12768/21600 (59%)]\tLoss: 0.000195\n",
            "Train Epoch: 7 [12864/21600 (60%)]\tLoss: 0.007279\n",
            "Train Epoch: 7 [12960/21600 (60%)]\tLoss: 0.043885\n",
            "Train Epoch: 7 [13056/21600 (60%)]\tLoss: 0.001799\n",
            "Train Epoch: 7 [13152/21600 (61%)]\tLoss: 0.000214\n",
            "Train Epoch: 7 [13248/21600 (61%)]\tLoss: 0.001456\n",
            "Train Epoch: 7 [13344/21600 (62%)]\tLoss: 0.001634\n",
            "Train Epoch: 7 [13440/21600 (62%)]\tLoss: 0.003198\n",
            "Train Epoch: 7 [13536/21600 (63%)]\tLoss: 0.036609\n",
            "Train Epoch: 7 [13632/21600 (63%)]\tLoss: 0.006040\n",
            "Train Epoch: 7 [13728/21600 (64%)]\tLoss: 0.002308\n",
            "Train Epoch: 7 [13824/21600 (64%)]\tLoss: 0.011591\n",
            "Train Epoch: 7 [13920/21600 (64%)]\tLoss: 0.000924\n",
            "Train Epoch: 7 [14016/21600 (65%)]\tLoss: 0.000842\n",
            "Train Epoch: 7 [14112/21600 (65%)]\tLoss: 0.000150\n",
            "Train Epoch: 7 [14208/21600 (66%)]\tLoss: 0.000692\n",
            "Train Epoch: 7 [14304/21600 (66%)]\tLoss: 0.000208\n",
            "Train Epoch: 7 [14400/21600 (67%)]\tLoss: 0.000957\n",
            "Train Epoch: 7 [14496/21600 (67%)]\tLoss: 0.000526\n",
            "Train Epoch: 7 [14592/21600 (68%)]\tLoss: 0.013910\n",
            "Train Epoch: 7 [14688/21600 (68%)]\tLoss: 0.004379\n",
            "Train Epoch: 7 [14784/21600 (68%)]\tLoss: 0.001142\n",
            "Train Epoch: 7 [14880/21600 (69%)]\tLoss: 0.000081\n",
            "Train Epoch: 7 [14976/21600 (69%)]\tLoss: 0.001840\n",
            "Train Epoch: 7 [15072/21600 (70%)]\tLoss: 0.002078\n",
            "Train Epoch: 7 [15168/21600 (70%)]\tLoss: 0.011320\n",
            "Train Epoch: 7 [15264/21600 (71%)]\tLoss: 0.002076\n",
            "Train Epoch: 7 [15360/21600 (71%)]\tLoss: 0.066267\n",
            "Train Epoch: 7 [15456/21600 (72%)]\tLoss: 0.001739\n",
            "Train Epoch: 7 [15552/21600 (72%)]\tLoss: 0.001849\n",
            "Train Epoch: 7 [15648/21600 (72%)]\tLoss: 0.000046\n",
            "Train Epoch: 7 [15744/21600 (73%)]\tLoss: 0.000245\n",
            "Train Epoch: 7 [15840/21600 (73%)]\tLoss: 0.002221\n",
            "Train Epoch: 7 [15936/21600 (74%)]\tLoss: 0.000840\n",
            "Train Epoch: 7 [16032/21600 (74%)]\tLoss: 0.000376\n",
            "Train Epoch: 7 [16128/21600 (75%)]\tLoss: 0.072402\n",
            "Train Epoch: 7 [16224/21600 (75%)]\tLoss: 0.003887\n",
            "Train Epoch: 7 [16320/21600 (76%)]\tLoss: 0.015181\n",
            "Train Epoch: 7 [16416/21600 (76%)]\tLoss: 0.004588\n",
            "Train Epoch: 7 [16512/21600 (76%)]\tLoss: 0.000782\n",
            "Train Epoch: 7 [16608/21600 (77%)]\tLoss: 0.000217\n",
            "Train Epoch: 7 [16704/21600 (77%)]\tLoss: 0.008517\n",
            "Train Epoch: 7 [16800/21600 (78%)]\tLoss: 0.002511\n",
            "Train Epoch: 7 [16896/21600 (78%)]\tLoss: 0.001126\n",
            "Train Epoch: 7 [16992/21600 (79%)]\tLoss: 0.028666\n",
            "Train Epoch: 7 [17088/21600 (79%)]\tLoss: 0.001886\n",
            "Train Epoch: 7 [17184/21600 (80%)]\tLoss: 0.000799\n",
            "Train Epoch: 7 [17280/21600 (80%)]\tLoss: 0.000387\n",
            "Train Epoch: 7 [17376/21600 (80%)]\tLoss: 0.089581\n",
            "Train Epoch: 7 [17472/21600 (81%)]\tLoss: 0.004257\n",
            "Train Epoch: 7 [17568/21600 (81%)]\tLoss: 0.001012\n",
            "Train Epoch: 7 [17664/21600 (82%)]\tLoss: 0.111792\n",
            "Train Epoch: 7 [17760/21600 (82%)]\tLoss: 0.047357\n",
            "Train Epoch: 7 [17856/21600 (83%)]\tLoss: 0.003793\n",
            "Train Epoch: 7 [17952/21600 (83%)]\tLoss: 0.000265\n",
            "Train Epoch: 7 [18048/21600 (84%)]\tLoss: 0.002936\n",
            "Train Epoch: 7 [18144/21600 (84%)]\tLoss: 0.000809\n",
            "Train Epoch: 7 [18240/21600 (84%)]\tLoss: 0.008106\n",
            "Train Epoch: 7 [18336/21600 (85%)]\tLoss: 0.005472\n",
            "Train Epoch: 7 [18432/21600 (85%)]\tLoss: 0.000179\n",
            "Train Epoch: 7 [18528/21600 (86%)]\tLoss: 0.008363\n",
            "Train Epoch: 7 [18624/21600 (86%)]\tLoss: 0.001934\n",
            "Train Epoch: 7 [18720/21600 (87%)]\tLoss: 0.000422\n",
            "Train Epoch: 7 [18816/21600 (87%)]\tLoss: 0.000740\n",
            "Train Epoch: 7 [18912/21600 (88%)]\tLoss: 0.000081\n",
            "Train Epoch: 7 [19008/21600 (88%)]\tLoss: 0.000178\n",
            "Train Epoch: 7 [19104/21600 (88%)]\tLoss: 0.000990\n",
            "Train Epoch: 7 [19200/21600 (89%)]\tLoss: 0.001163\n",
            "Train Epoch: 7 [19296/21600 (89%)]\tLoss: 0.004789\n",
            "Train Epoch: 7 [19392/21600 (90%)]\tLoss: 0.000548\n",
            "Train Epoch: 7 [19488/21600 (90%)]\tLoss: 0.023679\n",
            "Train Epoch: 7 [19584/21600 (91%)]\tLoss: 0.007691\n",
            "Train Epoch: 7 [19680/21600 (91%)]\tLoss: 0.002259\n",
            "Train Epoch: 7 [19776/21600 (92%)]\tLoss: 0.000383\n",
            "Train Epoch: 7 [19872/21600 (92%)]\tLoss: 0.002462\n",
            "Train Epoch: 7 [19968/21600 (92%)]\tLoss: 0.002422\n",
            "Train Epoch: 7 [20064/21600 (93%)]\tLoss: 0.022891\n",
            "Train Epoch: 7 [20160/21600 (93%)]\tLoss: 0.000426\n",
            "Train Epoch: 7 [20256/21600 (94%)]\tLoss: 0.000707\n",
            "Train Epoch: 7 [20352/21600 (94%)]\tLoss: 0.000356\n",
            "Train Epoch: 7 [20448/21600 (95%)]\tLoss: 0.000380\n",
            "Train Epoch: 7 [20544/21600 (95%)]\tLoss: 0.014482\n",
            "Train Epoch: 7 [20640/21600 (96%)]\tLoss: 0.014334\n",
            "Train Epoch: 7 [20736/21600 (96%)]\tLoss: 0.000091\n",
            "Train Epoch: 7 [20832/21600 (96%)]\tLoss: 0.000107\n",
            "Train Epoch: 7 [20928/21600 (97%)]\tLoss: 0.009980\n",
            "Train Epoch: 7 [21024/21600 (97%)]\tLoss: 0.034406\n",
            "Train Epoch: 7 [21120/21600 (98%)]\tLoss: 0.000615\n",
            "Train Epoch: 7 [21216/21600 (98%)]\tLoss: 0.001065\n",
            "Train Epoch: 7 [21312/21600 (99%)]\tLoss: 0.001453\n",
            "Train Epoch: 7 [21408/21600 (99%)]\tLoss: 0.000092\n",
            "Train Epoch: 7 [21504/21600 (100%)]\tLoss: 0.008675\n",
            "\n",
            "Validation set: Average loss: 0.0003, Accuracy: 5400/5400 (100%)\n",
            "\n",
            "\n",
            "Saved model to model_binary_7.pth.\n",
            "Train Epoch: 8 [0/21600 (0%)]\tLoss: 0.063528\n",
            "Train Epoch: 8 [96/21600 (0%)]\tLoss: 0.004095\n",
            "Train Epoch: 8 [192/21600 (1%)]\tLoss: 0.000049\n",
            "Train Epoch: 8 [288/21600 (1%)]\tLoss: 0.007283\n",
            "Train Epoch: 8 [384/21600 (2%)]\tLoss: 0.084647\n",
            "Train Epoch: 8 [480/21600 (2%)]\tLoss: 0.000470\n",
            "Train Epoch: 8 [576/21600 (3%)]\tLoss: 0.036192\n",
            "Train Epoch: 8 [672/21600 (3%)]\tLoss: 0.000632\n",
            "Train Epoch: 8 [768/21600 (4%)]\tLoss: 0.002052\n",
            "Train Epoch: 8 [864/21600 (4%)]\tLoss: 0.017920\n",
            "Train Epoch: 8 [960/21600 (4%)]\tLoss: 0.000354\n",
            "Train Epoch: 8 [1056/21600 (5%)]\tLoss: 0.000427\n",
            "Train Epoch: 8 [1152/21600 (5%)]\tLoss: 0.002681\n",
            "Train Epoch: 8 [1248/21600 (6%)]\tLoss: 0.000197\n",
            "Train Epoch: 8 [1344/21600 (6%)]\tLoss: 0.015094\n",
            "Train Epoch: 8 [1440/21600 (7%)]\tLoss: 0.000099\n",
            "Train Epoch: 8 [1536/21600 (7%)]\tLoss: 0.000305\n",
            "Train Epoch: 8 [1632/21600 (8%)]\tLoss: 0.003777\n",
            "Train Epoch: 8 [1728/21600 (8%)]\tLoss: 0.000823\n",
            "Train Epoch: 8 [1824/21600 (8%)]\tLoss: 0.001572\n",
            "Train Epoch: 8 [1920/21600 (9%)]\tLoss: 0.001327\n",
            "Train Epoch: 8 [2016/21600 (9%)]\tLoss: 0.003457\n",
            "Train Epoch: 8 [2112/21600 (10%)]\tLoss: 0.001978\n",
            "Train Epoch: 8 [2208/21600 (10%)]\tLoss: 0.001046\n",
            "Train Epoch: 8 [2304/21600 (11%)]\tLoss: 0.006420\n",
            "Train Epoch: 8 [2400/21600 (11%)]\tLoss: 0.013048\n",
            "Train Epoch: 8 [2496/21600 (12%)]\tLoss: 0.000229\n",
            "Train Epoch: 8 [2592/21600 (12%)]\tLoss: 0.006278\n",
            "Train Epoch: 8 [2688/21600 (12%)]\tLoss: 0.008670\n",
            "Train Epoch: 8 [2784/21600 (13%)]\tLoss: 0.003469\n",
            "Train Epoch: 8 [2880/21600 (13%)]\tLoss: 0.025697\n",
            "Train Epoch: 8 [2976/21600 (14%)]\tLoss: 0.000697\n",
            "Train Epoch: 8 [3072/21600 (14%)]\tLoss: 0.001684\n",
            "Train Epoch: 8 [3168/21600 (15%)]\tLoss: 0.000266\n",
            "Train Epoch: 8 [3264/21600 (15%)]\tLoss: 0.002260\n",
            "Train Epoch: 8 [3360/21600 (16%)]\tLoss: 0.000408\n",
            "Train Epoch: 8 [3456/21600 (16%)]\tLoss: 0.000655\n",
            "Train Epoch: 8 [3552/21600 (16%)]\tLoss: 0.029522\n",
            "Train Epoch: 8 [3648/21600 (17%)]\tLoss: 0.050386\n",
            "Train Epoch: 8 [3744/21600 (17%)]\tLoss: 0.002015\n",
            "Train Epoch: 8 [3840/21600 (18%)]\tLoss: 0.000958\n",
            "Train Epoch: 8 [3936/21600 (18%)]\tLoss: 0.001841\n",
            "Train Epoch: 8 [4032/21600 (19%)]\tLoss: 0.010910\n",
            "Train Epoch: 8 [4128/21600 (19%)]\tLoss: 0.000258\n",
            "Train Epoch: 8 [4224/21600 (20%)]\tLoss: 0.000201\n",
            "Train Epoch: 8 [4320/21600 (20%)]\tLoss: 0.003588\n",
            "Train Epoch: 8 [4416/21600 (20%)]\tLoss: 0.000318\n",
            "Train Epoch: 8 [4512/21600 (21%)]\tLoss: 0.000240\n",
            "Train Epoch: 8 [4608/21600 (21%)]\tLoss: 0.018338\n",
            "Train Epoch: 8 [4704/21600 (22%)]\tLoss: 0.000119\n",
            "Train Epoch: 8 [4800/21600 (22%)]\tLoss: 0.063031\n",
            "Train Epoch: 8 [4896/21600 (23%)]\tLoss: 0.004793\n",
            "Train Epoch: 8 [4992/21600 (23%)]\tLoss: 0.000145\n",
            "Train Epoch: 8 [5088/21600 (24%)]\tLoss: 0.000881\n",
            "Train Epoch: 8 [5184/21600 (24%)]\tLoss: 0.002009\n",
            "Train Epoch: 8 [5280/21600 (24%)]\tLoss: 0.009771\n",
            "Train Epoch: 8 [5376/21600 (25%)]\tLoss: 0.035006\n",
            "Train Epoch: 8 [5472/21600 (25%)]\tLoss: 0.000606\n",
            "Train Epoch: 8 [5568/21600 (26%)]\tLoss: 0.002257\n",
            "Train Epoch: 8 [5664/21600 (26%)]\tLoss: 0.000777\n",
            "Train Epoch: 8 [5760/21600 (27%)]\tLoss: 0.000674\n",
            "Train Epoch: 8 [5856/21600 (27%)]\tLoss: 0.003001\n",
            "Train Epoch: 8 [5952/21600 (28%)]\tLoss: 0.062664\n",
            "Train Epoch: 8 [6048/21600 (28%)]\tLoss: 0.000533\n",
            "Train Epoch: 8 [6144/21600 (28%)]\tLoss: 0.000198\n",
            "Train Epoch: 8 [6240/21600 (29%)]\tLoss: 0.001790\n",
            "Train Epoch: 8 [6336/21600 (29%)]\tLoss: 0.000503\n",
            "Train Epoch: 8 [6432/21600 (30%)]\tLoss: 0.000085\n",
            "Train Epoch: 8 [6528/21600 (30%)]\tLoss: 0.001004\n",
            "Train Epoch: 8 [6624/21600 (31%)]\tLoss: 0.034086\n",
            "Train Epoch: 8 [6720/21600 (31%)]\tLoss: 0.000758\n",
            "Train Epoch: 8 [6816/21600 (32%)]\tLoss: 0.001831\n",
            "Train Epoch: 8 [6912/21600 (32%)]\tLoss: 0.000127\n",
            "Train Epoch: 8 [7008/21600 (32%)]\tLoss: 0.000965\n",
            "Train Epoch: 8 [7104/21600 (33%)]\tLoss: 0.006991\n",
            "Train Epoch: 8 [7200/21600 (33%)]\tLoss: 0.000393\n",
            "Train Epoch: 8 [7296/21600 (34%)]\tLoss: 0.000223\n",
            "Train Epoch: 8 [7392/21600 (34%)]\tLoss: 0.000375\n",
            "Train Epoch: 8 [7488/21600 (35%)]\tLoss: 0.002304\n",
            "Train Epoch: 8 [7584/21600 (35%)]\tLoss: 0.015122\n",
            "Train Epoch: 8 [7680/21600 (36%)]\tLoss: 0.018319\n",
            "Train Epoch: 8 [7776/21600 (36%)]\tLoss: 0.000340\n",
            "Train Epoch: 8 [7872/21600 (36%)]\tLoss: 0.000821\n",
            "Train Epoch: 8 [7968/21600 (37%)]\tLoss: 0.000244\n",
            "Train Epoch: 8 [8064/21600 (37%)]\tLoss: 0.012456\n",
            "Train Epoch: 8 [8160/21600 (38%)]\tLoss: 0.023545\n",
            "Train Epoch: 8 [8256/21600 (38%)]\tLoss: 0.001051\n",
            "Train Epoch: 8 [8352/21600 (39%)]\tLoss: 0.003481\n",
            "Train Epoch: 8 [8448/21600 (39%)]\tLoss: 0.000096\n",
            "Train Epoch: 8 [8544/21600 (40%)]\tLoss: 0.036685\n",
            "Train Epoch: 8 [8640/21600 (40%)]\tLoss: 0.041052\n",
            "Train Epoch: 8 [8736/21600 (40%)]\tLoss: 0.000688\n",
            "Train Epoch: 8 [8832/21600 (41%)]\tLoss: 0.003455\n",
            "Train Epoch: 8 [8928/21600 (41%)]\tLoss: 0.000687\n",
            "Train Epoch: 8 [9024/21600 (42%)]\tLoss: 0.009678\n",
            "Train Epoch: 8 [9120/21600 (42%)]\tLoss: 0.000720\n",
            "Train Epoch: 8 [9216/21600 (43%)]\tLoss: 0.000379\n",
            "Train Epoch: 8 [9312/21600 (43%)]\tLoss: 0.003971\n",
            "Train Epoch: 8 [9408/21600 (44%)]\tLoss: 0.002552\n",
            "Train Epoch: 8 [9504/21600 (44%)]\tLoss: 0.000809\n",
            "Train Epoch: 8 [9600/21600 (44%)]\tLoss: 0.034633\n",
            "Train Epoch: 8 [9696/21600 (45%)]\tLoss: 0.000377\n",
            "Train Epoch: 8 [9792/21600 (45%)]\tLoss: 0.002367\n",
            "Train Epoch: 8 [9888/21600 (46%)]\tLoss: 0.044869\n",
            "Train Epoch: 8 [9984/21600 (46%)]\tLoss: 0.000525\n",
            "Train Epoch: 8 [10080/21600 (47%)]\tLoss: 0.000082\n",
            "Train Epoch: 8 [10176/21600 (47%)]\tLoss: 0.002082\n",
            "Train Epoch: 8 [10272/21600 (48%)]\tLoss: 0.001000\n",
            "Train Epoch: 8 [10368/21600 (48%)]\tLoss: 0.000206\n",
            "Train Epoch: 8 [10464/21600 (48%)]\tLoss: 0.007153\n",
            "Train Epoch: 8 [10560/21600 (49%)]\tLoss: 0.024137\n",
            "Train Epoch: 8 [10656/21600 (49%)]\tLoss: 0.007403\n",
            "Train Epoch: 8 [10752/21600 (50%)]\tLoss: 0.000728\n",
            "Train Epoch: 8 [10848/21600 (50%)]\tLoss: 0.000246\n",
            "Train Epoch: 8 [10944/21600 (51%)]\tLoss: 0.000704\n",
            "Train Epoch: 8 [11040/21600 (51%)]\tLoss: 0.002229\n",
            "Train Epoch: 8 [11136/21600 (52%)]\tLoss: 0.003111\n",
            "Train Epoch: 8 [11232/21600 (52%)]\tLoss: 0.019224\n",
            "Train Epoch: 8 [11328/21600 (52%)]\tLoss: 0.000663\n",
            "Train Epoch: 8 [11424/21600 (53%)]\tLoss: 0.000406\n",
            "Train Epoch: 8 [11520/21600 (53%)]\tLoss: 0.099281\n",
            "Train Epoch: 8 [11616/21600 (54%)]\tLoss: 0.046568\n",
            "Train Epoch: 8 [11712/21600 (54%)]\tLoss: 0.000458\n",
            "Train Epoch: 8 [11808/21600 (55%)]\tLoss: 0.004824\n",
            "Train Epoch: 8 [11904/21600 (55%)]\tLoss: 0.000565\n",
            "Train Epoch: 8 [12000/21600 (56%)]\tLoss: 0.000175\n",
            "Train Epoch: 8 [12096/21600 (56%)]\tLoss: 0.111753\n",
            "Train Epoch: 8 [12192/21600 (56%)]\tLoss: 0.000647\n",
            "Train Epoch: 8 [12288/21600 (57%)]\tLoss: 0.000964\n",
            "Train Epoch: 8 [12384/21600 (57%)]\tLoss: 0.040645\n",
            "Train Epoch: 8 [12480/21600 (58%)]\tLoss: 0.002874\n",
            "Train Epoch: 8 [12576/21600 (58%)]\tLoss: 0.010864\n",
            "Train Epoch: 8 [12672/21600 (59%)]\tLoss: 0.157975\n",
            "Train Epoch: 8 [12768/21600 (59%)]\tLoss: 0.000806\n",
            "Train Epoch: 8 [12864/21600 (60%)]\tLoss: 0.009707\n",
            "Train Epoch: 8 [12960/21600 (60%)]\tLoss: 0.006004\n",
            "Train Epoch: 8 [13056/21600 (60%)]\tLoss: 0.005182\n",
            "Train Epoch: 8 [13152/21600 (61%)]\tLoss: 0.002769\n",
            "Train Epoch: 8 [13248/21600 (61%)]\tLoss: 0.006572\n",
            "Train Epoch: 8 [13344/21600 (62%)]\tLoss: 0.021521\n",
            "Train Epoch: 8 [13440/21600 (62%)]\tLoss: 0.002605\n",
            "Train Epoch: 8 [13536/21600 (63%)]\tLoss: 0.000440\n",
            "Train Epoch: 8 [13632/21600 (63%)]\tLoss: 0.000043\n",
            "Train Epoch: 8 [13728/21600 (64%)]\tLoss: 0.001224\n",
            "Train Epoch: 8 [13824/21600 (64%)]\tLoss: 0.006071\n",
            "Train Epoch: 8 [13920/21600 (64%)]\tLoss: 0.000820\n",
            "Train Epoch: 8 [14016/21600 (65%)]\tLoss: 0.000170\n",
            "Train Epoch: 8 [14112/21600 (65%)]\tLoss: 0.001112\n",
            "Train Epoch: 8 [14208/21600 (66%)]\tLoss: 0.000114\n",
            "Train Epoch: 8 [14304/21600 (66%)]\tLoss: 0.003885\n",
            "Train Epoch: 8 [14400/21600 (67%)]\tLoss: 0.000341\n",
            "Train Epoch: 8 [14496/21600 (67%)]\tLoss: 0.000159\n",
            "Train Epoch: 8 [14592/21600 (68%)]\tLoss: 0.000051\n",
            "Train Epoch: 8 [14688/21600 (68%)]\tLoss: 0.053116\n",
            "Train Epoch: 8 [14784/21600 (68%)]\tLoss: 0.001133\n",
            "Train Epoch: 8 [14880/21600 (69%)]\tLoss: 0.003027\n",
            "Train Epoch: 8 [14976/21600 (69%)]\tLoss: 0.015192\n",
            "Train Epoch: 8 [15072/21600 (70%)]\tLoss: 0.000738\n",
            "Train Epoch: 8 [15168/21600 (70%)]\tLoss: 0.001890\n",
            "Train Epoch: 8 [15264/21600 (71%)]\tLoss: 0.000405\n",
            "Train Epoch: 8 [15360/21600 (71%)]\tLoss: 0.000292\n",
            "Train Epoch: 8 [15456/21600 (72%)]\tLoss: 0.004909\n",
            "Train Epoch: 8 [15552/21600 (72%)]\tLoss: 0.000995\n",
            "Train Epoch: 8 [15648/21600 (72%)]\tLoss: 0.002729\n",
            "Train Epoch: 8 [15744/21600 (73%)]\tLoss: 0.000778\n",
            "Train Epoch: 8 [15840/21600 (73%)]\tLoss: 0.048958\n",
            "Train Epoch: 8 [15936/21600 (74%)]\tLoss: 0.001645\n",
            "Train Epoch: 8 [16032/21600 (74%)]\tLoss: 0.035919\n",
            "Train Epoch: 8 [16128/21600 (75%)]\tLoss: 0.000056\n",
            "Train Epoch: 8 [16224/21600 (75%)]\tLoss: 0.000394\n",
            "Train Epoch: 8 [16320/21600 (76%)]\tLoss: 0.004311\n",
            "Train Epoch: 8 [16416/21600 (76%)]\tLoss: 0.001071\n",
            "Train Epoch: 8 [16512/21600 (76%)]\tLoss: 0.000764\n",
            "Train Epoch: 8 [16608/21600 (77%)]\tLoss: 0.001398\n",
            "Train Epoch: 8 [16704/21600 (77%)]\tLoss: 0.000118\n",
            "Train Epoch: 8 [16800/21600 (78%)]\tLoss: 0.000467\n",
            "Train Epoch: 8 [16896/21600 (78%)]\tLoss: 0.001821\n",
            "Train Epoch: 8 [16992/21600 (79%)]\tLoss: 0.000400\n",
            "Train Epoch: 8 [17088/21600 (79%)]\tLoss: 0.014399\n",
            "Train Epoch: 8 [17184/21600 (80%)]\tLoss: 0.001560\n",
            "Train Epoch: 8 [17280/21600 (80%)]\tLoss: 0.002219\n",
            "Train Epoch: 8 [17376/21600 (80%)]\tLoss: 0.019364\n",
            "Train Epoch: 8 [17472/21600 (81%)]\tLoss: 0.000418\n",
            "Train Epoch: 8 [17568/21600 (81%)]\tLoss: 0.005446\n",
            "Train Epoch: 8 [17664/21600 (82%)]\tLoss: 0.124952\n",
            "Train Epoch: 8 [17760/21600 (82%)]\tLoss: 0.000332\n",
            "Train Epoch: 8 [17856/21600 (83%)]\tLoss: 0.000579\n",
            "Train Epoch: 8 [17952/21600 (83%)]\tLoss: 0.001094\n",
            "Train Epoch: 8 [18048/21600 (84%)]\tLoss: 0.000262\n",
            "Train Epoch: 8 [18144/21600 (84%)]\tLoss: 0.074299\n",
            "Train Epoch: 8 [18240/21600 (84%)]\tLoss: 0.005662\n",
            "Train Epoch: 8 [18336/21600 (85%)]\tLoss: 0.018952\n",
            "Train Epoch: 8 [18432/21600 (85%)]\tLoss: 0.001976\n",
            "Train Epoch: 8 [18528/21600 (86%)]\tLoss: 0.012227\n",
            "Train Epoch: 8 [18624/21600 (86%)]\tLoss: 0.008543\n",
            "Train Epoch: 8 [18720/21600 (87%)]\tLoss: 0.000272\n",
            "Train Epoch: 8 [18816/21600 (87%)]\tLoss: 0.003766\n",
            "Train Epoch: 8 [18912/21600 (88%)]\tLoss: 0.000185\n",
            "Train Epoch: 8 [19008/21600 (88%)]\tLoss: 0.067221\n",
            "Train Epoch: 8 [19104/21600 (88%)]\tLoss: 0.010211\n",
            "Train Epoch: 8 [19200/21600 (89%)]\tLoss: 0.062852\n",
            "Train Epoch: 8 [19296/21600 (89%)]\tLoss: 0.000343\n",
            "Train Epoch: 8 [19392/21600 (90%)]\tLoss: 0.000035\n",
            "Train Epoch: 8 [19488/21600 (90%)]\tLoss: 0.016427\n",
            "Train Epoch: 8 [19584/21600 (91%)]\tLoss: 0.000179\n",
            "Train Epoch: 8 [19680/21600 (91%)]\tLoss: 0.000408\n",
            "Train Epoch: 8 [19776/21600 (92%)]\tLoss: 0.000329\n",
            "Train Epoch: 8 [19872/21600 (92%)]\tLoss: 0.031707\n",
            "Train Epoch: 8 [19968/21600 (92%)]\tLoss: 0.080666\n",
            "Train Epoch: 8 [20064/21600 (93%)]\tLoss: 0.000547\n",
            "Train Epoch: 8 [20160/21600 (93%)]\tLoss: 0.009144\n",
            "Train Epoch: 8 [20256/21600 (94%)]\tLoss: 0.041094\n",
            "Train Epoch: 8 [20352/21600 (94%)]\tLoss: 0.006489\n",
            "Train Epoch: 8 [20448/21600 (95%)]\tLoss: 0.011939\n",
            "Train Epoch: 8 [20544/21600 (95%)]\tLoss: 0.001830\n",
            "Train Epoch: 8 [20640/21600 (96%)]\tLoss: 0.018971\n",
            "Train Epoch: 8 [20736/21600 (96%)]\tLoss: 0.074552\n",
            "Train Epoch: 8 [20832/21600 (96%)]\tLoss: 0.002435\n",
            "Train Epoch: 8 [20928/21600 (97%)]\tLoss: 0.000185\n",
            "Train Epoch: 8 [21024/21600 (97%)]\tLoss: 0.000912\n",
            "Train Epoch: 8 [21120/21600 (98%)]\tLoss: 0.025364\n",
            "Train Epoch: 8 [21216/21600 (98%)]\tLoss: 0.002570\n",
            "Train Epoch: 8 [21312/21600 (99%)]\tLoss: 0.000221\n",
            "Train Epoch: 8 [21408/21600 (99%)]\tLoss: 0.022270\n",
            "Train Epoch: 8 [21504/21600 (100%)]\tLoss: 0.002290\n",
            "\n",
            "Validation set: Average loss: 0.0003, Accuracy: 5400/5400 (100%)\n",
            "\n",
            "\n",
            "Saved model to model_binary_8.pth.\n",
            "Train Epoch: 9 [0/21600 (0%)]\tLoss: 0.001867\n",
            "Train Epoch: 9 [96/21600 (0%)]\tLoss: 0.021704\n",
            "Train Epoch: 9 [192/21600 (1%)]\tLoss: 0.000130\n",
            "Train Epoch: 9 [288/21600 (1%)]\tLoss: 0.000492\n",
            "Train Epoch: 9 [384/21600 (2%)]\tLoss: 0.001145\n",
            "Train Epoch: 9 [480/21600 (2%)]\tLoss: 0.000234\n",
            "Train Epoch: 9 [576/21600 (3%)]\tLoss: 0.004016\n",
            "Train Epoch: 9 [672/21600 (3%)]\tLoss: 0.000062\n",
            "Train Epoch: 9 [768/21600 (4%)]\tLoss: 0.000157\n",
            "Train Epoch: 9 [864/21600 (4%)]\tLoss: 0.001077\n",
            "Train Epoch: 9 [960/21600 (4%)]\tLoss: 0.003503\n",
            "Train Epoch: 9 [1056/21600 (5%)]\tLoss: 0.001163\n",
            "Train Epoch: 9 [1152/21600 (5%)]\tLoss: 0.005831\n",
            "Train Epoch: 9 [1248/21600 (6%)]\tLoss: 0.000175\n",
            "Train Epoch: 9 [1344/21600 (6%)]\tLoss: 0.000261\n",
            "Train Epoch: 9 [1440/21600 (7%)]\tLoss: 0.000965\n",
            "Train Epoch: 9 [1536/21600 (7%)]\tLoss: 0.000455\n",
            "Train Epoch: 9 [1632/21600 (8%)]\tLoss: 0.000540\n",
            "Train Epoch: 9 [1728/21600 (8%)]\tLoss: 0.002022\n",
            "Train Epoch: 9 [1824/21600 (8%)]\tLoss: 0.038804\n",
            "Train Epoch: 9 [1920/21600 (9%)]\tLoss: 0.002184\n",
            "Train Epoch: 9 [2016/21600 (9%)]\tLoss: 0.000422\n",
            "Train Epoch: 9 [2112/21600 (10%)]\tLoss: 0.000301\n",
            "Train Epoch: 9 [2208/21600 (10%)]\tLoss: 0.007191\n",
            "Train Epoch: 9 [2304/21600 (11%)]\tLoss: 0.016980\n",
            "Train Epoch: 9 [2400/21600 (11%)]\tLoss: 0.001214\n",
            "Train Epoch: 9 [2496/21600 (12%)]\tLoss: 0.001693\n",
            "Train Epoch: 9 [2592/21600 (12%)]\tLoss: 0.001865\n",
            "Train Epoch: 9 [2688/21600 (12%)]\tLoss: 0.029843\n",
            "Train Epoch: 9 [2784/21600 (13%)]\tLoss: 0.002414\n",
            "Train Epoch: 9 [2880/21600 (13%)]\tLoss: 0.011274\n",
            "Train Epoch: 9 [2976/21600 (14%)]\tLoss: 0.002031\n",
            "Train Epoch: 9 [3072/21600 (14%)]\tLoss: 0.098843\n",
            "Train Epoch: 9 [3168/21600 (15%)]\tLoss: 0.000103\n",
            "Train Epoch: 9 [3264/21600 (15%)]\tLoss: 0.004753\n",
            "Train Epoch: 9 [3360/21600 (16%)]\tLoss: 0.004900\n",
            "Train Epoch: 9 [3456/21600 (16%)]\tLoss: 0.014589\n",
            "Train Epoch: 9 [3552/21600 (16%)]\tLoss: 0.004302\n",
            "Train Epoch: 9 [3648/21600 (17%)]\tLoss: 0.056445\n",
            "Train Epoch: 9 [3744/21600 (17%)]\tLoss: 0.004363\n",
            "Train Epoch: 9 [3840/21600 (18%)]\tLoss: 0.000990\n",
            "Train Epoch: 9 [3936/21600 (18%)]\tLoss: 0.000710\n",
            "Train Epoch: 9 [4032/21600 (19%)]\tLoss: 0.033370\n",
            "Train Epoch: 9 [4128/21600 (19%)]\tLoss: 0.001703\n",
            "Train Epoch: 9 [4224/21600 (20%)]\tLoss: 0.006019\n",
            "Train Epoch: 9 [4320/21600 (20%)]\tLoss: 0.000161\n",
            "Train Epoch: 9 [4416/21600 (20%)]\tLoss: 0.000248\n",
            "Train Epoch: 9 [4512/21600 (21%)]\tLoss: 0.000872\n",
            "Train Epoch: 9 [4608/21600 (21%)]\tLoss: 0.000090\n",
            "Train Epoch: 9 [4704/21600 (22%)]\tLoss: 0.000689\n",
            "Train Epoch: 9 [4800/21600 (22%)]\tLoss: 0.002180\n",
            "Train Epoch: 9 [4896/21600 (23%)]\tLoss: 0.000183\n",
            "Train Epoch: 9 [4992/21600 (23%)]\tLoss: 0.037899\n",
            "Train Epoch: 9 [5088/21600 (24%)]\tLoss: 0.000907\n",
            "Train Epoch: 9 [5184/21600 (24%)]\tLoss: 0.020076\n",
            "Train Epoch: 9 [5280/21600 (24%)]\tLoss: 0.000238\n",
            "Train Epoch: 9 [5376/21600 (25%)]\tLoss: 0.004684\n",
            "Train Epoch: 9 [5472/21600 (25%)]\tLoss: 0.023697\n",
            "Train Epoch: 9 [5568/21600 (26%)]\tLoss: 0.003261\n",
            "Train Epoch: 9 [5664/21600 (26%)]\tLoss: 0.000401\n",
            "Train Epoch: 9 [5760/21600 (27%)]\tLoss: 0.000090\n",
            "Train Epoch: 9 [5856/21600 (27%)]\tLoss: 0.000067\n",
            "Train Epoch: 9 [5952/21600 (28%)]\tLoss: 0.019122\n",
            "Train Epoch: 9 [6048/21600 (28%)]\tLoss: 0.004696\n",
            "Train Epoch: 9 [6144/21600 (28%)]\tLoss: 0.000217\n",
            "Train Epoch: 9 [6240/21600 (29%)]\tLoss: 0.001240\n",
            "Train Epoch: 9 [6336/21600 (29%)]\tLoss: 0.001445\n",
            "Train Epoch: 9 [6432/21600 (30%)]\tLoss: 0.000285\n",
            "Train Epoch: 9 [6528/21600 (30%)]\tLoss: 0.000034\n",
            "Train Epoch: 9 [6624/21600 (31%)]\tLoss: 0.001624\n",
            "Train Epoch: 9 [6720/21600 (31%)]\tLoss: 0.000131\n",
            "Train Epoch: 9 [6816/21600 (32%)]\tLoss: 0.000790\n",
            "Train Epoch: 9 [6912/21600 (32%)]\tLoss: 0.001088\n",
            "Train Epoch: 9 [7008/21600 (32%)]\tLoss: 0.000174\n",
            "Train Epoch: 9 [7104/21600 (33%)]\tLoss: 0.001381\n",
            "Train Epoch: 9 [7200/21600 (33%)]\tLoss: 0.002317\n",
            "Train Epoch: 9 [7296/21600 (34%)]\tLoss: 0.000111\n",
            "Train Epoch: 9 [7392/21600 (34%)]\tLoss: 0.000638\n",
            "Train Epoch: 9 [7488/21600 (35%)]\tLoss: 0.008421\n",
            "Train Epoch: 9 [7584/21600 (35%)]\tLoss: 0.000051\n",
            "Train Epoch: 9 [7680/21600 (36%)]\tLoss: 0.021219\n",
            "Train Epoch: 9 [7776/21600 (36%)]\tLoss: 0.005936\n",
            "Train Epoch: 9 [7872/21600 (36%)]\tLoss: 0.023382\n",
            "Train Epoch: 9 [7968/21600 (37%)]\tLoss: 0.000407\n",
            "Train Epoch: 9 [8064/21600 (37%)]\tLoss: 0.002077\n",
            "Train Epoch: 9 [8160/21600 (38%)]\tLoss: 0.000191\n",
            "Train Epoch: 9 [8256/21600 (38%)]\tLoss: 0.000327\n",
            "Train Epoch: 9 [8352/21600 (39%)]\tLoss: 0.000459\n",
            "Train Epoch: 9 [8448/21600 (39%)]\tLoss: 0.000060\n",
            "Train Epoch: 9 [8544/21600 (40%)]\tLoss: 0.000296\n",
            "Train Epoch: 9 [8640/21600 (40%)]\tLoss: 0.002391\n",
            "Train Epoch: 9 [8736/21600 (40%)]\tLoss: 0.000470\n",
            "Train Epoch: 9 [8832/21600 (41%)]\tLoss: 0.002179\n",
            "Train Epoch: 9 [8928/21600 (41%)]\tLoss: 0.000655\n",
            "Train Epoch: 9 [9024/21600 (42%)]\tLoss: 0.037438\n",
            "Train Epoch: 9 [9120/21600 (42%)]\tLoss: 0.001280\n",
            "Train Epoch: 9 [9216/21600 (43%)]\tLoss: 0.010901\n",
            "Train Epoch: 9 [9312/21600 (43%)]\tLoss: 0.004254\n",
            "Train Epoch: 9 [9408/21600 (44%)]\tLoss: 0.013626\n",
            "Train Epoch: 9 [9504/21600 (44%)]\tLoss: 0.000376\n",
            "Train Epoch: 9 [9600/21600 (44%)]\tLoss: 0.001245\n",
            "Train Epoch: 9 [9696/21600 (45%)]\tLoss: 0.039734\n",
            "Train Epoch: 9 [9792/21600 (45%)]\tLoss: 0.006084\n",
            "Train Epoch: 9 [9888/21600 (46%)]\tLoss: 0.007164\n",
            "Train Epoch: 9 [9984/21600 (46%)]\tLoss: 0.021153\n",
            "Train Epoch: 9 [10080/21600 (47%)]\tLoss: 0.000365\n",
            "Train Epoch: 9 [10176/21600 (47%)]\tLoss: 0.025153\n",
            "Train Epoch: 9 [10272/21600 (48%)]\tLoss: 0.004619\n",
            "Train Epoch: 9 [10368/21600 (48%)]\tLoss: 0.000494\n",
            "Train Epoch: 9 [10464/21600 (48%)]\tLoss: 0.000690\n",
            "Train Epoch: 9 [10560/21600 (49%)]\tLoss: 0.001210\n",
            "Train Epoch: 9 [10656/21600 (49%)]\tLoss: 0.000247\n",
            "Train Epoch: 9 [10752/21600 (50%)]\tLoss: 0.000234\n",
            "Train Epoch: 9 [10848/21600 (50%)]\tLoss: 0.000274\n",
            "Train Epoch: 9 [10944/21600 (51%)]\tLoss: 0.000240\n",
            "Train Epoch: 9 [11040/21600 (51%)]\tLoss: 0.000158\n",
            "Train Epoch: 9 [11136/21600 (52%)]\tLoss: 0.000613\n",
            "Train Epoch: 9 [11232/21600 (52%)]\tLoss: 0.005418\n",
            "Train Epoch: 9 [11328/21600 (52%)]\tLoss: 0.045320\n",
            "Train Epoch: 9 [11424/21600 (53%)]\tLoss: 0.005786\n",
            "Train Epoch: 9 [11520/21600 (53%)]\tLoss: 0.004694\n",
            "Train Epoch: 9 [11616/21600 (54%)]\tLoss: 0.000080\n",
            "Train Epoch: 9 [11712/21600 (54%)]\tLoss: 0.013851\n",
            "Train Epoch: 9 [11808/21600 (55%)]\tLoss: 0.000441\n",
            "Train Epoch: 9 [11904/21600 (55%)]\tLoss: 0.000143\n",
            "Train Epoch: 9 [12000/21600 (56%)]\tLoss: 0.000238\n",
            "Train Epoch: 9 [12096/21600 (56%)]\tLoss: 0.000093\n",
            "Train Epoch: 9 [12192/21600 (56%)]\tLoss: 0.033624\n",
            "Train Epoch: 9 [12288/21600 (57%)]\tLoss: 0.000462\n",
            "Train Epoch: 9 [12384/21600 (57%)]\tLoss: 0.070540\n",
            "Train Epoch: 9 [12480/21600 (58%)]\tLoss: 0.000274\n",
            "Train Epoch: 9 [12576/21600 (58%)]\tLoss: 0.003182\n",
            "Train Epoch: 9 [12672/21600 (59%)]\tLoss: 0.025960\n",
            "Train Epoch: 9 [12768/21600 (59%)]\tLoss: 0.054351\n",
            "Train Epoch: 9 [12864/21600 (60%)]\tLoss: 0.000041\n",
            "Train Epoch: 9 [12960/21600 (60%)]\tLoss: 0.018609\n",
            "Train Epoch: 9 [13056/21600 (60%)]\tLoss: 0.003480\n",
            "Train Epoch: 9 [13152/21600 (61%)]\tLoss: 0.001497\n",
            "Train Epoch: 9 [13248/21600 (61%)]\tLoss: 0.000823\n",
            "Train Epoch: 9 [13344/21600 (62%)]\tLoss: 0.008760\n",
            "Train Epoch: 9 [13440/21600 (62%)]\tLoss: 0.010517\n",
            "Train Epoch: 9 [13536/21600 (63%)]\tLoss: 0.064437\n",
            "Train Epoch: 9 [13632/21600 (63%)]\tLoss: 0.001023\n",
            "Train Epoch: 9 [13728/21600 (64%)]\tLoss: 0.000176\n",
            "Train Epoch: 9 [13824/21600 (64%)]\tLoss: 0.046535\n",
            "Train Epoch: 9 [13920/21600 (64%)]\tLoss: 0.000237\n",
            "Train Epoch: 9 [14016/21600 (65%)]\tLoss: 0.002386\n",
            "Train Epoch: 9 [14112/21600 (65%)]\tLoss: 0.002395\n",
            "Train Epoch: 9 [14208/21600 (66%)]\tLoss: 0.002470\n",
            "Train Epoch: 9 [14304/21600 (66%)]\tLoss: 0.001353\n",
            "Train Epoch: 9 [14400/21600 (67%)]\tLoss: 0.000917\n",
            "Train Epoch: 9 [14496/21600 (67%)]\tLoss: 0.002382\n",
            "Train Epoch: 9 [14592/21600 (68%)]\tLoss: 0.004588\n",
            "Train Epoch: 9 [14688/21600 (68%)]\tLoss: 0.000889\n",
            "Train Epoch: 9 [14784/21600 (68%)]\tLoss: 0.044092\n",
            "Train Epoch: 9 [14880/21600 (69%)]\tLoss: 0.000050\n",
            "Train Epoch: 9 [14976/21600 (69%)]\tLoss: 0.000375\n",
            "Train Epoch: 9 [15072/21600 (70%)]\tLoss: 0.000501\n",
            "Train Epoch: 9 [15168/21600 (70%)]\tLoss: 0.004254\n",
            "Train Epoch: 9 [15264/21600 (71%)]\tLoss: 0.000106\n",
            "Train Epoch: 9 [15360/21600 (71%)]\tLoss: 0.000242\n",
            "Train Epoch: 9 [15456/21600 (72%)]\tLoss: 0.000112\n",
            "Train Epoch: 9 [15552/21600 (72%)]\tLoss: 0.001320\n",
            "Train Epoch: 9 [15648/21600 (72%)]\tLoss: 0.000443\n",
            "Train Epoch: 9 [15744/21600 (73%)]\tLoss: 0.012977\n",
            "Train Epoch: 9 [15840/21600 (73%)]\tLoss: 0.000099\n",
            "Train Epoch: 9 [15936/21600 (74%)]\tLoss: 0.004453\n",
            "Train Epoch: 9 [16032/21600 (74%)]\tLoss: 0.002736\n",
            "Train Epoch: 9 [16128/21600 (75%)]\tLoss: 0.006366\n",
            "Train Epoch: 9 [16224/21600 (75%)]\tLoss: 0.010003\n",
            "Train Epoch: 9 [16320/21600 (76%)]\tLoss: 0.000854\n",
            "Train Epoch: 9 [16416/21600 (76%)]\tLoss: 0.006091\n",
            "Train Epoch: 9 [16512/21600 (76%)]\tLoss: 0.000064\n",
            "Train Epoch: 9 [16608/21600 (77%)]\tLoss: 0.003337\n",
            "Train Epoch: 9 [16704/21600 (77%)]\tLoss: 0.000175\n",
            "Train Epoch: 9 [16800/21600 (78%)]\tLoss: 0.058207\n",
            "Train Epoch: 9 [16896/21600 (78%)]\tLoss: 0.000066\n",
            "Train Epoch: 9 [16992/21600 (79%)]\tLoss: 0.009885\n",
            "Train Epoch: 9 [17088/21600 (79%)]\tLoss: 0.000070\n",
            "Train Epoch: 9 [17184/21600 (80%)]\tLoss: 0.000157\n",
            "Train Epoch: 9 [17280/21600 (80%)]\tLoss: 0.000078\n",
            "Train Epoch: 9 [17376/21600 (80%)]\tLoss: 0.000172\n",
            "Train Epoch: 9 [17472/21600 (81%)]\tLoss: 0.000029\n",
            "Train Epoch: 9 [17568/21600 (81%)]\tLoss: 0.015157\n",
            "Train Epoch: 9 [17664/21600 (82%)]\tLoss: 0.001563\n",
            "Train Epoch: 9 [17760/21600 (82%)]\tLoss: 0.000171\n",
            "Train Epoch: 9 [17856/21600 (83%)]\tLoss: 0.000304\n",
            "Train Epoch: 9 [17952/21600 (83%)]\tLoss: 0.000472\n",
            "Train Epoch: 9 [18048/21600 (84%)]\tLoss: 0.000581\n",
            "Train Epoch: 9 [18144/21600 (84%)]\tLoss: 0.000084\n",
            "Train Epoch: 9 [18240/21600 (84%)]\tLoss: 0.002852\n",
            "Train Epoch: 9 [18336/21600 (85%)]\tLoss: 0.001150\n",
            "Train Epoch: 9 [18432/21600 (85%)]\tLoss: 0.140224\n",
            "Train Epoch: 9 [18528/21600 (86%)]\tLoss: 0.002011\n",
            "Train Epoch: 9 [18624/21600 (86%)]\tLoss: 0.000099\n",
            "Train Epoch: 9 [18720/21600 (87%)]\tLoss: 0.002739\n",
            "Train Epoch: 9 [18816/21600 (87%)]\tLoss: 0.000493\n",
            "Train Epoch: 9 [18912/21600 (88%)]\tLoss: 0.026642\n",
            "Train Epoch: 9 [19008/21600 (88%)]\tLoss: 0.001336\n",
            "Train Epoch: 9 [19104/21600 (88%)]\tLoss: 0.000138\n",
            "Train Epoch: 9 [19200/21600 (89%)]\tLoss: 0.000601\n",
            "Train Epoch: 9 [19296/21600 (89%)]\tLoss: 0.000165\n",
            "Train Epoch: 9 [19392/21600 (90%)]\tLoss: 0.004628\n",
            "Train Epoch: 9 [19488/21600 (90%)]\tLoss: 0.000086\n",
            "Train Epoch: 9 [19584/21600 (91%)]\tLoss: 0.000488\n",
            "Train Epoch: 9 [19680/21600 (91%)]\tLoss: 0.000111\n",
            "Train Epoch: 9 [19776/21600 (92%)]\tLoss: 0.000296\n",
            "Train Epoch: 9 [19872/21600 (92%)]\tLoss: 0.000341\n",
            "Train Epoch: 9 [19968/21600 (92%)]\tLoss: 0.001571\n",
            "Train Epoch: 9 [20064/21600 (93%)]\tLoss: 0.000036\n",
            "Train Epoch: 9 [20160/21600 (93%)]\tLoss: 0.002576\n",
            "Train Epoch: 9 [20256/21600 (94%)]\tLoss: 0.000352\n",
            "Train Epoch: 9 [20352/21600 (94%)]\tLoss: 0.010076\n",
            "Train Epoch: 9 [20448/21600 (95%)]\tLoss: 0.000180\n",
            "Train Epoch: 9 [20544/21600 (95%)]\tLoss: 0.182325\n",
            "Train Epoch: 9 [20640/21600 (96%)]\tLoss: 0.002943\n",
            "Train Epoch: 9 [20736/21600 (96%)]\tLoss: 0.000146\n",
            "Train Epoch: 9 [20832/21600 (96%)]\tLoss: 0.000036\n",
            "Train Epoch: 9 [20928/21600 (97%)]\tLoss: 0.007498\n",
            "Train Epoch: 9 [21024/21600 (97%)]\tLoss: 0.000096\n",
            "Train Epoch: 9 [21120/21600 (98%)]\tLoss: 0.005746\n",
            "Train Epoch: 9 [21216/21600 (98%)]\tLoss: 0.000487\n",
            "Train Epoch: 9 [21312/21600 (99%)]\tLoss: 0.000439\n",
            "Train Epoch: 9 [21408/21600 (99%)]\tLoss: 0.000099\n",
            "Train Epoch: 9 [21504/21600 (100%)]\tLoss: 0.000017\n",
            "\n",
            "Validation set: Average loss: 0.0002, Accuracy: 5400/5400 (100%)\n",
            "\n",
            "\n",
            "Saved model to model_binary_9.pth.\n",
            "Train Epoch: 10 [0/21600 (0%)]\tLoss: 0.002283\n",
            "Train Epoch: 10 [96/21600 (0%)]\tLoss: 0.002914\n",
            "Train Epoch: 10 [192/21600 (1%)]\tLoss: 0.005534\n",
            "Train Epoch: 10 [288/21600 (1%)]\tLoss: 0.000900\n",
            "Train Epoch: 10 [384/21600 (2%)]\tLoss: 0.028385\n",
            "Train Epoch: 10 [480/21600 (2%)]\tLoss: 0.000849\n",
            "Train Epoch: 10 [576/21600 (3%)]\tLoss: 0.017716\n",
            "Train Epoch: 10 [672/21600 (3%)]\tLoss: 0.000056\n",
            "Train Epoch: 10 [768/21600 (4%)]\tLoss: 0.003498\n",
            "Train Epoch: 10 [864/21600 (4%)]\tLoss: 0.000696\n",
            "Train Epoch: 10 [960/21600 (4%)]\tLoss: 0.000183\n",
            "Train Epoch: 10 [1056/21600 (5%)]\tLoss: 0.019616\n",
            "Train Epoch: 10 [1152/21600 (5%)]\tLoss: 0.054338\n",
            "Train Epoch: 10 [1248/21600 (6%)]\tLoss: 0.024569\n",
            "Train Epoch: 10 [1344/21600 (6%)]\tLoss: 0.000028\n",
            "Train Epoch: 10 [1440/21600 (7%)]\tLoss: 0.001124\n",
            "Train Epoch: 10 [1536/21600 (7%)]\tLoss: 0.001357\n",
            "Train Epoch: 10 [1632/21600 (8%)]\tLoss: 0.000526\n",
            "Train Epoch: 10 [1728/21600 (8%)]\tLoss: 0.001013\n",
            "Train Epoch: 10 [1824/21600 (8%)]\tLoss: 0.001359\n",
            "Train Epoch: 10 [1920/21600 (9%)]\tLoss: 0.000921\n",
            "Train Epoch: 10 [2016/21600 (9%)]\tLoss: 0.000134\n",
            "Train Epoch: 10 [2112/21600 (10%)]\tLoss: 0.062602\n",
            "Train Epoch: 10 [2208/21600 (10%)]\tLoss: 0.000520\n",
            "Train Epoch: 10 [2304/21600 (11%)]\tLoss: 0.002269\n",
            "Train Epoch: 10 [2400/21600 (11%)]\tLoss: 0.000628\n",
            "Train Epoch: 10 [2496/21600 (12%)]\tLoss: 0.000034\n",
            "Train Epoch: 10 [2592/21600 (12%)]\tLoss: 0.000125\n",
            "Train Epoch: 10 [2688/21600 (12%)]\tLoss: 0.000383\n",
            "Train Epoch: 10 [2784/21600 (13%)]\tLoss: 0.000055\n",
            "Train Epoch: 10 [2880/21600 (13%)]\tLoss: 0.052409\n",
            "Train Epoch: 10 [2976/21600 (14%)]\tLoss: 0.039972\n",
            "Train Epoch: 10 [3072/21600 (14%)]\tLoss: 0.000100\n",
            "Train Epoch: 10 [3168/21600 (15%)]\tLoss: 0.004716\n",
            "Train Epoch: 10 [3264/21600 (15%)]\tLoss: 0.000804\n",
            "Train Epoch: 10 [3360/21600 (16%)]\tLoss: 0.054625\n",
            "Train Epoch: 10 [3456/21600 (16%)]\tLoss: 0.207672\n",
            "Train Epoch: 10 [3552/21600 (16%)]\tLoss: 0.000148\n",
            "Train Epoch: 10 [3648/21600 (17%)]\tLoss: 0.051849\n",
            "Train Epoch: 10 [3744/21600 (17%)]\tLoss: 0.005386\n",
            "Train Epoch: 10 [3840/21600 (18%)]\tLoss: 0.000656\n",
            "Train Epoch: 10 [3936/21600 (18%)]\tLoss: 0.005522\n",
            "Train Epoch: 10 [4032/21600 (19%)]\tLoss: 0.002554\n",
            "Train Epoch: 10 [4128/21600 (19%)]\tLoss: 0.000292\n",
            "Train Epoch: 10 [4224/21600 (20%)]\tLoss: 0.047645\n",
            "Train Epoch: 10 [4320/21600 (20%)]\tLoss: 0.000664\n",
            "Train Epoch: 10 [4416/21600 (20%)]\tLoss: 0.000689\n",
            "Train Epoch: 10 [4512/21600 (21%)]\tLoss: 0.000426\n",
            "Train Epoch: 10 [4608/21600 (21%)]\tLoss: 0.000109\n",
            "Train Epoch: 10 [4704/21600 (22%)]\tLoss: 0.000147\n",
            "Train Epoch: 10 [4800/21600 (22%)]\tLoss: 0.027926\n",
            "Train Epoch: 10 [4896/21600 (23%)]\tLoss: 0.038330\n",
            "Train Epoch: 10 [4992/21600 (23%)]\tLoss: 0.000020\n",
            "Train Epoch: 10 [5088/21600 (24%)]\tLoss: 0.000289\n",
            "Train Epoch: 10 [5184/21600 (24%)]\tLoss: 0.001637\n",
            "Train Epoch: 10 [5280/21600 (24%)]\tLoss: 0.000468\n",
            "Train Epoch: 10 [5376/21600 (25%)]\tLoss: 0.011865\n",
            "Train Epoch: 10 [5472/21600 (25%)]\tLoss: 0.015067\n",
            "Train Epoch: 10 [5568/21600 (26%)]\tLoss: 0.002840\n",
            "Train Epoch: 10 [5664/21600 (26%)]\tLoss: 0.000149\n",
            "Train Epoch: 10 [5760/21600 (27%)]\tLoss: 0.010104\n",
            "Train Epoch: 10 [5856/21600 (27%)]\tLoss: 0.000950\n",
            "Train Epoch: 10 [5952/21600 (28%)]\tLoss: 0.050316\n",
            "Train Epoch: 10 [6048/21600 (28%)]\tLoss: 0.003963\n",
            "Train Epoch: 10 [6144/21600 (28%)]\tLoss: 0.002125\n",
            "Train Epoch: 10 [6240/21600 (29%)]\tLoss: 0.004486\n",
            "Train Epoch: 10 [6336/21600 (29%)]\tLoss: 0.000760\n",
            "Train Epoch: 10 [6432/21600 (30%)]\tLoss: 0.000582\n",
            "Train Epoch: 10 [6528/21600 (30%)]\tLoss: 0.017389\n",
            "Train Epoch: 10 [6624/21600 (31%)]\tLoss: 0.008554\n",
            "Train Epoch: 10 [6720/21600 (31%)]\tLoss: 0.000096\n",
            "Train Epoch: 10 [6816/21600 (32%)]\tLoss: 0.000146\n",
            "Train Epoch: 10 [6912/21600 (32%)]\tLoss: 0.000096\n",
            "Train Epoch: 10 [7008/21600 (32%)]\tLoss: 0.001636\n",
            "Train Epoch: 10 [7104/21600 (33%)]\tLoss: 0.000121\n",
            "Train Epoch: 10 [7200/21600 (33%)]\tLoss: 0.001562\n",
            "Train Epoch: 10 [7296/21600 (34%)]\tLoss: 0.027756\n",
            "Train Epoch: 10 [7392/21600 (34%)]\tLoss: 0.000228\n",
            "Train Epoch: 10 [7488/21600 (35%)]\tLoss: 0.003352\n",
            "Train Epoch: 10 [7584/21600 (35%)]\tLoss: 0.034561\n",
            "Train Epoch: 10 [7680/21600 (36%)]\tLoss: 0.001530\n",
            "Train Epoch: 10 [7776/21600 (36%)]\tLoss: 0.006289\n",
            "Train Epoch: 10 [7872/21600 (36%)]\tLoss: 0.000204\n",
            "Train Epoch: 10 [7968/21600 (37%)]\tLoss: 0.000612\n",
            "Train Epoch: 10 [8064/21600 (37%)]\tLoss: 0.001954\n",
            "Train Epoch: 10 [8160/21600 (38%)]\tLoss: 0.002016\n",
            "Train Epoch: 10 [8256/21600 (38%)]\tLoss: 0.004615\n",
            "Train Epoch: 10 [8352/21600 (39%)]\tLoss: 0.005615\n",
            "Train Epoch: 10 [8448/21600 (39%)]\tLoss: 0.111106\n",
            "Train Epoch: 10 [8544/21600 (40%)]\tLoss: 0.000134\n",
            "Train Epoch: 10 [8640/21600 (40%)]\tLoss: 0.001585\n",
            "Train Epoch: 10 [8736/21600 (40%)]\tLoss: 0.025346\n",
            "Train Epoch: 10 [8832/21600 (41%)]\tLoss: 0.000203\n",
            "Train Epoch: 10 [8928/21600 (41%)]\tLoss: 0.000132\n",
            "Train Epoch: 10 [9024/21600 (42%)]\tLoss: 0.000207\n",
            "Train Epoch: 10 [9120/21600 (42%)]\tLoss: 0.018183\n",
            "Train Epoch: 10 [9216/21600 (43%)]\tLoss: 0.000865\n",
            "Train Epoch: 10 [9312/21600 (43%)]\tLoss: 0.005716\n",
            "Train Epoch: 10 [9408/21600 (44%)]\tLoss: 0.001282\n",
            "Train Epoch: 10 [9504/21600 (44%)]\tLoss: 0.002012\n",
            "Train Epoch: 10 [9600/21600 (44%)]\tLoss: 0.000077\n",
            "Train Epoch: 10 [9696/21600 (45%)]\tLoss: 0.162277\n",
            "Train Epoch: 10 [9792/21600 (45%)]\tLoss: 0.019599\n",
            "Train Epoch: 10 [9888/21600 (46%)]\tLoss: 0.000420\n",
            "Train Epoch: 10 [9984/21600 (46%)]\tLoss: 0.000039\n",
            "Train Epoch: 10 [10080/21600 (47%)]\tLoss: 0.009120\n",
            "Train Epoch: 10 [10176/21600 (47%)]\tLoss: 0.001789\n",
            "Train Epoch: 10 [10272/21600 (48%)]\tLoss: 0.002062\n",
            "Train Epoch: 10 [10368/21600 (48%)]\tLoss: 0.003805\n",
            "Train Epoch: 10 [10464/21600 (48%)]\tLoss: 0.007269\n",
            "Train Epoch: 10 [10560/21600 (49%)]\tLoss: 0.033605\n",
            "Train Epoch: 10 [10656/21600 (49%)]\tLoss: 0.000062\n",
            "Train Epoch: 10 [10752/21600 (50%)]\tLoss: 0.000127\n",
            "Train Epoch: 10 [10848/21600 (50%)]\tLoss: 0.044902\n",
            "Train Epoch: 10 [10944/21600 (51%)]\tLoss: 0.000770\n",
            "Train Epoch: 10 [11040/21600 (51%)]\tLoss: 0.000239\n",
            "Train Epoch: 10 [11136/21600 (52%)]\tLoss: 0.000039\n",
            "Train Epoch: 10 [11232/21600 (52%)]\tLoss: 0.000170\n",
            "Train Epoch: 10 [11328/21600 (52%)]\tLoss: 0.000688\n",
            "Train Epoch: 10 [11424/21600 (53%)]\tLoss: 0.007183\n",
            "Train Epoch: 10 [11520/21600 (53%)]\tLoss: 0.000762\n",
            "Train Epoch: 10 [11616/21600 (54%)]\tLoss: 0.001371\n",
            "Train Epoch: 10 [11712/21600 (54%)]\tLoss: 0.000255\n",
            "Train Epoch: 10 [11808/21600 (55%)]\tLoss: 0.000135\n",
            "Train Epoch: 10 [11904/21600 (55%)]\tLoss: 0.105351\n",
            "Train Epoch: 10 [12000/21600 (56%)]\tLoss: 0.000441\n",
            "Train Epoch: 10 [12096/21600 (56%)]\tLoss: 0.000169\n",
            "Train Epoch: 10 [12192/21600 (56%)]\tLoss: 0.040249\n",
            "Train Epoch: 10 [12288/21600 (57%)]\tLoss: 0.008194\n",
            "Train Epoch: 10 [12384/21600 (57%)]\tLoss: 0.003964\n",
            "Train Epoch: 10 [12480/21600 (58%)]\tLoss: 0.000264\n",
            "Train Epoch: 10 [12576/21600 (58%)]\tLoss: 0.000555\n",
            "Train Epoch: 10 [12672/21600 (59%)]\tLoss: 0.001454\n",
            "Train Epoch: 10 [12768/21600 (59%)]\tLoss: 0.002812\n",
            "Train Epoch: 10 [12864/21600 (60%)]\tLoss: 0.014667\n",
            "Train Epoch: 10 [12960/21600 (60%)]\tLoss: 0.001301\n",
            "Train Epoch: 10 [13056/21600 (60%)]\tLoss: 0.003553\n",
            "Train Epoch: 10 [13152/21600 (61%)]\tLoss: 0.000085\n",
            "Train Epoch: 10 [13248/21600 (61%)]\tLoss: 0.003418\n",
            "Train Epoch: 10 [13344/21600 (62%)]\tLoss: 0.000533\n",
            "Train Epoch: 10 [13440/21600 (62%)]\tLoss: 0.024535\n",
            "Train Epoch: 10 [13536/21600 (63%)]\tLoss: 0.003268\n",
            "Train Epoch: 10 [13632/21600 (63%)]\tLoss: 0.008692\n",
            "Train Epoch: 10 [13728/21600 (64%)]\tLoss: 0.000387\n",
            "Train Epoch: 10 [13824/21600 (64%)]\tLoss: 0.005814\n",
            "Train Epoch: 10 [13920/21600 (64%)]\tLoss: 0.000046\n",
            "Train Epoch: 10 [14016/21600 (65%)]\tLoss: 0.001342\n",
            "Train Epoch: 10 [14112/21600 (65%)]\tLoss: 0.000378\n",
            "Train Epoch: 10 [14208/21600 (66%)]\tLoss: 0.012793\n",
            "Train Epoch: 10 [14304/21600 (66%)]\tLoss: 0.000075\n",
            "Train Epoch: 10 [14400/21600 (67%)]\tLoss: 0.005987\n",
            "Train Epoch: 10 [14496/21600 (67%)]\tLoss: 0.000470\n",
            "Train Epoch: 10 [14592/21600 (68%)]\tLoss: 0.000162\n",
            "Train Epoch: 10 [14688/21600 (68%)]\tLoss: 0.000059\n",
            "Train Epoch: 10 [14784/21600 (68%)]\tLoss: 0.005536\n",
            "Train Epoch: 10 [14880/21600 (69%)]\tLoss: 0.000623\n",
            "Train Epoch: 10 [14976/21600 (69%)]\tLoss: 0.000120\n",
            "Train Epoch: 10 [15072/21600 (70%)]\tLoss: 0.007941\n",
            "Train Epoch: 10 [15168/21600 (70%)]\tLoss: 0.020723\n",
            "Train Epoch: 10 [15264/21600 (71%)]\tLoss: 0.000069\n",
            "Train Epoch: 10 [15360/21600 (71%)]\tLoss: 0.000029\n",
            "Train Epoch: 10 [15456/21600 (72%)]\tLoss: 0.000485\n",
            "Train Epoch: 10 [15552/21600 (72%)]\tLoss: 0.001541\n",
            "Train Epoch: 10 [15648/21600 (72%)]\tLoss: 0.001664\n",
            "Train Epoch: 10 [15744/21600 (73%)]\tLoss: 0.003757\n",
            "Train Epoch: 10 [15840/21600 (73%)]\tLoss: 0.001516\n",
            "Train Epoch: 10 [15936/21600 (74%)]\tLoss: 0.001589\n",
            "Train Epoch: 10 [16032/21600 (74%)]\tLoss: 0.002308\n",
            "Train Epoch: 10 [16128/21600 (75%)]\tLoss: 0.001493\n",
            "Train Epoch: 10 [16224/21600 (75%)]\tLoss: 0.000092\n",
            "Train Epoch: 10 [16320/21600 (76%)]\tLoss: 0.001488\n",
            "Train Epoch: 10 [16416/21600 (76%)]\tLoss: 0.117076\n",
            "Train Epoch: 10 [16512/21600 (76%)]\tLoss: 0.027152\n",
            "Train Epoch: 10 [16608/21600 (77%)]\tLoss: 0.000337\n",
            "Train Epoch: 10 [16704/21600 (77%)]\tLoss: 0.000116\n",
            "Train Epoch: 10 [16800/21600 (78%)]\tLoss: 0.000067\n",
            "Train Epoch: 10 [16896/21600 (78%)]\tLoss: 0.000925\n",
            "Train Epoch: 10 [16992/21600 (79%)]\tLoss: 0.002386\n",
            "Train Epoch: 10 [17088/21600 (79%)]\tLoss: 0.026443\n",
            "Train Epoch: 10 [17184/21600 (80%)]\tLoss: 0.030310\n",
            "Train Epoch: 10 [17280/21600 (80%)]\tLoss: 0.000033\n",
            "Train Epoch: 10 [17376/21600 (80%)]\tLoss: 0.000528\n",
            "Train Epoch: 10 [17472/21600 (81%)]\tLoss: 0.056435\n",
            "Train Epoch: 10 [17568/21600 (81%)]\tLoss: 0.000108\n",
            "Train Epoch: 10 [17664/21600 (82%)]\tLoss: 0.000106\n",
            "Train Epoch: 10 [17760/21600 (82%)]\tLoss: 0.000245\n",
            "Train Epoch: 10 [17856/21600 (83%)]\tLoss: 0.000029\n",
            "Train Epoch: 10 [17952/21600 (83%)]\tLoss: 0.000322\n",
            "Train Epoch: 10 [18048/21600 (84%)]\tLoss: 0.000239\n",
            "Train Epoch: 10 [18144/21600 (84%)]\tLoss: 0.026275\n",
            "Train Epoch: 10 [18240/21600 (84%)]\tLoss: 0.049404\n",
            "Train Epoch: 10 [18336/21600 (85%)]\tLoss: 0.000091\n",
            "Train Epoch: 10 [18432/21600 (85%)]\tLoss: 0.002418\n",
            "Train Epoch: 10 [18528/21600 (86%)]\tLoss: 0.000259\n",
            "Train Epoch: 10 [18624/21600 (86%)]\tLoss: 0.000239\n",
            "Train Epoch: 10 [18720/21600 (87%)]\tLoss: 0.001361\n",
            "Train Epoch: 10 [18816/21600 (87%)]\tLoss: 0.024478\n",
            "Train Epoch: 10 [18912/21600 (88%)]\tLoss: 0.000049\n",
            "Train Epoch: 10 [19008/21600 (88%)]\tLoss: 0.000269\n",
            "Train Epoch: 10 [19104/21600 (88%)]\tLoss: 0.005738\n",
            "Train Epoch: 10 [19200/21600 (89%)]\tLoss: 0.003797\n",
            "Train Epoch: 10 [19296/21600 (89%)]\tLoss: 0.005793\n",
            "Train Epoch: 10 [19392/21600 (90%)]\tLoss: 0.000034\n",
            "Train Epoch: 10 [19488/21600 (90%)]\tLoss: 0.000750\n",
            "Train Epoch: 10 [19584/21600 (91%)]\tLoss: 0.000050\n",
            "Train Epoch: 10 [19680/21600 (91%)]\tLoss: 0.013040\n",
            "Train Epoch: 10 [19776/21600 (92%)]\tLoss: 0.000141\n",
            "Train Epoch: 10 [19872/21600 (92%)]\tLoss: 0.000075\n",
            "Train Epoch: 10 [19968/21600 (92%)]\tLoss: 0.003505\n",
            "Train Epoch: 10 [20064/21600 (93%)]\tLoss: 0.001109\n",
            "Train Epoch: 10 [20160/21600 (93%)]\tLoss: 0.009727\n",
            "Train Epoch: 10 [20256/21600 (94%)]\tLoss: 0.001174\n",
            "Train Epoch: 10 [20352/21600 (94%)]\tLoss: 0.005496\n",
            "Train Epoch: 10 [20448/21600 (95%)]\tLoss: 0.012119\n",
            "Train Epoch: 10 [20544/21600 (95%)]\tLoss: 0.025087\n",
            "Train Epoch: 10 [20640/21600 (96%)]\tLoss: 0.000409\n",
            "Train Epoch: 10 [20736/21600 (96%)]\tLoss: 0.010940\n",
            "Train Epoch: 10 [20832/21600 (96%)]\tLoss: 0.002181\n",
            "Train Epoch: 10 [20928/21600 (97%)]\tLoss: 0.000763\n",
            "Train Epoch: 10 [21024/21600 (97%)]\tLoss: 0.000491\n",
            "Train Epoch: 10 [21120/21600 (98%)]\tLoss: 0.001965\n",
            "Train Epoch: 10 [21216/21600 (98%)]\tLoss: 0.033493\n",
            "Train Epoch: 10 [21312/21600 (99%)]\tLoss: 0.000202\n",
            "Train Epoch: 10 [21408/21600 (99%)]\tLoss: 0.029426\n",
            "Train Epoch: 10 [21504/21600 (100%)]\tLoss: 0.009952\n",
            "\n",
            "Validation set: Average loss: 0.0001, Accuracy: 5400/5400 (100%)\n",
            "\n",
            "\n",
            "Saved model to model_binary_10.pth.\n"
          ]
        }
      ],
      "source": [
        "# Training and validating the model\n",
        "\n",
        "# import torchsummary\n",
        "log_interval = 2\n",
        "epochs = 10\n",
        "\n",
        "# Selecting GPU if available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "model = Net()\n",
        "model.to(device)\n",
        "\n",
        "# Different hyperparameters, optimizers, loss functions used for experimentation\n",
        "# print(model)\n",
        "\n",
        "# print(torchsummary.summary(model))\n",
        "\n",
        "# optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
        "\n",
        "decayRate = 0.96\n",
        "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decayRate)\n",
        "\n",
        "train_dataset = Train()\n",
        "train_loader = DataLoader(train_dataset, batch_size=48, shuffle=True)\n",
        "\n",
        "test_dataset = Test()\n",
        "val_loader = DataLoader(test_dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "# The train function\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        # loss = F.nll_loss(output, target)\n",
        "        # print(output.shape)\n",
        "        # print(target.shape)\n",
        "        loss = F.cross_entropy(output, target)\n",
        "        # loss = torch.nn.BCEWithLogitsLoss()\n",
        "        # loss = loss(output, target.unsqueeze(1).float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        losses.append(loss.item())\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "    return losses\n",
        "\n",
        "# The validation function\n",
        "def validation():\n",
        "    model.eval()\n",
        "    validation_loss = 0\n",
        "    correct = 0\n",
        "    for data, target in val_loader:\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        output = model(data)\n",
        "\n",
        "        # loss = torch.nn.BCEWithLogitsLoss(reduction=\"sum\")\n",
        "        # loss = loss(output, target.unsqueeze(1).float())\n",
        "        # validation_loss += loss.item() \n",
        " \n",
        "        validation_loss += F.cross_entropy(output, target, reduction=\"sum\").item() # sum up batch loss\n",
        "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "    validation_loss /= len(val_loader.dataset)\n",
        "    print('\\nValidation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        validation_loss, correct, len(val_loader.dataset),\n",
        "        100. * correct / len(val_loader.dataset)))\n",
        "\n",
        "\n",
        "losses_list = []\n",
        "for epoch in range(1, epochs + 1):\n",
        "    losses_list.extend(train(epoch))\n",
        "    validation()\n",
        "    lr_scheduler.step()\n",
        "    model_file = 'model_binary_' + str(epoch) + '.pth'\n",
        "    torch.save(model.state_dict(), model_file)\n",
        "    print('\\nSaved model to ' + model_file + '.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqZEO0qX44-b",
        "outputId": "f7359e91-edb9-4318-9452-ed505de1d508"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (layers): Sequential(\n",
            "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (7): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (9): ReLU(inplace=True)\n",
            "    (10): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (12): ReLU(inplace=True)\n",
            "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): Dropout(p=0.5, inplace=False)\n",
            "    (1): Linear(in_features=133760, out_features=512, bias=True)\n",
            "    (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): Dropout(p=0.5, inplace=False)\n",
            "    (5): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (7): ReLU(inplace=True)\n",
            "    (8): Dropout(p=0.5, inplace=False)\n",
            "    (9): Linear(in_features=512, out_features=128, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Printing the model\n",
        "model = Net()\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vXsR7oN6404h",
        "outputId": "8b4efab1-5f52-42f1-cb8f-bd9e535e9aff"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1440x1440 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plotting the losses during training\n",
        "plt.rcParams['figure.figsize'] = [20, 20]\n",
        "plt.plot(losses_list)\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "project_cedar_binary.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
