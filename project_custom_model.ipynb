{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "project_cedar_binary.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9F7vFmK-VoR",
        "outputId": "55ed8860-fdd4-4a3c-a472-482e24c4c924"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/cedar/signatures\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd  /content/drive/'My Drive'/cedar/signatures/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import RMSprop, Adam\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "from torch import save\n",
        "from torch import load\n",
        "from torch.nn import Linear, Conv2d, MaxPool2d, LocalResponseNorm, Dropout\n",
        "from torch.nn.functional import relu\n",
        "from torch.nn import Module\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "from torch import Tensor\n",
        "import torchsummary\n",
        "import torch\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "from PIL import Image\n",
        "from PIL.ImageOps import invert\n",
        "import pickle\n",
        "from random import randrange"
      ],
      "metadata": {
        "id": "nZqiGMMl-hNx"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def invert_image_path(path):\n",
        "    image_file = Image.open(path)  # open colour image\n",
        "    image_file = image_file.convert('L').resize([220, 155])\n",
        "    image_file = invert(image_file)\n",
        "    image_array = np.array(image_file)\n",
        "    for i in range(image_array.shape[0]):\n",
        "        for j in range(image_array.shape[1]):\n",
        "            if image_array[i][j] <= 50:\n",
        "                image_array[i][j] = 0\n",
        "            else:\n",
        "                image_array[i][j] = 255\n",
        "    return image_array\n",
        "\n",
        "\n",
        "def convert_to_image_tensor(image_array):\n",
        "    image_array = image_array / 255.0\n",
        "    return Tensor(image_array).view(1, 220, 155)\n",
        "\n",
        "\n",
        "def show_inverted(path):\n",
        "    img = Image.fromarray(invert_image_path(path))\n",
        "    img.show()\n",
        "\n",
        "\n",
        "def invert_image(image_file):\n",
        "    image_file = image_file.convert('L').resize([220, 155])\n",
        "    image_file = invert(image_file)\n",
        "    image_array = np.array(image_file)\n",
        "    for i in range(image_array.shape[0]):\n",
        "        for j in range(image_array.shape[1]):\n",
        "            if image_array[i][j] <= 50:\n",
        "                image_array[i][j] = 0\n",
        "            else:\n",
        "                image_array[i][j] = 255\n",
        "    return image_array"
      ],
      "metadata": {
        "id": "zC8SZs9u-loC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_path_org = 'signatures/full_org/original_%d_%d.png'\n",
        "base_path_forg = 'signatures/full_forg/forgeries_%d_%d.png'\n",
        "\n",
        "data = []\n",
        "n_samples_of_each_class = 13500\n",
        "\n",
        "for _ in range(n_samples_of_each_class):\n",
        "    anchor_person = randrange(1, 55)\n",
        "    # anchor_sign = randrange(1, 24)\n",
        "    pos_sign = randrange(1, 24)\n",
        "    # anchor_sign, pos_sign = fix_pair(anchor_sign, pos_sign)\n",
        "    neg_sign = randrange(1, 24)\n",
        "\n",
        "    positive = [base_path_org % (anchor_person, pos_sign), 1]\n",
        "    negative = [base_path_forg % (anchor_person, neg_sign), 0]\n",
        "\n",
        "    # positive = [base_path_org % (anchor_person, anchor_sign), base_path_org % (anchor_person, pos_sign), 1]\n",
        "    # negative = [base_path_org % (anchor_person, anchor_sign), base_path_forg % (anchor_person, neg_sign), 0]\n",
        "    data.append(positive)\n",
        "    data.append(negative)\n",
        "\n",
        "train, test = train_test_split(data, test_size=0.20)\n",
        "with open('train_index_binary.pkl', 'wb') as train_index_file:\n",
        "    pickle.dump(train, train_index_file)\n",
        "\n",
        "with open('test_index_binary.pkl', 'wb') as test_index_file:\n",
        "    pickle.dump(test, test_index_file)\n",
        "\n",
        "\n",
        "class TrainDataset(Dataset):\n",
        "\n",
        "    def __init__(self):\n",
        "        with open('train_index_binary.pkl', 'rb') as train_index_file:\n",
        "            self.pairs = pickle.load(train_index_file)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        item = self.pairs[index]\n",
        "        X = convert_to_image_tensor(invert_image_path(item[0]))\n",
        "        # Y = convert_to_image_tensor(invert_image_path(item[1]))\n",
        "        return [X, item[1]]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "\n",
        "class TestDataset(Dataset):\n",
        "\n",
        "    def __init__(self):\n",
        "        with open('test_index_binary.pkl', 'rb') as test_index_file:\n",
        "            self.pairs = pickle.load(test_index_file)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        item = self.pairs[index]\n",
        "        X = convert_to_image_tensor(invert_image_path(item[0]))\n",
        "        # Y = convert_to_image_tensor(invert_image_path(item[1]))\n",
        "        return [X, item[1]]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)"
      ],
      "metadata": {
        "id": "_QTqpK1y-qhE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import math\n",
        "\n",
        "nclasses = 128 # GTSRB has 43 classes\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "          \n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p = 0.5),\n",
        "            nn.Linear(133760, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p = 0.5),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p = 0.5),\n",
        "            nn.Linear(512, nclasses),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layers(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "    \n",
        "        return F.log_softmax(x,dim=1) \n"
      ],
      "metadata": {
        "id": "HNxI5I3KAqgV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torchsummary\n",
        "log_interval = 2\n",
        "epochs = 10\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "model = Net()\n",
        "model.to(device)\n",
        "\n",
        "# print(model)\n",
        "\n",
        "# print(torchsummary.summary(model))\n",
        "\n",
        "# optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
        "\n",
        "decayRate = 0.96\n",
        "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decayRate)\n",
        "\n",
        "train_dataset = TrainDataset()\n",
        "train_loader = DataLoader(train_dataset, batch_size=48, shuffle=True)\n",
        "\n",
        "test_dataset = TestDataset()\n",
        "val_loader = DataLoader(test_dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        # loss = F.nll_loss(output, target)\n",
        "        # print(output.shape)\n",
        "        # print(target.shape)\n",
        "        loss = F.cross_entropy(output, target)\n",
        "        # loss = torch.nn.BCEWithLogitsLoss()\n",
        "        # loss = loss(output, target.unsqueeze(1).float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        losses.append(loss.item())\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "    return losses\n",
        "\n",
        "def validation():\n",
        "    model.eval()\n",
        "    validation_loss = 0\n",
        "    correct = 0\n",
        "    for data, target in val_loader:\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        output = model(data)\n",
        "\n",
        "        # loss = torch.nn.BCEWithLogitsLoss(reduction=\"sum\")\n",
        "        # loss = loss(output, target.unsqueeze(1).float())\n",
        "        # validation_loss += loss.item() \n",
        " \n",
        "        validation_loss += F.cross_entropy(output, target, reduction=\"sum\").item() # sum up batch loss\n",
        "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "    validation_loss /= len(val_loader.dataset)\n",
        "    print('\\nValidation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        validation_loss, correct, len(val_loader.dataset),\n",
        "        100. * correct / len(val_loader.dataset)))\n",
        "\n",
        "\n",
        "losses_list = []\n",
        "for epoch in range(1, epochs + 1):\n",
        "    losses_list.extend(train(epoch))\n",
        "    validation()\n",
        "    lr_scheduler.step()\n",
        "    model_file = 'model_binary_' + str(epoch) + '.pth'\n",
        "    torch.save(model.state_dict(), model_file)\n",
        "    print('\\nSaved model to ' + model_file + '.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nt-XrmTEAvf1",
        "outputId": "57d3805e-b26f-4660-87e8-e485882a19be"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n",
            "Train Epoch: 1 [0/21600 (0%)]\tLoss: 5.014287\n",
            "Train Epoch: 1 [96/21600 (0%)]\tLoss: 3.825138\n",
            "Train Epoch: 1 [192/21600 (1%)]\tLoss: 2.875783\n",
            "Train Epoch: 1 [288/21600 (1%)]\tLoss: 2.351973\n",
            "Train Epoch: 1 [384/21600 (2%)]\tLoss: 1.513813\n",
            "Train Epoch: 1 [480/21600 (2%)]\tLoss: 1.306931\n",
            "Train Epoch: 1 [576/21600 (3%)]\tLoss: 0.984637\n",
            "Train Epoch: 1 [672/21600 (3%)]\tLoss: 0.668337\n",
            "Train Epoch: 1 [768/21600 (4%)]\tLoss: 0.623043\n",
            "Train Epoch: 1 [864/21600 (4%)]\tLoss: 0.725847\n",
            "Train Epoch: 1 [960/21600 (4%)]\tLoss: 0.769949\n",
            "Train Epoch: 1 [1056/21600 (5%)]\tLoss: 0.691719\n",
            "Train Epoch: 1 [1152/21600 (5%)]\tLoss: 0.747567\n",
            "Train Epoch: 1 [1248/21600 (6%)]\tLoss: 0.411536\n",
            "Train Epoch: 1 [1344/21600 (6%)]\tLoss: 0.529067\n",
            "Train Epoch: 1 [1440/21600 (7%)]\tLoss: 0.458953\n",
            "Train Epoch: 1 [1536/21600 (7%)]\tLoss: 0.488704\n",
            "Train Epoch: 1 [1632/21600 (8%)]\tLoss: 0.401880\n",
            "Train Epoch: 1 [1728/21600 (8%)]\tLoss: 0.420006\n",
            "Train Epoch: 1 [1824/21600 (8%)]\tLoss: 0.272656\n",
            "Train Epoch: 1 [1920/21600 (9%)]\tLoss: 0.409668\n",
            "Train Epoch: 1 [2016/21600 (9%)]\tLoss: 0.203608\n",
            "Train Epoch: 1 [2112/21600 (10%)]\tLoss: 0.529065\n",
            "Train Epoch: 1 [2208/21600 (10%)]\tLoss: 0.331349\n",
            "Train Epoch: 1 [2304/21600 (11%)]\tLoss: 0.358207\n",
            "Train Epoch: 1 [2400/21600 (11%)]\tLoss: 0.355535\n",
            "Train Epoch: 1 [2496/21600 (12%)]\tLoss: 0.356392\n",
            "Train Epoch: 1 [2592/21600 (12%)]\tLoss: 0.485028\n",
            "Train Epoch: 1 [2688/21600 (12%)]\tLoss: 0.258526\n",
            "Train Epoch: 1 [2784/21600 (13%)]\tLoss: 0.308491\n",
            "Train Epoch: 1 [2880/21600 (13%)]\tLoss: 0.302695\n",
            "Train Epoch: 1 [2976/21600 (14%)]\tLoss: 0.588303\n",
            "Train Epoch: 1 [3072/21600 (14%)]\tLoss: 0.288694\n",
            "Train Epoch: 1 [3168/21600 (15%)]\tLoss: 0.251564\n",
            "Train Epoch: 1 [3264/21600 (15%)]\tLoss: 0.294088\n",
            "Train Epoch: 1 [3360/21600 (16%)]\tLoss: 0.240710\n",
            "Train Epoch: 1 [3456/21600 (16%)]\tLoss: 0.271386\n",
            "Train Epoch: 1 [3552/21600 (16%)]\tLoss: 0.390069\n",
            "Train Epoch: 1 [3648/21600 (17%)]\tLoss: 0.391123\n",
            "Train Epoch: 1 [3744/21600 (17%)]\tLoss: 0.268783\n",
            "Train Epoch: 1 [3840/21600 (18%)]\tLoss: 0.382076\n",
            "Train Epoch: 1 [3936/21600 (18%)]\tLoss: 0.404430\n",
            "Train Epoch: 1 [4032/21600 (19%)]\tLoss: 0.333556\n",
            "Train Epoch: 1 [4128/21600 (19%)]\tLoss: 0.345508\n",
            "Train Epoch: 1 [4224/21600 (20%)]\tLoss: 0.295452\n",
            "Train Epoch: 1 [4320/21600 (20%)]\tLoss: 0.251829\n",
            "Train Epoch: 1 [4416/21600 (20%)]\tLoss: 0.213314\n",
            "Train Epoch: 1 [4512/21600 (21%)]\tLoss: 0.325794\n",
            "Train Epoch: 1 [4608/21600 (21%)]\tLoss: 0.301509\n",
            "Train Epoch: 1 [4704/21600 (22%)]\tLoss: 0.275355\n",
            "Train Epoch: 1 [4800/21600 (22%)]\tLoss: 0.165847\n",
            "Train Epoch: 1 [4896/21600 (23%)]\tLoss: 0.231791\n",
            "Train Epoch: 1 [4992/21600 (23%)]\tLoss: 0.171008\n",
            "Train Epoch: 1 [5088/21600 (24%)]\tLoss: 0.358848\n",
            "Train Epoch: 1 [5184/21600 (24%)]\tLoss: 0.381828\n",
            "Train Epoch: 1 [5280/21600 (24%)]\tLoss: 0.132449\n",
            "Train Epoch: 1 [5376/21600 (25%)]\tLoss: 0.542191\n",
            "Train Epoch: 1 [5472/21600 (25%)]\tLoss: 0.464004\n",
            "Train Epoch: 1 [5568/21600 (26%)]\tLoss: 0.367214\n",
            "Train Epoch: 1 [5664/21600 (26%)]\tLoss: 0.238959\n",
            "Train Epoch: 1 [5760/21600 (27%)]\tLoss: 0.215769\n",
            "Train Epoch: 1 [5856/21600 (27%)]\tLoss: 0.300908\n",
            "Train Epoch: 1 [5952/21600 (28%)]\tLoss: 0.148296\n",
            "Train Epoch: 1 [6048/21600 (28%)]\tLoss: 0.194885\n",
            "Train Epoch: 1 [6144/21600 (28%)]\tLoss: 0.139499\n",
            "Train Epoch: 1 [6240/21600 (29%)]\tLoss: 0.127083\n",
            "Train Epoch: 1 [6336/21600 (29%)]\tLoss: 0.359675\n",
            "Train Epoch: 1 [6432/21600 (30%)]\tLoss: 0.321225\n",
            "Train Epoch: 1 [6528/21600 (30%)]\tLoss: 0.287420\n",
            "Train Epoch: 1 [6624/21600 (31%)]\tLoss: 0.294524\n",
            "Train Epoch: 1 [6720/21600 (31%)]\tLoss: 0.157141\n",
            "Train Epoch: 1 [6816/21600 (32%)]\tLoss: 0.242176\n",
            "Train Epoch: 1 [6912/21600 (32%)]\tLoss: 0.337141\n",
            "Train Epoch: 1 [7008/21600 (32%)]\tLoss: 0.140884\n",
            "Train Epoch: 1 [7104/21600 (33%)]\tLoss: 0.224566\n",
            "Train Epoch: 1 [7200/21600 (33%)]\tLoss: 0.211415\n",
            "Train Epoch: 1 [7296/21600 (34%)]\tLoss: 0.215108\n",
            "Train Epoch: 1 [7392/21600 (34%)]\tLoss: 0.205359\n",
            "Train Epoch: 1 [7488/21600 (35%)]\tLoss: 0.218897\n",
            "Train Epoch: 1 [7584/21600 (35%)]\tLoss: 0.131675\n",
            "Train Epoch: 1 [7680/21600 (36%)]\tLoss: 0.102385\n",
            "Train Epoch: 1 [7776/21600 (36%)]\tLoss: 0.123465\n",
            "Train Epoch: 1 [7872/21600 (36%)]\tLoss: 0.179162\n",
            "Train Epoch: 1 [7968/21600 (37%)]\tLoss: 0.180689\n",
            "Train Epoch: 1 [8064/21600 (37%)]\tLoss: 0.450251\n",
            "Train Epoch: 1 [8160/21600 (38%)]\tLoss: 0.134622\n",
            "Train Epoch: 1 [8256/21600 (38%)]\tLoss: 0.132438\n",
            "Train Epoch: 1 [8352/21600 (39%)]\tLoss: 0.161315\n",
            "Train Epoch: 1 [8448/21600 (39%)]\tLoss: 0.140031\n",
            "Train Epoch: 1 [8544/21600 (40%)]\tLoss: 0.219888\n",
            "Train Epoch: 1 [8640/21600 (40%)]\tLoss: 0.085563\n",
            "Train Epoch: 1 [8736/21600 (40%)]\tLoss: 0.150743\n",
            "Train Epoch: 1 [8832/21600 (41%)]\tLoss: 0.076108\n",
            "Train Epoch: 1 [8928/21600 (41%)]\tLoss: 0.154082\n",
            "Train Epoch: 1 [9024/21600 (42%)]\tLoss: 0.204375\n",
            "Train Epoch: 1 [9120/21600 (42%)]\tLoss: 0.126560\n",
            "Train Epoch: 1 [9216/21600 (43%)]\tLoss: 0.233247\n",
            "Train Epoch: 1 [9312/21600 (43%)]\tLoss: 0.350904\n",
            "Train Epoch: 1 [9408/21600 (44%)]\tLoss: 0.131506\n",
            "Train Epoch: 1 [9504/21600 (44%)]\tLoss: 0.103017\n",
            "Train Epoch: 1 [9600/21600 (44%)]\tLoss: 0.269159\n",
            "Train Epoch: 1 [9696/21600 (45%)]\tLoss: 0.098275\n",
            "Train Epoch: 1 [9792/21600 (45%)]\tLoss: 0.147965\n",
            "Train Epoch: 1 [9888/21600 (46%)]\tLoss: 0.082698\n",
            "Train Epoch: 1 [9984/21600 (46%)]\tLoss: 0.130180\n",
            "Train Epoch: 1 [10080/21600 (47%)]\tLoss: 0.129391\n",
            "Train Epoch: 1 [10176/21600 (47%)]\tLoss: 0.137880\n",
            "Train Epoch: 1 [10272/21600 (48%)]\tLoss: 0.073631\n",
            "Train Epoch: 1 [10368/21600 (48%)]\tLoss: 0.164162\n",
            "Train Epoch: 1 [10464/21600 (48%)]\tLoss: 0.123999\n",
            "Train Epoch: 1 [10560/21600 (49%)]\tLoss: 0.160791\n",
            "Train Epoch: 1 [10656/21600 (49%)]\tLoss: 0.096902\n",
            "Train Epoch: 1 [10752/21600 (50%)]\tLoss: 0.100271\n",
            "Train Epoch: 1 [10848/21600 (50%)]\tLoss: 0.055124\n",
            "Train Epoch: 1 [10944/21600 (51%)]\tLoss: 0.098375\n",
            "Train Epoch: 1 [11040/21600 (51%)]\tLoss: 0.061514\n",
            "Train Epoch: 1 [11136/21600 (52%)]\tLoss: 0.080945\n",
            "Train Epoch: 1 [11232/21600 (52%)]\tLoss: 0.044895\n",
            "Train Epoch: 1 [11328/21600 (52%)]\tLoss: 0.177056\n",
            "Train Epoch: 1 [11424/21600 (53%)]\tLoss: 0.080830\n",
            "Train Epoch: 1 [11520/21600 (53%)]\tLoss: 0.135386\n",
            "Train Epoch: 1 [11616/21600 (54%)]\tLoss: 0.119095\n",
            "Train Epoch: 1 [11712/21600 (54%)]\tLoss: 0.139639\n",
            "Train Epoch: 1 [11808/21600 (55%)]\tLoss: 0.055005\n",
            "Train Epoch: 1 [11904/21600 (55%)]\tLoss: 0.108802\n",
            "Train Epoch: 1 [12000/21600 (56%)]\tLoss: 0.072605\n",
            "Train Epoch: 1 [12096/21600 (56%)]\tLoss: 0.139962\n",
            "Train Epoch: 1 [12192/21600 (56%)]\tLoss: 0.048456\n",
            "Train Epoch: 1 [12288/21600 (57%)]\tLoss: 0.119842\n",
            "Train Epoch: 1 [12384/21600 (57%)]\tLoss: 0.096049\n",
            "Train Epoch: 1 [12480/21600 (58%)]\tLoss: 0.123156\n",
            "Train Epoch: 1 [12576/21600 (58%)]\tLoss: 0.092769\n",
            "Train Epoch: 1 [12672/21600 (59%)]\tLoss: 0.099271\n",
            "Train Epoch: 1 [12768/21600 (59%)]\tLoss: 0.179412\n",
            "Train Epoch: 1 [12864/21600 (60%)]\tLoss: 0.052277\n",
            "Train Epoch: 1 [12960/21600 (60%)]\tLoss: 0.118512\n",
            "Train Epoch: 1 [13056/21600 (60%)]\tLoss: 0.048010\n",
            "Train Epoch: 1 [13152/21600 (61%)]\tLoss: 0.071923\n",
            "Train Epoch: 1 [13248/21600 (61%)]\tLoss: 0.045885\n",
            "Train Epoch: 1 [13344/21600 (62%)]\tLoss: 0.149812\n",
            "Train Epoch: 1 [13440/21600 (62%)]\tLoss: 0.083458\n",
            "Train Epoch: 1 [13536/21600 (63%)]\tLoss: 0.120424\n",
            "Train Epoch: 1 [13632/21600 (63%)]\tLoss: 0.083296\n",
            "Train Epoch: 1 [13728/21600 (64%)]\tLoss: 0.083336\n",
            "Train Epoch: 1 [13824/21600 (64%)]\tLoss: 0.063828\n",
            "Train Epoch: 1 [13920/21600 (64%)]\tLoss: 0.091932\n",
            "Train Epoch: 1 [14016/21600 (65%)]\tLoss: 0.141888\n",
            "Train Epoch: 1 [14112/21600 (65%)]\tLoss: 0.037135\n",
            "Train Epoch: 1 [14208/21600 (66%)]\tLoss: 0.182299\n",
            "Train Epoch: 1 [14304/21600 (66%)]\tLoss: 0.024909\n",
            "Train Epoch: 1 [14400/21600 (67%)]\tLoss: 0.095252\n",
            "Train Epoch: 1 [14496/21600 (67%)]\tLoss: 0.096233\n",
            "Train Epoch: 1 [14592/21600 (68%)]\tLoss: 0.060680\n",
            "Train Epoch: 1 [14688/21600 (68%)]\tLoss: 0.119711\n",
            "Train Epoch: 1 [14784/21600 (68%)]\tLoss: 0.091241\n",
            "Train Epoch: 1 [14880/21600 (69%)]\tLoss: 0.356404\n",
            "Train Epoch: 1 [14976/21600 (69%)]\tLoss: 0.227152\n",
            "Train Epoch: 1 [15072/21600 (70%)]\tLoss: 0.138125\n",
            "Train Epoch: 1 [15168/21600 (70%)]\tLoss: 0.137576\n",
            "Train Epoch: 1 [15264/21600 (71%)]\tLoss: 0.065997\n",
            "Train Epoch: 1 [15360/21600 (71%)]\tLoss: 0.039009\n",
            "Train Epoch: 1 [15456/21600 (72%)]\tLoss: 0.132496\n",
            "Train Epoch: 1 [15552/21600 (72%)]\tLoss: 0.097948\n",
            "Train Epoch: 1 [15648/21600 (72%)]\tLoss: 0.253397\n",
            "Train Epoch: 1 [15744/21600 (73%)]\tLoss: 0.077135\n",
            "Train Epoch: 1 [15840/21600 (73%)]\tLoss: 0.262337\n",
            "Train Epoch: 1 [15936/21600 (74%)]\tLoss: 0.026779\n",
            "Train Epoch: 1 [16032/21600 (74%)]\tLoss: 0.068680\n",
            "Train Epoch: 1 [16128/21600 (75%)]\tLoss: 0.132972\n",
            "Train Epoch: 1 [16224/21600 (75%)]\tLoss: 0.031101\n",
            "Train Epoch: 1 [16320/21600 (76%)]\tLoss: 0.050094\n",
            "Train Epoch: 1 [16416/21600 (76%)]\tLoss: 0.140156\n",
            "Train Epoch: 1 [16512/21600 (76%)]\tLoss: 0.029988\n",
            "Train Epoch: 1 [16608/21600 (77%)]\tLoss: 0.118699\n",
            "Train Epoch: 1 [16704/21600 (77%)]\tLoss: 0.030458\n",
            "Train Epoch: 1 [16800/21600 (78%)]\tLoss: 0.109682\n",
            "Train Epoch: 1 [16896/21600 (78%)]\tLoss: 0.033019\n",
            "Train Epoch: 1 [16992/21600 (79%)]\tLoss: 0.112494\n",
            "Train Epoch: 1 [17088/21600 (79%)]\tLoss: 0.066023\n",
            "Train Epoch: 1 [17184/21600 (80%)]\tLoss: 0.037967\n",
            "Train Epoch: 1 [17280/21600 (80%)]\tLoss: 0.038813\n",
            "Train Epoch: 1 [17376/21600 (80%)]\tLoss: 0.066847\n",
            "Train Epoch: 1 [17472/21600 (81%)]\tLoss: 0.120468\n",
            "Train Epoch: 1 [17568/21600 (81%)]\tLoss: 0.041205\n",
            "Train Epoch: 1 [17664/21600 (82%)]\tLoss: 0.040205\n",
            "Train Epoch: 1 [17760/21600 (82%)]\tLoss: 0.024693\n",
            "Train Epoch: 1 [17856/21600 (83%)]\tLoss: 0.111526\n",
            "Train Epoch: 1 [17952/21600 (83%)]\tLoss: 0.021388\n",
            "Train Epoch: 1 [18048/21600 (84%)]\tLoss: 0.042554\n",
            "Train Epoch: 1 [18144/21600 (84%)]\tLoss: 0.185621\n",
            "Train Epoch: 1 [18240/21600 (84%)]\tLoss: 0.052513\n",
            "Train Epoch: 1 [18336/21600 (85%)]\tLoss: 0.018514\n",
            "Train Epoch: 1 [18432/21600 (85%)]\tLoss: 0.071650\n",
            "Train Epoch: 1 [18528/21600 (86%)]\tLoss: 0.026434\n",
            "Train Epoch: 1 [18624/21600 (86%)]\tLoss: 0.035263\n",
            "Train Epoch: 1 [18720/21600 (87%)]\tLoss: 0.054425\n",
            "Train Epoch: 1 [18816/21600 (87%)]\tLoss: 0.053407\n",
            "Train Epoch: 1 [18912/21600 (88%)]\tLoss: 0.021423\n",
            "Train Epoch: 1 [19008/21600 (88%)]\tLoss: 0.106152\n",
            "Train Epoch: 1 [19104/21600 (88%)]\tLoss: 0.055364\n",
            "Train Epoch: 1 [19200/21600 (89%)]\tLoss: 0.152033\n",
            "Train Epoch: 1 [19296/21600 (89%)]\tLoss: 0.027315\n",
            "Train Epoch: 1 [19392/21600 (90%)]\tLoss: 0.016205\n",
            "Train Epoch: 1 [19488/21600 (90%)]\tLoss: 0.143463\n",
            "Train Epoch: 1 [19584/21600 (91%)]\tLoss: 0.020421\n",
            "Train Epoch: 1 [19680/21600 (91%)]\tLoss: 0.089271\n",
            "Train Epoch: 1 [19776/21600 (92%)]\tLoss: 0.057139\n",
            "Train Epoch: 1 [19872/21600 (92%)]\tLoss: 0.129940\n",
            "Train Epoch: 1 [19968/21600 (92%)]\tLoss: 0.098443\n",
            "Train Epoch: 1 [20064/21600 (93%)]\tLoss: 0.134808\n",
            "Train Epoch: 1 [20160/21600 (93%)]\tLoss: 0.106756\n",
            "Train Epoch: 1 [20256/21600 (94%)]\tLoss: 0.032987\n",
            "Train Epoch: 1 [20352/21600 (94%)]\tLoss: 0.044437\n",
            "Train Epoch: 1 [20448/21600 (95%)]\tLoss: 0.032849\n",
            "Train Epoch: 1 [20544/21600 (95%)]\tLoss: 0.027305\n",
            "Train Epoch: 1 [20640/21600 (96%)]\tLoss: 0.020719\n",
            "Train Epoch: 1 [20736/21600 (96%)]\tLoss: 0.087995\n",
            "Train Epoch: 1 [20832/21600 (96%)]\tLoss: 0.069303\n",
            "Train Epoch: 1 [20928/21600 (97%)]\tLoss: 0.077255\n",
            "Train Epoch: 1 [21024/21600 (97%)]\tLoss: 0.012437\n",
            "Train Epoch: 1 [21120/21600 (98%)]\tLoss: 0.022152\n",
            "Train Epoch: 1 [21216/21600 (98%)]\tLoss: 0.048921\n",
            "Train Epoch: 1 [21312/21600 (99%)]\tLoss: 0.084982\n",
            "Train Epoch: 1 [21408/21600 (99%)]\tLoss: 0.079609\n",
            "Train Epoch: 1 [21504/21600 (100%)]\tLoss: 0.015829\n",
            "\n",
            "Validation set: Average loss: 0.1065, Accuracy: 5199/5400 (96%)\n",
            "\n",
            "\n",
            "Saved model to model_binary_1.pth.\n",
            "Train Epoch: 2 [0/21600 (0%)]\tLoss: 0.071078\n",
            "Train Epoch: 2 [96/21600 (0%)]\tLoss: 0.011135\n",
            "Train Epoch: 2 [192/21600 (1%)]\tLoss: 0.099666\n",
            "Train Epoch: 2 [288/21600 (1%)]\tLoss: 0.069816\n",
            "Train Epoch: 2 [384/21600 (2%)]\tLoss: 0.146430\n",
            "Train Epoch: 2 [480/21600 (2%)]\tLoss: 0.249733\n",
            "Train Epoch: 2 [576/21600 (3%)]\tLoss: 0.176408\n",
            "Train Epoch: 2 [672/21600 (3%)]\tLoss: 0.005580\n",
            "Train Epoch: 2 [768/21600 (4%)]\tLoss: 0.055743\n",
            "Train Epoch: 2 [864/21600 (4%)]\tLoss: 0.062537\n",
            "Train Epoch: 2 [960/21600 (4%)]\tLoss: 0.017036\n",
            "Train Epoch: 2 [1056/21600 (5%)]\tLoss: 0.036353\n",
            "Train Epoch: 2 [1152/21600 (5%)]\tLoss: 0.037926\n",
            "Train Epoch: 2 [1248/21600 (6%)]\tLoss: 0.008364\n",
            "Train Epoch: 2 [1344/21600 (6%)]\tLoss: 0.011169\n",
            "Train Epoch: 2 [1440/21600 (7%)]\tLoss: 0.084091\n",
            "Train Epoch: 2 [1536/21600 (7%)]\tLoss: 0.018656\n",
            "Train Epoch: 2 [1632/21600 (8%)]\tLoss: 0.117339\n",
            "Train Epoch: 2 [1728/21600 (8%)]\tLoss: 0.079272\n",
            "Train Epoch: 2 [1824/21600 (8%)]\tLoss: 0.019644\n",
            "Train Epoch: 2 [1920/21600 (9%)]\tLoss: 0.027125\n",
            "Train Epoch: 2 [2016/21600 (9%)]\tLoss: 0.009810\n",
            "Train Epoch: 2 [2112/21600 (10%)]\tLoss: 0.134746\n",
            "Train Epoch: 2 [2208/21600 (10%)]\tLoss: 0.196018\n",
            "Train Epoch: 2 [2304/21600 (11%)]\tLoss: 0.062585\n",
            "Train Epoch: 2 [2400/21600 (11%)]\tLoss: 0.063745\n",
            "Train Epoch: 2 [2496/21600 (12%)]\tLoss: 0.039753\n",
            "Train Epoch: 2 [2592/21600 (12%)]\tLoss: 0.076351\n",
            "Train Epoch: 2 [2688/21600 (12%)]\tLoss: 0.021614\n",
            "Train Epoch: 2 [2784/21600 (13%)]\tLoss: 0.010853\n",
            "Train Epoch: 2 [2880/21600 (13%)]\tLoss: 0.085081\n",
            "Train Epoch: 2 [2976/21600 (14%)]\tLoss: 0.078497\n",
            "Train Epoch: 2 [3072/21600 (14%)]\tLoss: 0.053804\n",
            "Train Epoch: 2 [3168/21600 (15%)]\tLoss: 0.019923\n",
            "Train Epoch: 2 [3264/21600 (15%)]\tLoss: 0.006642\n",
            "Train Epoch: 2 [3360/21600 (16%)]\tLoss: 0.035379\n",
            "Train Epoch: 2 [3456/21600 (16%)]\tLoss: 0.012432\n",
            "Train Epoch: 2 [3552/21600 (16%)]\tLoss: 0.031157\n",
            "Train Epoch: 2 [3648/21600 (17%)]\tLoss: 0.047830\n",
            "Train Epoch: 2 [3744/21600 (17%)]\tLoss: 0.125185\n",
            "Train Epoch: 2 [3840/21600 (18%)]\tLoss: 0.047082\n",
            "Train Epoch: 2 [3936/21600 (18%)]\tLoss: 0.052133\n",
            "Train Epoch: 2 [4032/21600 (19%)]\tLoss: 0.022363\n",
            "Train Epoch: 2 [4128/21600 (19%)]\tLoss: 0.053520\n",
            "Train Epoch: 2 [4224/21600 (20%)]\tLoss: 0.011334\n",
            "Train Epoch: 2 [4320/21600 (20%)]\tLoss: 0.012065\n",
            "Train Epoch: 2 [4416/21600 (20%)]\tLoss: 0.016257\n",
            "Train Epoch: 2 [4512/21600 (21%)]\tLoss: 0.036335\n",
            "Train Epoch: 2 [4608/21600 (21%)]\tLoss: 0.019013\n",
            "Train Epoch: 2 [4704/21600 (22%)]\tLoss: 0.035456\n",
            "Train Epoch: 2 [4800/21600 (22%)]\tLoss: 0.004291\n",
            "Train Epoch: 2 [4896/21600 (23%)]\tLoss: 0.014154\n",
            "Train Epoch: 2 [4992/21600 (23%)]\tLoss: 0.087617\n",
            "Train Epoch: 2 [5088/21600 (24%)]\tLoss: 0.116717\n",
            "Train Epoch: 2 [5184/21600 (24%)]\tLoss: 0.056405\n",
            "Train Epoch: 2 [5280/21600 (24%)]\tLoss: 0.068914\n",
            "Train Epoch: 2 [5376/21600 (25%)]\tLoss: 0.010240\n",
            "Train Epoch: 2 [5472/21600 (25%)]\tLoss: 0.007451\n",
            "Train Epoch: 2 [5568/21600 (26%)]\tLoss: 0.023361\n",
            "Train Epoch: 2 [5664/21600 (26%)]\tLoss: 0.016140\n",
            "Train Epoch: 2 [5760/21600 (27%)]\tLoss: 0.006833\n",
            "Train Epoch: 2 [5856/21600 (27%)]\tLoss: 0.153397\n",
            "Train Epoch: 2 [5952/21600 (28%)]\tLoss: 0.041381\n",
            "Train Epoch: 2 [6048/21600 (28%)]\tLoss: 0.104015\n",
            "Train Epoch: 2 [6144/21600 (28%)]\tLoss: 0.078738\n",
            "Train Epoch: 2 [6240/21600 (29%)]\tLoss: 0.028069\n",
            "Train Epoch: 2 [6336/21600 (29%)]\tLoss: 0.004724\n",
            "Train Epoch: 2 [6432/21600 (30%)]\tLoss: 0.003975\n",
            "Train Epoch: 2 [6528/21600 (30%)]\tLoss: 0.024006\n",
            "Train Epoch: 2 [6624/21600 (31%)]\tLoss: 0.025434\n",
            "Train Epoch: 2 [6720/21600 (31%)]\tLoss: 0.039589\n",
            "Train Epoch: 2 [6816/21600 (32%)]\tLoss: 0.034735\n",
            "Train Epoch: 2 [6912/21600 (32%)]\tLoss: 0.026975\n",
            "Train Epoch: 2 [7008/21600 (32%)]\tLoss: 0.051822\n",
            "Train Epoch: 2 [7104/21600 (33%)]\tLoss: 0.004577\n",
            "Train Epoch: 2 [7200/21600 (33%)]\tLoss: 0.036931\n",
            "Train Epoch: 2 [7296/21600 (34%)]\tLoss: 0.012561\n",
            "Train Epoch: 2 [7392/21600 (34%)]\tLoss: 0.022615\n",
            "Train Epoch: 2 [7488/21600 (35%)]\tLoss: 0.013196\n",
            "Train Epoch: 2 [7584/21600 (35%)]\tLoss: 0.030347\n",
            "Train Epoch: 2 [7680/21600 (36%)]\tLoss: 0.087898\n",
            "Train Epoch: 2 [7776/21600 (36%)]\tLoss: 0.052271\n",
            "Train Epoch: 2 [7872/21600 (36%)]\tLoss: 0.004421\n",
            "Train Epoch: 2 [7968/21600 (37%)]\tLoss: 0.044549\n",
            "Train Epoch: 2 [8064/21600 (37%)]\tLoss: 0.046042\n",
            "Train Epoch: 2 [8160/21600 (38%)]\tLoss: 0.053974\n",
            "Train Epoch: 2 [8256/21600 (38%)]\tLoss: 0.023502\n",
            "Train Epoch: 2 [8352/21600 (39%)]\tLoss: 0.026657\n",
            "Train Epoch: 2 [8448/21600 (39%)]\tLoss: 0.007069\n",
            "Train Epoch: 2 [8544/21600 (40%)]\tLoss: 0.035459\n",
            "Train Epoch: 2 [8640/21600 (40%)]\tLoss: 0.085893\n",
            "Train Epoch: 2 [8736/21600 (40%)]\tLoss: 0.068008\n",
            "Train Epoch: 2 [8832/21600 (41%)]\tLoss: 0.205799\n",
            "Train Epoch: 2 [8928/21600 (41%)]\tLoss: 0.039086\n",
            "Train Epoch: 2 [9024/21600 (42%)]\tLoss: 0.008565\n",
            "Train Epoch: 2 [9120/21600 (42%)]\tLoss: 0.032395\n",
            "Train Epoch: 2 [9216/21600 (43%)]\tLoss: 0.017175\n",
            "Train Epoch: 2 [9312/21600 (43%)]\tLoss: 0.050860\n",
            "Train Epoch: 2 [9408/21600 (44%)]\tLoss: 0.080221\n",
            "Train Epoch: 2 [9504/21600 (44%)]\tLoss: 0.009879\n",
            "Train Epoch: 2 [9600/21600 (44%)]\tLoss: 0.150155\n",
            "Train Epoch: 2 [9696/21600 (45%)]\tLoss: 0.012027\n",
            "Train Epoch: 2 [9792/21600 (45%)]\tLoss: 0.019396\n",
            "Train Epoch: 2 [9888/21600 (46%)]\tLoss: 0.015068\n",
            "Train Epoch: 2 [9984/21600 (46%)]\tLoss: 0.072435\n",
            "Train Epoch: 2 [10080/21600 (47%)]\tLoss: 0.005085\n",
            "Train Epoch: 2 [10176/21600 (47%)]\tLoss: 0.027762\n",
            "Train Epoch: 2 [10272/21600 (48%)]\tLoss: 0.018606\n",
            "Train Epoch: 2 [10368/21600 (48%)]\tLoss: 0.064443\n",
            "Train Epoch: 2 [10464/21600 (48%)]\tLoss: 0.062242\n",
            "Train Epoch: 2 [10560/21600 (49%)]\tLoss: 0.030864\n",
            "Train Epoch: 2 [10656/21600 (49%)]\tLoss: 0.018521\n",
            "Train Epoch: 2 [10752/21600 (50%)]\tLoss: 0.016634\n",
            "Train Epoch: 2 [10848/21600 (50%)]\tLoss: 0.007679\n",
            "Train Epoch: 2 [10944/21600 (51%)]\tLoss: 0.057309\n",
            "Train Epoch: 2 [11040/21600 (51%)]\tLoss: 0.077822\n",
            "Train Epoch: 2 [11136/21600 (52%)]\tLoss: 0.061158\n",
            "Train Epoch: 2 [11232/21600 (52%)]\tLoss: 0.016772\n",
            "Train Epoch: 2 [11328/21600 (52%)]\tLoss: 0.015740\n",
            "Train Epoch: 2 [11424/21600 (53%)]\tLoss: 0.014642\n",
            "Train Epoch: 2 [11520/21600 (53%)]\tLoss: 0.009764\n",
            "Train Epoch: 2 [11616/21600 (54%)]\tLoss: 0.082040\n",
            "Train Epoch: 2 [11712/21600 (54%)]\tLoss: 0.015585\n",
            "Train Epoch: 2 [11808/21600 (55%)]\tLoss: 0.040874\n",
            "Train Epoch: 2 [11904/21600 (55%)]\tLoss: 0.007619\n",
            "Train Epoch: 2 [12000/21600 (56%)]\tLoss: 0.026531\n",
            "Train Epoch: 2 [12096/21600 (56%)]\tLoss: 0.060573\n",
            "Train Epoch: 2 [12192/21600 (56%)]\tLoss: 0.020538\n",
            "Train Epoch: 2 [12288/21600 (57%)]\tLoss: 0.005523\n",
            "Train Epoch: 2 [12384/21600 (57%)]\tLoss: 0.041489\n",
            "Train Epoch: 2 [12480/21600 (58%)]\tLoss: 0.003227\n",
            "Train Epoch: 2 [12576/21600 (58%)]\tLoss: 0.060952\n",
            "Train Epoch: 2 [12672/21600 (59%)]\tLoss: 0.014495\n",
            "Train Epoch: 2 [12768/21600 (59%)]\tLoss: 0.029331\n",
            "Train Epoch: 2 [12864/21600 (60%)]\tLoss: 0.020591\n",
            "Train Epoch: 2 [12960/21600 (60%)]\tLoss: 0.129401\n",
            "Train Epoch: 2 [13056/21600 (60%)]\tLoss: 0.053963\n",
            "Train Epoch: 2 [13152/21600 (61%)]\tLoss: 0.180283\n",
            "Train Epoch: 2 [13248/21600 (61%)]\tLoss: 0.007263\n",
            "Train Epoch: 2 [13344/21600 (62%)]\tLoss: 0.163441\n",
            "Train Epoch: 2 [13440/21600 (62%)]\tLoss: 0.018617\n",
            "Train Epoch: 2 [13536/21600 (63%)]\tLoss: 0.029215\n",
            "Train Epoch: 2 [13632/21600 (63%)]\tLoss: 0.017908\n",
            "Train Epoch: 2 [13728/21600 (64%)]\tLoss: 0.095230\n",
            "Train Epoch: 2 [13824/21600 (64%)]\tLoss: 0.006735\n",
            "Train Epoch: 2 [13920/21600 (64%)]\tLoss: 0.021349\n",
            "Train Epoch: 2 [14016/21600 (65%)]\tLoss: 0.020489\n",
            "Train Epoch: 2 [14112/21600 (65%)]\tLoss: 0.001241\n",
            "Train Epoch: 2 [14208/21600 (66%)]\tLoss: 0.023023\n",
            "Train Epoch: 2 [14304/21600 (66%)]\tLoss: 0.048296\n",
            "Train Epoch: 2 [14400/21600 (67%)]\tLoss: 0.047828\n",
            "Train Epoch: 2 [14496/21600 (67%)]\tLoss: 0.034758\n",
            "Train Epoch: 2 [14592/21600 (68%)]\tLoss: 0.033641\n",
            "Train Epoch: 2 [14688/21600 (68%)]\tLoss: 0.021240\n",
            "Train Epoch: 2 [14784/21600 (68%)]\tLoss: 0.139585\n",
            "Train Epoch: 2 [14880/21600 (69%)]\tLoss: 0.008884\n",
            "Train Epoch: 2 [14976/21600 (69%)]\tLoss: 0.012259\n",
            "Train Epoch: 2 [15072/21600 (70%)]\tLoss: 0.022090\n",
            "Train Epoch: 2 [15168/21600 (70%)]\tLoss: 0.027007\n",
            "Train Epoch: 2 [15264/21600 (71%)]\tLoss: 0.007239\n",
            "Train Epoch: 2 [15360/21600 (71%)]\tLoss: 0.034367\n",
            "Train Epoch: 2 [15456/21600 (72%)]\tLoss: 0.010829\n",
            "Train Epoch: 2 [15552/21600 (72%)]\tLoss: 0.049600\n",
            "Train Epoch: 2 [15648/21600 (72%)]\tLoss: 0.126270\n",
            "Train Epoch: 2 [15744/21600 (73%)]\tLoss: 0.028097\n",
            "Train Epoch: 2 [15840/21600 (73%)]\tLoss: 0.020626\n",
            "Train Epoch: 2 [15936/21600 (74%)]\tLoss: 0.022041\n",
            "Train Epoch: 2 [16032/21600 (74%)]\tLoss: 0.013198\n",
            "Train Epoch: 2 [16128/21600 (75%)]\tLoss: 0.003519\n",
            "Train Epoch: 2 [16224/21600 (75%)]\tLoss: 0.002935\n",
            "Train Epoch: 2 [16320/21600 (76%)]\tLoss: 0.013265\n",
            "Train Epoch: 2 [16416/21600 (76%)]\tLoss: 0.001859\n",
            "Train Epoch: 2 [16512/21600 (76%)]\tLoss: 0.005219\n",
            "Train Epoch: 2 [16608/21600 (77%)]\tLoss: 0.025337\n",
            "Train Epoch: 2 [16704/21600 (77%)]\tLoss: 0.025252\n",
            "Train Epoch: 2 [16800/21600 (78%)]\tLoss: 0.006771\n",
            "Train Epoch: 2 [16896/21600 (78%)]\tLoss: 0.001910\n",
            "Train Epoch: 2 [16992/21600 (79%)]\tLoss: 0.061115\n",
            "Train Epoch: 2 [17088/21600 (79%)]\tLoss: 0.161944\n",
            "Train Epoch: 2 [17184/21600 (80%)]\tLoss: 0.014248\n",
            "Train Epoch: 2 [17280/21600 (80%)]\tLoss: 0.002896\n",
            "Train Epoch: 2 [17376/21600 (80%)]\tLoss: 0.081120\n",
            "Train Epoch: 2 [17472/21600 (81%)]\tLoss: 0.045149\n",
            "Train Epoch: 2 [17568/21600 (81%)]\tLoss: 0.005686\n",
            "Train Epoch: 2 [17664/21600 (82%)]\tLoss: 0.026592\n",
            "Train Epoch: 2 [17760/21600 (82%)]\tLoss: 0.272385\n",
            "Train Epoch: 2 [17856/21600 (83%)]\tLoss: 0.023690\n",
            "Train Epoch: 2 [17952/21600 (83%)]\tLoss: 0.077851\n",
            "Train Epoch: 2 [18048/21600 (84%)]\tLoss: 0.032084\n",
            "Train Epoch: 2 [18144/21600 (84%)]\tLoss: 0.007802\n",
            "Train Epoch: 2 [18240/21600 (84%)]\tLoss: 0.007548\n",
            "Train Epoch: 2 [18336/21600 (85%)]\tLoss: 0.027264\n",
            "Train Epoch: 2 [18432/21600 (85%)]\tLoss: 0.132278\n",
            "Train Epoch: 2 [18528/21600 (86%)]\tLoss: 0.014023\n",
            "Train Epoch: 2 [18624/21600 (86%)]\tLoss: 0.133708\n",
            "Train Epoch: 2 [18720/21600 (87%)]\tLoss: 0.004833\n",
            "Train Epoch: 2 [18816/21600 (87%)]\tLoss: 0.026868\n",
            "Train Epoch: 2 [18912/21600 (88%)]\tLoss: 0.032943\n",
            "Train Epoch: 2 [19008/21600 (88%)]\tLoss: 0.036269\n",
            "Train Epoch: 2 [19104/21600 (88%)]\tLoss: 0.033086\n",
            "Train Epoch: 2 [19200/21600 (89%)]\tLoss: 0.052733\n",
            "Train Epoch: 2 [19296/21600 (89%)]\tLoss: 0.007130\n",
            "Train Epoch: 2 [19392/21600 (90%)]\tLoss: 0.005158\n",
            "Train Epoch: 2 [19488/21600 (90%)]\tLoss: 0.015372\n",
            "Train Epoch: 2 [19584/21600 (91%)]\tLoss: 0.002856\n",
            "Train Epoch: 2 [19680/21600 (91%)]\tLoss: 0.006252\n",
            "Train Epoch: 2 [19776/21600 (92%)]\tLoss: 0.001228\n",
            "Train Epoch: 2 [19872/21600 (92%)]\tLoss: 0.016467\n",
            "Train Epoch: 2 [19968/21600 (92%)]\tLoss: 0.011755\n",
            "Train Epoch: 2 [20064/21600 (93%)]\tLoss: 0.025702\n",
            "Train Epoch: 2 [20160/21600 (93%)]\tLoss: 0.012952\n",
            "Train Epoch: 2 [20256/21600 (94%)]\tLoss: 0.053919\n",
            "Train Epoch: 2 [20352/21600 (94%)]\tLoss: 0.010972\n",
            "Train Epoch: 2 [20448/21600 (95%)]\tLoss: 0.012487\n",
            "Train Epoch: 2 [20544/21600 (95%)]\tLoss: 0.024149\n",
            "Train Epoch: 2 [20640/21600 (96%)]\tLoss: 0.031258\n",
            "Train Epoch: 2 [20736/21600 (96%)]\tLoss: 0.051514\n",
            "Train Epoch: 2 [20832/21600 (96%)]\tLoss: 0.048141\n",
            "Train Epoch: 2 [20928/21600 (97%)]\tLoss: 0.002186\n",
            "Train Epoch: 2 [21024/21600 (97%)]\tLoss: 0.008995\n",
            "Train Epoch: 2 [21120/21600 (98%)]\tLoss: 0.001235\n",
            "Train Epoch: 2 [21216/21600 (98%)]\tLoss: 0.016641\n",
            "Train Epoch: 2 [21312/21600 (99%)]\tLoss: 0.032309\n",
            "Train Epoch: 2 [21408/21600 (99%)]\tLoss: 0.005495\n",
            "Train Epoch: 2 [21504/21600 (100%)]\tLoss: 0.013434\n",
            "\n",
            "Validation set: Average loss: 0.0091, Accuracy: 5384/5400 (100%)\n",
            "\n",
            "\n",
            "Saved model to model_binary_2.pth.\n",
            "Train Epoch: 3 [0/21600 (0%)]\tLoss: 0.049853\n",
            "Train Epoch: 3 [96/21600 (0%)]\tLoss: 0.002493\n",
            "Train Epoch: 3 [192/21600 (1%)]\tLoss: 0.038270\n",
            "Train Epoch: 3 [288/21600 (1%)]\tLoss: 0.002918\n",
            "Train Epoch: 3 [384/21600 (2%)]\tLoss: 0.003865\n",
            "Train Epoch: 3 [480/21600 (2%)]\tLoss: 0.003283\n",
            "Train Epoch: 3 [576/21600 (3%)]\tLoss: 0.008239\n",
            "Train Epoch: 3 [672/21600 (3%)]\tLoss: 0.007375\n",
            "Train Epoch: 3 [768/21600 (4%)]\tLoss: 0.016319\n",
            "Train Epoch: 3 [864/21600 (4%)]\tLoss: 0.054225\n",
            "Train Epoch: 3 [960/21600 (4%)]\tLoss: 0.011335\n",
            "Train Epoch: 3 [1056/21600 (5%)]\tLoss: 0.008226\n",
            "Train Epoch: 3 [1152/21600 (5%)]\tLoss: 0.004506\n",
            "Train Epoch: 3 [1248/21600 (6%)]\tLoss: 0.044999\n",
            "Train Epoch: 3 [1344/21600 (6%)]\tLoss: 0.053229\n",
            "Train Epoch: 3 [1440/21600 (7%)]\tLoss: 0.018668\n",
            "Train Epoch: 3 [1536/21600 (7%)]\tLoss: 0.089626\n",
            "Train Epoch: 3 [1632/21600 (8%)]\tLoss: 0.123096\n",
            "Train Epoch: 3 [1728/21600 (8%)]\tLoss: 0.019755\n",
            "Train Epoch: 3 [1824/21600 (8%)]\tLoss: 0.078169\n",
            "Train Epoch: 3 [1920/21600 (9%)]\tLoss: 0.004870\n",
            "Train Epoch: 3 [2016/21600 (9%)]\tLoss: 0.032963\n",
            "Train Epoch: 3 [2112/21600 (10%)]\tLoss: 0.001525\n",
            "Train Epoch: 3 [2208/21600 (10%)]\tLoss: 0.062144\n",
            "Train Epoch: 3 [2304/21600 (11%)]\tLoss: 0.005574\n",
            "Train Epoch: 3 [2400/21600 (11%)]\tLoss: 0.002415\n",
            "Train Epoch: 3 [2496/21600 (12%)]\tLoss: 0.003377\n",
            "Train Epoch: 3 [2592/21600 (12%)]\tLoss: 0.007150\n",
            "Train Epoch: 3 [2688/21600 (12%)]\tLoss: 0.054352\n",
            "Train Epoch: 3 [2784/21600 (13%)]\tLoss: 0.018915\n",
            "Train Epoch: 3 [2880/21600 (13%)]\tLoss: 0.018252\n",
            "Train Epoch: 3 [2976/21600 (14%)]\tLoss: 0.026941\n",
            "Train Epoch: 3 [3072/21600 (14%)]\tLoss: 0.094674\n",
            "Train Epoch: 3 [3168/21600 (15%)]\tLoss: 0.041368\n",
            "Train Epoch: 3 [3264/21600 (15%)]\tLoss: 0.023272\n",
            "Train Epoch: 3 [3360/21600 (16%)]\tLoss: 0.007555\n",
            "Train Epoch: 3 [3456/21600 (16%)]\tLoss: 0.016243\n",
            "Train Epoch: 3 [3552/21600 (16%)]\tLoss: 0.046945\n",
            "Train Epoch: 3 [3648/21600 (17%)]\tLoss: 0.009137\n",
            "Train Epoch: 3 [3744/21600 (17%)]\tLoss: 0.012879\n",
            "Train Epoch: 3 [3840/21600 (18%)]\tLoss: 0.167146\n",
            "Train Epoch: 3 [3936/21600 (18%)]\tLoss: 0.020569\n",
            "Train Epoch: 3 [4032/21600 (19%)]\tLoss: 0.016540\n",
            "Train Epoch: 3 [4128/21600 (19%)]\tLoss: 0.008405\n",
            "Train Epoch: 3 [4224/21600 (20%)]\tLoss: 0.165475\n",
            "Train Epoch: 3 [4320/21600 (20%)]\tLoss: 0.003748\n",
            "Train Epoch: 3 [4416/21600 (20%)]\tLoss: 0.005593\n",
            "Train Epoch: 3 [4512/21600 (21%)]\tLoss: 0.005194\n",
            "Train Epoch: 3 [4608/21600 (21%)]\tLoss: 0.095997\n",
            "Train Epoch: 3 [4704/21600 (22%)]\tLoss: 0.029502\n",
            "Train Epoch: 3 [4800/21600 (22%)]\tLoss: 0.001182\n",
            "Train Epoch: 3 [4896/21600 (23%)]\tLoss: 0.014019\n",
            "Train Epoch: 3 [4992/21600 (23%)]\tLoss: 0.008893\n",
            "Train Epoch: 3 [5088/21600 (24%)]\tLoss: 0.002028\n",
            "Train Epoch: 3 [5184/21600 (24%)]\tLoss: 0.000711\n",
            "Train Epoch: 3 [5280/21600 (24%)]\tLoss: 0.037466\n",
            "Train Epoch: 3 [5376/21600 (25%)]\tLoss: 0.012498\n",
            "Train Epoch: 3 [5472/21600 (25%)]\tLoss: 0.003598\n",
            "Train Epoch: 3 [5568/21600 (26%)]\tLoss: 0.011064\n",
            "Train Epoch: 3 [5664/21600 (26%)]\tLoss: 0.004521\n",
            "Train Epoch: 3 [5760/21600 (27%)]\tLoss: 0.045019\n",
            "Train Epoch: 3 [5856/21600 (27%)]\tLoss: 0.007112\n",
            "Train Epoch: 3 [5952/21600 (28%)]\tLoss: 0.007118\n",
            "Train Epoch: 3 [6048/21600 (28%)]\tLoss: 0.036470\n",
            "Train Epoch: 3 [6144/21600 (28%)]\tLoss: 0.014896\n",
            "Train Epoch: 3 [6240/21600 (29%)]\tLoss: 0.098831\n",
            "Train Epoch: 3 [6336/21600 (29%)]\tLoss: 0.018039\n",
            "Train Epoch: 3 [6432/21600 (30%)]\tLoss: 0.004705\n",
            "Train Epoch: 3 [6528/21600 (30%)]\tLoss: 0.004763\n",
            "Train Epoch: 3 [6624/21600 (31%)]\tLoss: 0.001095\n",
            "Train Epoch: 3 [6720/21600 (31%)]\tLoss: 0.019595\n",
            "Train Epoch: 3 [6816/21600 (32%)]\tLoss: 0.002404\n",
            "Train Epoch: 3 [6912/21600 (32%)]\tLoss: 0.011457\n",
            "Train Epoch: 3 [7008/21600 (32%)]\tLoss: 0.008497\n",
            "Train Epoch: 3 [7104/21600 (33%)]\tLoss: 0.055813\n",
            "Train Epoch: 3 [7200/21600 (33%)]\tLoss: 0.086790\n",
            "Train Epoch: 3 [7296/21600 (34%)]\tLoss: 0.028136\n",
            "Train Epoch: 3 [7392/21600 (34%)]\tLoss: 0.004007\n",
            "Train Epoch: 3 [7488/21600 (35%)]\tLoss: 0.001342\n",
            "Train Epoch: 3 [7584/21600 (35%)]\tLoss: 0.016176\n",
            "Train Epoch: 3 [7680/21600 (36%)]\tLoss: 0.011714\n",
            "Train Epoch: 3 [7776/21600 (36%)]\tLoss: 0.005273\n",
            "Train Epoch: 3 [7872/21600 (36%)]\tLoss: 0.031375\n",
            "Train Epoch: 3 [7968/21600 (37%)]\tLoss: 0.004484\n",
            "Train Epoch: 3 [8064/21600 (37%)]\tLoss: 0.007588\n",
            "Train Epoch: 3 [8160/21600 (38%)]\tLoss: 0.009683\n",
            "Train Epoch: 3 [8256/21600 (38%)]\tLoss: 0.009542\n",
            "Train Epoch: 3 [8352/21600 (39%)]\tLoss: 0.086082\n",
            "Train Epoch: 3 [8448/21600 (39%)]\tLoss: 0.001993\n",
            "Train Epoch: 3 [8544/21600 (40%)]\tLoss: 0.000876\n",
            "Train Epoch: 3 [8640/21600 (40%)]\tLoss: 0.007332\n",
            "Train Epoch: 3 [8736/21600 (40%)]\tLoss: 0.072004\n",
            "Train Epoch: 3 [8832/21600 (41%)]\tLoss: 0.004159\n",
            "Train Epoch: 3 [8928/21600 (41%)]\tLoss: 0.061204\n",
            "Train Epoch: 3 [9024/21600 (42%)]\tLoss: 0.003094\n",
            "Train Epoch: 3 [9120/21600 (42%)]\tLoss: 0.004785\n",
            "Train Epoch: 3 [9216/21600 (43%)]\tLoss: 0.014660\n",
            "Train Epoch: 3 [9312/21600 (43%)]\tLoss: 0.001692\n",
            "Train Epoch: 3 [9408/21600 (44%)]\tLoss: 0.049650\n",
            "Train Epoch: 3 [9504/21600 (44%)]\tLoss: 0.010865\n",
            "Train Epoch: 3 [9600/21600 (44%)]\tLoss: 0.042891\n",
            "Train Epoch: 3 [9696/21600 (45%)]\tLoss: 0.009673\n",
            "Train Epoch: 3 [9792/21600 (45%)]\tLoss: 0.037465\n",
            "Train Epoch: 3 [9888/21600 (46%)]\tLoss: 0.008518\n",
            "Train Epoch: 3 [9984/21600 (46%)]\tLoss: 0.042222\n",
            "Train Epoch: 3 [10080/21600 (47%)]\tLoss: 0.013979\n",
            "Train Epoch: 3 [10176/21600 (47%)]\tLoss: 0.001233\n",
            "Train Epoch: 3 [10272/21600 (48%)]\tLoss: 0.035311\n",
            "Train Epoch: 3 [10368/21600 (48%)]\tLoss: 0.027013\n",
            "Train Epoch: 3 [10464/21600 (48%)]\tLoss: 0.005358\n",
            "Train Epoch: 3 [10560/21600 (49%)]\tLoss: 0.008775\n",
            "Train Epoch: 3 [10656/21600 (49%)]\tLoss: 0.006072\n",
            "Train Epoch: 3 [10752/21600 (50%)]\tLoss: 0.010290\n",
            "Train Epoch: 3 [10848/21600 (50%)]\tLoss: 0.120349\n",
            "Train Epoch: 3 [10944/21600 (51%)]\tLoss: 0.048004\n",
            "Train Epoch: 3 [11040/21600 (51%)]\tLoss: 0.002327\n",
            "Train Epoch: 3 [11136/21600 (52%)]\tLoss: 0.003853\n",
            "Train Epoch: 3 [11232/21600 (52%)]\tLoss: 0.061461\n",
            "Train Epoch: 3 [11328/21600 (52%)]\tLoss: 0.001077\n",
            "Train Epoch: 3 [11424/21600 (53%)]\tLoss: 0.325219\n",
            "Train Epoch: 3 [11520/21600 (53%)]\tLoss: 0.003338\n",
            "Train Epoch: 3 [11616/21600 (54%)]\tLoss: 0.001122\n",
            "Train Epoch: 3 [11712/21600 (54%)]\tLoss: 0.000550\n",
            "Train Epoch: 3 [11808/21600 (55%)]\tLoss: 0.001328\n",
            "Train Epoch: 3 [11904/21600 (55%)]\tLoss: 0.001835\n",
            "Train Epoch: 3 [12000/21600 (56%)]\tLoss: 0.089333\n",
            "Train Epoch: 3 [12096/21600 (56%)]\tLoss: 0.020804\n",
            "Train Epoch: 3 [12192/21600 (56%)]\tLoss: 0.010281\n",
            "Train Epoch: 3 [12288/21600 (57%)]\tLoss: 0.031043\n",
            "Train Epoch: 3 [12384/21600 (57%)]\tLoss: 0.012771\n",
            "Train Epoch: 3 [12480/21600 (58%)]\tLoss: 0.053460\n",
            "Train Epoch: 3 [12576/21600 (58%)]\tLoss: 0.000943\n",
            "Train Epoch: 3 [12672/21600 (59%)]\tLoss: 0.000559\n",
            "Train Epoch: 3 [12768/21600 (59%)]\tLoss: 0.001760\n",
            "Train Epoch: 3 [12864/21600 (60%)]\tLoss: 0.017674\n",
            "Train Epoch: 3 [12960/21600 (60%)]\tLoss: 0.098398\n",
            "Train Epoch: 3 [13056/21600 (60%)]\tLoss: 0.003638\n",
            "Train Epoch: 3 [13152/21600 (61%)]\tLoss: 0.021449\n",
            "Train Epoch: 3 [13248/21600 (61%)]\tLoss: 0.136892\n",
            "Train Epoch: 3 [13344/21600 (62%)]\tLoss: 0.002509\n",
            "Train Epoch: 3 [13440/21600 (62%)]\tLoss: 0.003456\n",
            "Train Epoch: 3 [13536/21600 (63%)]\tLoss: 0.017948\n",
            "Train Epoch: 3 [13632/21600 (63%)]\tLoss: 0.033727\n",
            "Train Epoch: 3 [13728/21600 (64%)]\tLoss: 0.016841\n",
            "Train Epoch: 3 [13824/21600 (64%)]\tLoss: 0.018484\n",
            "Train Epoch: 3 [13920/21600 (64%)]\tLoss: 0.001947\n",
            "Train Epoch: 3 [14016/21600 (65%)]\tLoss: 0.007464\n",
            "Train Epoch: 3 [14112/21600 (65%)]\tLoss: 0.015054\n",
            "Train Epoch: 3 [14208/21600 (66%)]\tLoss: 0.133607\n",
            "Train Epoch: 3 [14304/21600 (66%)]\tLoss: 0.027866\n",
            "Train Epoch: 3 [14400/21600 (67%)]\tLoss: 0.011747\n",
            "Train Epoch: 3 [14496/21600 (67%)]\tLoss: 0.015139\n",
            "Train Epoch: 3 [14592/21600 (68%)]\tLoss: 0.025074\n",
            "Train Epoch: 3 [14688/21600 (68%)]\tLoss: 0.062942\n",
            "Train Epoch: 3 [14784/21600 (68%)]\tLoss: 0.003158\n",
            "Train Epoch: 3 [14880/21600 (69%)]\tLoss: 0.005137\n",
            "Train Epoch: 3 [14976/21600 (69%)]\tLoss: 0.119888\n",
            "Train Epoch: 3 [15072/21600 (70%)]\tLoss: 0.007721\n",
            "Train Epoch: 3 [15168/21600 (70%)]\tLoss: 0.003664\n",
            "Train Epoch: 3 [15264/21600 (71%)]\tLoss: 0.012624\n",
            "Train Epoch: 3 [15360/21600 (71%)]\tLoss: 0.006115\n",
            "Train Epoch: 3 [15456/21600 (72%)]\tLoss: 0.013220\n",
            "Train Epoch: 3 [15552/21600 (72%)]\tLoss: 0.021995\n",
            "Train Epoch: 3 [15648/21600 (72%)]\tLoss: 0.007260\n",
            "Train Epoch: 3 [15744/21600 (73%)]\tLoss: 0.026542\n",
            "Train Epoch: 3 [15840/21600 (73%)]\tLoss: 0.112519\n",
            "Train Epoch: 3 [15936/21600 (74%)]\tLoss: 0.048296\n",
            "Train Epoch: 3 [16032/21600 (74%)]\tLoss: 0.003117\n",
            "Train Epoch: 3 [16128/21600 (75%)]\tLoss: 0.074054\n",
            "Train Epoch: 3 [16224/21600 (75%)]\tLoss: 0.001057\n",
            "Train Epoch: 3 [16320/21600 (76%)]\tLoss: 0.114687\n",
            "Train Epoch: 3 [16416/21600 (76%)]\tLoss: 0.013868\n",
            "Train Epoch: 3 [16512/21600 (76%)]\tLoss: 0.018774\n",
            "Train Epoch: 3 [16608/21600 (77%)]\tLoss: 0.003379\n",
            "Train Epoch: 3 [16704/21600 (77%)]\tLoss: 0.012121\n",
            "Train Epoch: 3 [16800/21600 (78%)]\tLoss: 0.038945\n",
            "Train Epoch: 3 [16896/21600 (78%)]\tLoss: 0.018762\n",
            "Train Epoch: 3 [16992/21600 (79%)]\tLoss: 0.002829\n",
            "Train Epoch: 3 [17088/21600 (79%)]\tLoss: 0.006260\n",
            "Train Epoch: 3 [17184/21600 (80%)]\tLoss: 0.011611\n",
            "Train Epoch: 3 [17280/21600 (80%)]\tLoss: 0.003211\n",
            "Train Epoch: 3 [17376/21600 (80%)]\tLoss: 0.005974\n",
            "Train Epoch: 3 [17472/21600 (81%)]\tLoss: 0.018187\n",
            "Train Epoch: 3 [17568/21600 (81%)]\tLoss: 0.035857\n",
            "Train Epoch: 3 [17664/21600 (82%)]\tLoss: 0.006934\n",
            "Train Epoch: 3 [17760/21600 (82%)]\tLoss: 0.029169\n",
            "Train Epoch: 3 [17856/21600 (83%)]\tLoss: 0.022935\n",
            "Train Epoch: 3 [17952/21600 (83%)]\tLoss: 0.071683\n",
            "Train Epoch: 3 [18048/21600 (84%)]\tLoss: 0.001979\n",
            "Train Epoch: 3 [18144/21600 (84%)]\tLoss: 0.008690\n",
            "Train Epoch: 3 [18240/21600 (84%)]\tLoss: 0.002904\n",
            "Train Epoch: 3 [18336/21600 (85%)]\tLoss: 0.011847\n",
            "Train Epoch: 3 [18432/21600 (85%)]\tLoss: 0.043783\n",
            "Train Epoch: 3 [18528/21600 (86%)]\tLoss: 0.004787\n",
            "Train Epoch: 3 [18624/21600 (86%)]\tLoss: 0.022559\n",
            "Train Epoch: 3 [18720/21600 (87%)]\tLoss: 0.007459\n",
            "Train Epoch: 3 [18816/21600 (87%)]\tLoss: 0.021364\n",
            "Train Epoch: 3 [18912/21600 (88%)]\tLoss: 0.001025\n",
            "Train Epoch: 3 [19008/21600 (88%)]\tLoss: 0.058360\n",
            "Train Epoch: 3 [19104/21600 (88%)]\tLoss: 0.140409\n",
            "Train Epoch: 3 [19200/21600 (89%)]\tLoss: 0.001923\n",
            "Train Epoch: 3 [19296/21600 (89%)]\tLoss: 0.005467\n",
            "Train Epoch: 3 [19392/21600 (90%)]\tLoss: 0.004872\n",
            "Train Epoch: 3 [19488/21600 (90%)]\tLoss: 0.002227\n",
            "Train Epoch: 3 [19584/21600 (91%)]\tLoss: 0.019580\n",
            "Train Epoch: 3 [19680/21600 (91%)]\tLoss: 0.000933\n",
            "Train Epoch: 3 [19776/21600 (92%)]\tLoss: 0.018590\n",
            "Train Epoch: 3 [19872/21600 (92%)]\tLoss: 0.024707\n",
            "Train Epoch: 3 [19968/21600 (92%)]\tLoss: 0.010512\n",
            "Train Epoch: 3 [20064/21600 (93%)]\tLoss: 0.002171\n",
            "Train Epoch: 3 [20160/21600 (93%)]\tLoss: 0.002190\n",
            "Train Epoch: 3 [20256/21600 (94%)]\tLoss: 0.019649\n",
            "Train Epoch: 3 [20352/21600 (94%)]\tLoss: 0.003798\n",
            "Train Epoch: 3 [20448/21600 (95%)]\tLoss: 0.005950\n",
            "Train Epoch: 3 [20544/21600 (95%)]\tLoss: 0.004859\n",
            "Train Epoch: 3 [20640/21600 (96%)]\tLoss: 0.017150\n",
            "Train Epoch: 3 [20736/21600 (96%)]\tLoss: 0.038465\n",
            "Train Epoch: 3 [20832/21600 (96%)]\tLoss: 0.005319\n",
            "Train Epoch: 3 [20928/21600 (97%)]\tLoss: 0.164546\n",
            "Train Epoch: 3 [21024/21600 (97%)]\tLoss: 0.015895\n",
            "Train Epoch: 3 [21120/21600 (98%)]\tLoss: 0.097993\n",
            "Train Epoch: 3 [21216/21600 (98%)]\tLoss: 0.004476\n",
            "Train Epoch: 3 [21312/21600 (99%)]\tLoss: 0.002130\n",
            "Train Epoch: 3 [21408/21600 (99%)]\tLoss: 0.004364\n",
            "Train Epoch: 3 [21504/21600 (100%)]\tLoss: 0.013983\n",
            "\n",
            "Validation set: Average loss: 0.1007, Accuracy: 5223/5400 (97%)\n",
            "\n",
            "\n",
            "Saved model to model_binary_3.pth.\n",
            "Train Epoch: 4 [0/21600 (0%)]\tLoss: 0.009738\n",
            "Train Epoch: 4 [96/21600 (0%)]\tLoss: 0.040488\n",
            "Train Epoch: 4 [192/21600 (1%)]\tLoss: 0.008804\n",
            "Train Epoch: 4 [288/21600 (1%)]\tLoss: 0.009883\n",
            "Train Epoch: 4 [384/21600 (2%)]\tLoss: 0.018659\n",
            "Train Epoch: 4 [480/21600 (2%)]\tLoss: 0.001290\n",
            "Train Epoch: 4 [576/21600 (3%)]\tLoss: 0.077037\n",
            "Train Epoch: 4 [672/21600 (3%)]\tLoss: 0.079463\n",
            "Train Epoch: 4 [768/21600 (4%)]\tLoss: 0.002140\n",
            "Train Epoch: 4 [864/21600 (4%)]\tLoss: 0.079291\n",
            "Train Epoch: 4 [960/21600 (4%)]\tLoss: 0.014948\n",
            "Train Epoch: 4 [1056/21600 (5%)]\tLoss: 0.001243\n",
            "Train Epoch: 4 [1152/21600 (5%)]\tLoss: 0.002304\n",
            "Train Epoch: 4 [1248/21600 (6%)]\tLoss: 0.004473\n",
            "Train Epoch: 4 [1344/21600 (6%)]\tLoss: 0.071600\n",
            "Train Epoch: 4 [1440/21600 (7%)]\tLoss: 0.016662\n",
            "Train Epoch: 4 [1536/21600 (7%)]\tLoss: 0.000940\n",
            "Train Epoch: 4 [1632/21600 (8%)]\tLoss: 0.014332\n",
            "Train Epoch: 4 [1728/21600 (8%)]\tLoss: 0.108113\n",
            "Train Epoch: 4 [1824/21600 (8%)]\tLoss: 0.000966\n",
            "Train Epoch: 4 [1920/21600 (9%)]\tLoss: 0.044328\n",
            "Train Epoch: 4 [2016/21600 (9%)]\tLoss: 0.013594\n",
            "Train Epoch: 4 [2112/21600 (10%)]\tLoss: 0.007116\n",
            "Train Epoch: 4 [2208/21600 (10%)]\tLoss: 0.003195\n",
            "Train Epoch: 4 [2304/21600 (11%)]\tLoss: 0.004532\n",
            "Train Epoch: 4 [2400/21600 (11%)]\tLoss: 0.016156\n",
            "Train Epoch: 4 [2496/21600 (12%)]\tLoss: 0.007058\n",
            "Train Epoch: 4 [2592/21600 (12%)]\tLoss: 0.001062\n",
            "Train Epoch: 4 [2688/21600 (12%)]\tLoss: 0.009136\n",
            "Train Epoch: 4 [2784/21600 (13%)]\tLoss: 0.002456\n",
            "Train Epoch: 4 [2880/21600 (13%)]\tLoss: 0.000978\n",
            "Train Epoch: 4 [2976/21600 (14%)]\tLoss: 0.010197\n",
            "Train Epoch: 4 [3072/21600 (14%)]\tLoss: 0.010702\n",
            "Train Epoch: 4 [3168/21600 (15%)]\tLoss: 0.001597\n",
            "Train Epoch: 4 [3264/21600 (15%)]\tLoss: 0.005929\n",
            "Train Epoch: 4 [3360/21600 (16%)]\tLoss: 0.099408\n",
            "Train Epoch: 4 [3456/21600 (16%)]\tLoss: 0.000801\n",
            "Train Epoch: 4 [3552/21600 (16%)]\tLoss: 0.001488\n",
            "Train Epoch: 4 [3648/21600 (17%)]\tLoss: 0.079170\n",
            "Train Epoch: 4 [3744/21600 (17%)]\tLoss: 0.009894\n",
            "Train Epoch: 4 [3840/21600 (18%)]\tLoss: 0.000879\n",
            "Train Epoch: 4 [3936/21600 (18%)]\tLoss: 0.002530\n",
            "Train Epoch: 4 [4032/21600 (19%)]\tLoss: 0.016489\n",
            "Train Epoch: 4 [4128/21600 (19%)]\tLoss: 0.006781\n",
            "Train Epoch: 4 [4224/21600 (20%)]\tLoss: 0.013938\n",
            "Train Epoch: 4 [4320/21600 (20%)]\tLoss: 0.021641\n",
            "Train Epoch: 4 [4416/21600 (20%)]\tLoss: 0.000790\n",
            "Train Epoch: 4 [4512/21600 (21%)]\tLoss: 0.017630\n",
            "Train Epoch: 4 [4608/21600 (21%)]\tLoss: 0.004096\n",
            "Train Epoch: 4 [4704/21600 (22%)]\tLoss: 0.007399\n",
            "Train Epoch: 4 [4800/21600 (22%)]\tLoss: 0.057476\n",
            "Train Epoch: 4 [4896/21600 (23%)]\tLoss: 0.006612\n",
            "Train Epoch: 4 [4992/21600 (23%)]\tLoss: 0.012661\n",
            "Train Epoch: 4 [5088/21600 (24%)]\tLoss: 0.007456\n",
            "Train Epoch: 4 [5184/21600 (24%)]\tLoss: 0.004524\n",
            "Train Epoch: 4 [5280/21600 (24%)]\tLoss: 0.008486\n",
            "Train Epoch: 4 [5376/21600 (25%)]\tLoss: 0.007992\n",
            "Train Epoch: 4 [5472/21600 (25%)]\tLoss: 0.001391\n",
            "Train Epoch: 4 [5568/21600 (26%)]\tLoss: 0.002591\n",
            "Train Epoch: 4 [5664/21600 (26%)]\tLoss: 0.014021\n",
            "Train Epoch: 4 [5760/21600 (27%)]\tLoss: 0.039365\n",
            "Train Epoch: 4 [5856/21600 (27%)]\tLoss: 0.003151\n",
            "Train Epoch: 4 [5952/21600 (28%)]\tLoss: 0.016012\n",
            "Train Epoch: 4 [6048/21600 (28%)]\tLoss: 0.009450\n",
            "Train Epoch: 4 [6144/21600 (28%)]\tLoss: 0.003348\n",
            "Train Epoch: 4 [6240/21600 (29%)]\tLoss: 0.206586\n",
            "Train Epoch: 4 [6336/21600 (29%)]\tLoss: 0.003480\n",
            "Train Epoch: 4 [6432/21600 (30%)]\tLoss: 0.011692\n",
            "Train Epoch: 4 [6528/21600 (30%)]\tLoss: 0.022087\n",
            "Train Epoch: 4 [6624/21600 (31%)]\tLoss: 0.036481\n",
            "Train Epoch: 4 [6720/21600 (31%)]\tLoss: 0.085381\n",
            "Train Epoch: 4 [6816/21600 (32%)]\tLoss: 0.008409\n",
            "Train Epoch: 4 [6912/21600 (32%)]\tLoss: 0.055929\n",
            "Train Epoch: 4 [7008/21600 (32%)]\tLoss: 0.044836\n",
            "Train Epoch: 4 [7104/21600 (33%)]\tLoss: 0.010062\n",
            "Train Epoch: 4 [7200/21600 (33%)]\tLoss: 0.006029\n",
            "Train Epoch: 4 [7296/21600 (34%)]\tLoss: 0.003240\n",
            "Train Epoch: 4 [7392/21600 (34%)]\tLoss: 0.020743\n",
            "Train Epoch: 4 [7488/21600 (35%)]\tLoss: 0.007233\n",
            "Train Epoch: 4 [7584/21600 (35%)]\tLoss: 0.007920\n",
            "Train Epoch: 4 [7680/21600 (36%)]\tLoss: 0.012178\n",
            "Train Epoch: 4 [7776/21600 (36%)]\tLoss: 0.008691\n",
            "Train Epoch: 4 [7872/21600 (36%)]\tLoss: 0.001181\n",
            "Train Epoch: 4 [7968/21600 (37%)]\tLoss: 0.038958\n",
            "Train Epoch: 4 [8064/21600 (37%)]\tLoss: 0.001988\n",
            "Train Epoch: 4 [8160/21600 (38%)]\tLoss: 0.002899\n",
            "Train Epoch: 4 [8256/21600 (38%)]\tLoss: 0.002354\n",
            "Train Epoch: 4 [8352/21600 (39%)]\tLoss: 0.006023\n",
            "Train Epoch: 4 [8448/21600 (39%)]\tLoss: 0.043846\n",
            "Train Epoch: 4 [8544/21600 (40%)]\tLoss: 0.019279\n",
            "Train Epoch: 4 [8640/21600 (40%)]\tLoss: 0.001024\n",
            "Train Epoch: 4 [8736/21600 (40%)]\tLoss: 0.022418\n",
            "Train Epoch: 4 [8832/21600 (41%)]\tLoss: 0.001816\n",
            "Train Epoch: 4 [8928/21600 (41%)]\tLoss: 0.001490\n",
            "Train Epoch: 4 [9024/21600 (42%)]\tLoss: 0.001805\n",
            "Train Epoch: 4 [9120/21600 (42%)]\tLoss: 0.004550\n",
            "Train Epoch: 4 [9216/21600 (43%)]\tLoss: 0.001336\n",
            "Train Epoch: 4 [9312/21600 (43%)]\tLoss: 0.012456\n",
            "Train Epoch: 4 [9408/21600 (44%)]\tLoss: 0.005973\n",
            "Train Epoch: 4 [9504/21600 (44%)]\tLoss: 0.000807\n",
            "Train Epoch: 4 [9600/21600 (44%)]\tLoss: 0.025873\n",
            "Train Epoch: 4 [9696/21600 (45%)]\tLoss: 0.001448\n",
            "Train Epoch: 4 [9792/21600 (45%)]\tLoss: 0.000677\n",
            "Train Epoch: 4 [9888/21600 (46%)]\tLoss: 0.000454\n",
            "Train Epoch: 4 [9984/21600 (46%)]\tLoss: 0.080952\n",
            "Train Epoch: 4 [10080/21600 (47%)]\tLoss: 0.028354\n",
            "Train Epoch: 4 [10176/21600 (47%)]\tLoss: 0.003075\n",
            "Train Epoch: 4 [10272/21600 (48%)]\tLoss: 0.017201\n",
            "Train Epoch: 4 [10368/21600 (48%)]\tLoss: 0.003519\n",
            "Train Epoch: 4 [10464/21600 (48%)]\tLoss: 0.012368\n",
            "Train Epoch: 4 [10560/21600 (49%)]\tLoss: 0.007684\n",
            "Train Epoch: 4 [10656/21600 (49%)]\tLoss: 0.141468\n",
            "Train Epoch: 4 [10752/21600 (50%)]\tLoss: 0.002079\n",
            "Train Epoch: 4 [10848/21600 (50%)]\tLoss: 0.057608\n",
            "Train Epoch: 4 [10944/21600 (51%)]\tLoss: 0.027016\n",
            "Train Epoch: 4 [11040/21600 (51%)]\tLoss: 0.001960\n",
            "Train Epoch: 4 [11136/21600 (52%)]\tLoss: 0.019215\n",
            "Train Epoch: 4 [11232/21600 (52%)]\tLoss: 0.020543\n",
            "Train Epoch: 4 [11328/21600 (52%)]\tLoss: 0.002835\n",
            "Train Epoch: 4 [11424/21600 (53%)]\tLoss: 0.026000\n",
            "Train Epoch: 4 [11520/21600 (53%)]\tLoss: 0.005189\n",
            "Train Epoch: 4 [11616/21600 (54%)]\tLoss: 0.000251\n",
            "Train Epoch: 4 [11712/21600 (54%)]\tLoss: 0.001921\n",
            "Train Epoch: 4 [11808/21600 (55%)]\tLoss: 0.000871\n",
            "Train Epoch: 4 [11904/21600 (55%)]\tLoss: 0.002555\n",
            "Train Epoch: 4 [12000/21600 (56%)]\tLoss: 0.018790\n",
            "Train Epoch: 4 [12096/21600 (56%)]\tLoss: 0.000108\n",
            "Train Epoch: 4 [12192/21600 (56%)]\tLoss: 0.017208\n",
            "Train Epoch: 4 [12288/21600 (57%)]\tLoss: 0.014969\n",
            "Train Epoch: 4 [12384/21600 (57%)]\tLoss: 0.002117\n",
            "Train Epoch: 4 [12480/21600 (58%)]\tLoss: 0.010748\n",
            "Train Epoch: 4 [12576/21600 (58%)]\tLoss: 0.013789\n",
            "Train Epoch: 4 [12672/21600 (59%)]\tLoss: 0.002365\n",
            "Train Epoch: 4 [12768/21600 (59%)]\tLoss: 0.004960\n",
            "Train Epoch: 4 [12864/21600 (60%)]\tLoss: 0.006719\n",
            "Train Epoch: 4 [12960/21600 (60%)]\tLoss: 0.002992\n",
            "Train Epoch: 4 [13056/21600 (60%)]\tLoss: 0.001393\n",
            "Train Epoch: 4 [13152/21600 (61%)]\tLoss: 0.004258\n",
            "Train Epoch: 4 [13248/21600 (61%)]\tLoss: 0.199683\n",
            "Train Epoch: 4 [13344/21600 (62%)]\tLoss: 0.007459\n",
            "Train Epoch: 4 [13440/21600 (62%)]\tLoss: 0.001541\n",
            "Train Epoch: 4 [13536/21600 (63%)]\tLoss: 0.001742\n",
            "Train Epoch: 4 [13632/21600 (63%)]\tLoss: 0.006629\n",
            "Train Epoch: 4 [13728/21600 (64%)]\tLoss: 0.004337\n",
            "Train Epoch: 4 [13824/21600 (64%)]\tLoss: 0.020989\n",
            "Train Epoch: 4 [13920/21600 (64%)]\tLoss: 0.006433\n",
            "Train Epoch: 4 [14016/21600 (65%)]\tLoss: 0.058022\n",
            "Train Epoch: 4 [14112/21600 (65%)]\tLoss: 0.008089\n",
            "Train Epoch: 4 [14208/21600 (66%)]\tLoss: 0.002682\n",
            "Train Epoch: 4 [14304/21600 (66%)]\tLoss: 0.029570\n",
            "Train Epoch: 4 [14400/21600 (67%)]\tLoss: 0.000677\n",
            "Train Epoch: 4 [14496/21600 (67%)]\tLoss: 0.033905\n",
            "Train Epoch: 4 [14592/21600 (68%)]\tLoss: 0.010524\n",
            "Train Epoch: 4 [14688/21600 (68%)]\tLoss: 0.004921\n",
            "Train Epoch: 4 [14784/21600 (68%)]\tLoss: 0.006958\n",
            "Train Epoch: 4 [14880/21600 (69%)]\tLoss: 0.001911\n",
            "Train Epoch: 4 [14976/21600 (69%)]\tLoss: 0.007795\n",
            "Train Epoch: 4 [15072/21600 (70%)]\tLoss: 0.002228\n",
            "Train Epoch: 4 [15168/21600 (70%)]\tLoss: 0.030856\n",
            "Train Epoch: 4 [15264/21600 (71%)]\tLoss: 0.058776\n",
            "Train Epoch: 4 [15360/21600 (71%)]\tLoss: 0.043261\n",
            "Train Epoch: 4 [15456/21600 (72%)]\tLoss: 0.000788\n",
            "Train Epoch: 4 [15552/21600 (72%)]\tLoss: 0.002150\n",
            "Train Epoch: 4 [15648/21600 (72%)]\tLoss: 0.060887\n",
            "Train Epoch: 4 [15744/21600 (73%)]\tLoss: 0.012897\n",
            "Train Epoch: 4 [15840/21600 (73%)]\tLoss: 0.001508\n",
            "Train Epoch: 4 [15936/21600 (74%)]\tLoss: 0.011565\n",
            "Train Epoch: 4 [16032/21600 (74%)]\tLoss: 0.004720\n",
            "Train Epoch: 4 [16128/21600 (75%)]\tLoss: 0.027928\n",
            "Train Epoch: 4 [16224/21600 (75%)]\tLoss: 0.010534\n",
            "Train Epoch: 4 [16320/21600 (76%)]\tLoss: 0.010042\n",
            "Train Epoch: 4 [16416/21600 (76%)]\tLoss: 0.001279\n",
            "Train Epoch: 4 [16512/21600 (76%)]\tLoss: 0.000737\n",
            "Train Epoch: 4 [16608/21600 (77%)]\tLoss: 0.004012\n",
            "Train Epoch: 4 [16704/21600 (77%)]\tLoss: 0.005753\n",
            "Train Epoch: 4 [16800/21600 (78%)]\tLoss: 0.172598\n",
            "Train Epoch: 4 [16896/21600 (78%)]\tLoss: 0.000653\n",
            "Train Epoch: 4 [16992/21600 (79%)]\tLoss: 0.002358\n",
            "Train Epoch: 4 [17088/21600 (79%)]\tLoss: 0.000905\n",
            "Train Epoch: 4 [17184/21600 (80%)]\tLoss: 0.001035\n",
            "Train Epoch: 4 [17280/21600 (80%)]\tLoss: 0.001390\n",
            "Train Epoch: 4 [17376/21600 (80%)]\tLoss: 0.006603\n",
            "Train Epoch: 4 [17472/21600 (81%)]\tLoss: 0.067302\n",
            "Train Epoch: 4 [17568/21600 (81%)]\tLoss: 0.030078\n",
            "Train Epoch: 4 [17664/21600 (82%)]\tLoss: 0.002875\n",
            "Train Epoch: 4 [17760/21600 (82%)]\tLoss: 0.091693\n",
            "Train Epoch: 4 [17856/21600 (83%)]\tLoss: 0.028084\n",
            "Train Epoch: 4 [17952/21600 (83%)]\tLoss: 0.002289\n",
            "Train Epoch: 4 [18048/21600 (84%)]\tLoss: 0.003222\n",
            "Train Epoch: 4 [18144/21600 (84%)]\tLoss: 0.057756\n",
            "Train Epoch: 4 [18240/21600 (84%)]\tLoss: 0.003279\n",
            "Train Epoch: 4 [18336/21600 (85%)]\tLoss: 0.004526\n",
            "Train Epoch: 4 [18432/21600 (85%)]\tLoss: 0.000251\n",
            "Train Epoch: 4 [18528/21600 (86%)]\tLoss: 0.053520\n",
            "Train Epoch: 4 [18624/21600 (86%)]\tLoss: 0.001347\n",
            "Train Epoch: 4 [18720/21600 (87%)]\tLoss: 0.004922\n",
            "Train Epoch: 4 [18816/21600 (87%)]\tLoss: 0.000243\n",
            "Train Epoch: 4 [18912/21600 (88%)]\tLoss: 0.017147\n",
            "Train Epoch: 4 [19008/21600 (88%)]\tLoss: 0.003149\n",
            "Train Epoch: 4 [19104/21600 (88%)]\tLoss: 0.001751\n",
            "Train Epoch: 4 [19200/21600 (89%)]\tLoss: 0.004212\n",
            "Train Epoch: 4 [19296/21600 (89%)]\tLoss: 0.142810\n",
            "Train Epoch: 4 [19392/21600 (90%)]\tLoss: 0.007158\n",
            "Train Epoch: 4 [19488/21600 (90%)]\tLoss: 0.001831\n",
            "Train Epoch: 4 [19584/21600 (91%)]\tLoss: 0.009449\n",
            "Train Epoch: 4 [19680/21600 (91%)]\tLoss: 0.001455\n",
            "Train Epoch: 4 [19776/21600 (92%)]\tLoss: 0.036938\n",
            "Train Epoch: 4 [19872/21600 (92%)]\tLoss: 0.010330\n",
            "Train Epoch: 4 [19968/21600 (92%)]\tLoss: 0.000911\n",
            "Train Epoch: 4 [20064/21600 (93%)]\tLoss: 0.000792\n",
            "Train Epoch: 4 [20160/21600 (93%)]\tLoss: 0.011680\n",
            "Train Epoch: 4 [20256/21600 (94%)]\tLoss: 0.002294\n",
            "Train Epoch: 4 [20352/21600 (94%)]\tLoss: 0.012534\n",
            "Train Epoch: 4 [20448/21600 (95%)]\tLoss: 0.005064\n",
            "Train Epoch: 4 [20544/21600 (95%)]\tLoss: 0.074544\n",
            "Train Epoch: 4 [20640/21600 (96%)]\tLoss: 0.000220\n",
            "Train Epoch: 4 [20736/21600 (96%)]\tLoss: 0.005251\n",
            "Train Epoch: 4 [20832/21600 (96%)]\tLoss: 0.000787\n",
            "Train Epoch: 4 [20928/21600 (97%)]\tLoss: 0.012514\n",
            "Train Epoch: 4 [21024/21600 (97%)]\tLoss: 0.097430\n",
            "Train Epoch: 4 [21120/21600 (98%)]\tLoss: 0.001220\n",
            "Train Epoch: 4 [21216/21600 (98%)]\tLoss: 0.000725\n",
            "Train Epoch: 4 [21312/21600 (99%)]\tLoss: 0.001210\n",
            "Train Epoch: 4 [21408/21600 (99%)]\tLoss: 0.009896\n",
            "Train Epoch: 4 [21504/21600 (100%)]\tLoss: 0.004150\n",
            "\n",
            "Validation set: Average loss: 0.0621, Accuracy: 5266/5400 (98%)\n",
            "\n",
            "\n",
            "Saved model to model_binary_4.pth.\n",
            "Train Epoch: 5 [0/21600 (0%)]\tLoss: 0.011519\n",
            "Train Epoch: 5 [96/21600 (0%)]\tLoss: 0.013846\n",
            "Train Epoch: 5 [192/21600 (1%)]\tLoss: 0.003011\n",
            "Train Epoch: 5 [288/21600 (1%)]\tLoss: 0.008385\n",
            "Train Epoch: 5 [384/21600 (2%)]\tLoss: 0.005522\n",
            "Train Epoch: 5 [480/21600 (2%)]\tLoss: 0.019230\n",
            "Train Epoch: 5 [576/21600 (3%)]\tLoss: 0.001294\n",
            "Train Epoch: 5 [672/21600 (3%)]\tLoss: 0.001047\n",
            "Train Epoch: 5 [768/21600 (4%)]\tLoss: 0.075547\n",
            "Train Epoch: 5 [864/21600 (4%)]\tLoss: 0.001043\n",
            "Train Epoch: 5 [960/21600 (4%)]\tLoss: 0.030800\n",
            "Train Epoch: 5 [1056/21600 (5%)]\tLoss: 0.022097\n",
            "Train Epoch: 5 [1152/21600 (5%)]\tLoss: 0.003765\n",
            "Train Epoch: 5 [1248/21600 (6%)]\tLoss: 0.002955\n",
            "Train Epoch: 5 [1344/21600 (6%)]\tLoss: 0.117987\n",
            "Train Epoch: 5 [1440/21600 (7%)]\tLoss: 0.000657\n",
            "Train Epoch: 5 [1536/21600 (7%)]\tLoss: 0.001429\n",
            "Train Epoch: 5 [1632/21600 (8%)]\tLoss: 0.001300\n",
            "Train Epoch: 5 [1728/21600 (8%)]\tLoss: 0.009756\n",
            "Train Epoch: 5 [1824/21600 (8%)]\tLoss: 0.049949\n",
            "Train Epoch: 5 [1920/21600 (9%)]\tLoss: 0.004646\n",
            "Train Epoch: 5 [2016/21600 (9%)]\tLoss: 0.005300\n",
            "Train Epoch: 5 [2112/21600 (10%)]\tLoss: 0.009064\n",
            "Train Epoch: 5 [2208/21600 (10%)]\tLoss: 0.013216\n",
            "Train Epoch: 5 [2304/21600 (11%)]\tLoss: 0.088755\n",
            "Train Epoch: 5 [2400/21600 (11%)]\tLoss: 0.002791\n",
            "Train Epoch: 5 [2496/21600 (12%)]\tLoss: 0.001450\n",
            "Train Epoch: 5 [2592/21600 (12%)]\tLoss: 0.003187\n",
            "Train Epoch: 5 [2688/21600 (12%)]\tLoss: 0.014365\n",
            "Train Epoch: 5 [2784/21600 (13%)]\tLoss: 0.005827\n",
            "Train Epoch: 5 [2880/21600 (13%)]\tLoss: 0.160493\n",
            "Train Epoch: 5 [2976/21600 (14%)]\tLoss: 0.005212\n",
            "Train Epoch: 5 [3072/21600 (14%)]\tLoss: 0.000542\n",
            "Train Epoch: 5 [3168/21600 (15%)]\tLoss: 0.000715\n",
            "Train Epoch: 5 [3264/21600 (15%)]\tLoss: 0.036331\n",
            "Train Epoch: 5 [3360/21600 (16%)]\tLoss: 0.005233\n",
            "Train Epoch: 5 [3456/21600 (16%)]\tLoss: 0.020885\n",
            "Train Epoch: 5 [3552/21600 (16%)]\tLoss: 0.018943\n",
            "Train Epoch: 5 [3648/21600 (17%)]\tLoss: 0.001466\n",
            "Train Epoch: 5 [3744/21600 (17%)]\tLoss: 0.153628\n",
            "Train Epoch: 5 [3840/21600 (18%)]\tLoss: 0.002986\n",
            "Train Epoch: 5 [3936/21600 (18%)]\tLoss: 0.028513\n",
            "Train Epoch: 5 [4032/21600 (19%)]\tLoss: 0.003675\n",
            "Train Epoch: 5 [4128/21600 (19%)]\tLoss: 0.001974\n",
            "Train Epoch: 5 [4224/21600 (20%)]\tLoss: 0.039192\n",
            "Train Epoch: 5 [4320/21600 (20%)]\tLoss: 0.001118\n",
            "Train Epoch: 5 [4416/21600 (20%)]\tLoss: 0.000384\n",
            "Train Epoch: 5 [4512/21600 (21%)]\tLoss: 0.003057\n",
            "Train Epoch: 5 [4608/21600 (21%)]\tLoss: 0.000864\n",
            "Train Epoch: 5 [4704/21600 (22%)]\tLoss: 0.001066\n",
            "Train Epoch: 5 [4800/21600 (22%)]\tLoss: 0.001216\n",
            "Train Epoch: 5 [4896/21600 (23%)]\tLoss: 0.004905\n",
            "Train Epoch: 5 [4992/21600 (23%)]\tLoss: 0.004856\n",
            "Train Epoch: 5 [5088/21600 (24%)]\tLoss: 0.002796\n",
            "Train Epoch: 5 [5184/21600 (24%)]\tLoss: 0.007117\n",
            "Train Epoch: 5 [5280/21600 (24%)]\tLoss: 0.000611\n",
            "Train Epoch: 5 [5376/21600 (25%)]\tLoss: 0.005320\n",
            "Train Epoch: 5 [5472/21600 (25%)]\tLoss: 0.051046\n",
            "Train Epoch: 5 [5568/21600 (26%)]\tLoss: 0.008373\n",
            "Train Epoch: 5 [5664/21600 (26%)]\tLoss: 0.228489\n",
            "Train Epoch: 5 [5760/21600 (27%)]\tLoss: 0.002109\n",
            "Train Epoch: 5 [5856/21600 (27%)]\tLoss: 0.010505\n",
            "Train Epoch: 5 [5952/21600 (28%)]\tLoss: 0.015674\n",
            "Train Epoch: 5 [6048/21600 (28%)]\tLoss: 0.001982\n",
            "Train Epoch: 5 [6144/21600 (28%)]\tLoss: 0.000619\n",
            "Train Epoch: 5 [6240/21600 (29%)]\tLoss: 0.000582\n",
            "Train Epoch: 5 [6336/21600 (29%)]\tLoss: 0.137439\n",
            "Train Epoch: 5 [6432/21600 (30%)]\tLoss: 0.021183\n",
            "Train Epoch: 5 [6528/21600 (30%)]\tLoss: 0.020299\n",
            "Train Epoch: 5 [6624/21600 (31%)]\tLoss: 0.027789\n",
            "Train Epoch: 5 [6720/21600 (31%)]\tLoss: 0.209054\n",
            "Train Epoch: 5 [6816/21600 (32%)]\tLoss: 0.000507\n",
            "Train Epoch: 5 [6912/21600 (32%)]\tLoss: 0.006056\n",
            "Train Epoch: 5 [7008/21600 (32%)]\tLoss: 0.119738\n",
            "Train Epoch: 5 [7104/21600 (33%)]\tLoss: 0.014783\n",
            "Train Epoch: 5 [7200/21600 (33%)]\tLoss: 0.106218\n",
            "Train Epoch: 5 [7296/21600 (34%)]\tLoss: 0.002970\n",
            "Train Epoch: 5 [7392/21600 (34%)]\tLoss: 0.039410\n",
            "Train Epoch: 5 [7488/21600 (35%)]\tLoss: 0.015093\n",
            "Train Epoch: 5 [7584/21600 (35%)]\tLoss: 0.015080\n",
            "Train Epoch: 5 [7680/21600 (36%)]\tLoss: 0.001664\n",
            "Train Epoch: 5 [7776/21600 (36%)]\tLoss: 0.001609\n",
            "Train Epoch: 5 [7872/21600 (36%)]\tLoss: 0.002618\n",
            "Train Epoch: 5 [7968/21600 (37%)]\tLoss: 0.046152\n",
            "Train Epoch: 5 [8064/21600 (37%)]\tLoss: 0.007968\n",
            "Train Epoch: 5 [8160/21600 (38%)]\tLoss: 0.033649\n",
            "Train Epoch: 5 [8256/21600 (38%)]\tLoss: 0.009561\n",
            "Train Epoch: 5 [8352/21600 (39%)]\tLoss: 0.002589\n",
            "Train Epoch: 5 [8448/21600 (39%)]\tLoss: 0.029524\n",
            "Train Epoch: 5 [8544/21600 (40%)]\tLoss: 0.001093\n",
            "Train Epoch: 5 [8640/21600 (40%)]\tLoss: 0.006608\n",
            "Train Epoch: 5 [8736/21600 (40%)]\tLoss: 0.000416\n",
            "Train Epoch: 5 [8832/21600 (41%)]\tLoss: 0.084866\n",
            "Train Epoch: 5 [8928/21600 (41%)]\tLoss: 0.129167\n",
            "Train Epoch: 5 [9024/21600 (42%)]\tLoss: 0.009557\n",
            "Train Epoch: 5 [9120/21600 (42%)]\tLoss: 0.014228\n",
            "Train Epoch: 5 [9216/21600 (43%)]\tLoss: 0.005314\n",
            "Train Epoch: 5 [9312/21600 (43%)]\tLoss: 0.004191\n",
            "Train Epoch: 5 [9408/21600 (44%)]\tLoss: 0.000512\n",
            "Train Epoch: 5 [9504/21600 (44%)]\tLoss: 0.001538\n",
            "Train Epoch: 5 [9600/21600 (44%)]\tLoss: 0.009268\n",
            "Train Epoch: 5 [9696/21600 (45%)]\tLoss: 0.000559\n",
            "Train Epoch: 5 [9792/21600 (45%)]\tLoss: 0.003220\n",
            "Train Epoch: 5 [9888/21600 (46%)]\tLoss: 0.238556\n",
            "Train Epoch: 5 [9984/21600 (46%)]\tLoss: 0.003633\n",
            "Train Epoch: 5 [10080/21600 (47%)]\tLoss: 0.008049\n",
            "Train Epoch: 5 [10176/21600 (47%)]\tLoss: 0.006712\n",
            "Train Epoch: 5 [10272/21600 (48%)]\tLoss: 0.001307\n",
            "Train Epoch: 5 [10368/21600 (48%)]\tLoss: 0.010315\n",
            "Train Epoch: 5 [10464/21600 (48%)]\tLoss: 0.001400\n",
            "Train Epoch: 5 [10560/21600 (49%)]\tLoss: 0.008013\n",
            "Train Epoch: 5 [10656/21600 (49%)]\tLoss: 0.002516\n",
            "Train Epoch: 5 [10752/21600 (50%)]\tLoss: 0.000338\n",
            "Train Epoch: 5 [10848/21600 (50%)]\tLoss: 0.000142\n",
            "Train Epoch: 5 [10944/21600 (51%)]\tLoss: 0.016125\n",
            "Train Epoch: 5 [11040/21600 (51%)]\tLoss: 0.019950\n",
            "Train Epoch: 5 [11136/21600 (52%)]\tLoss: 0.000692\n",
            "Train Epoch: 5 [11232/21600 (52%)]\tLoss: 0.033376\n",
            "Train Epoch: 5 [11328/21600 (52%)]\tLoss: 0.019071\n",
            "Train Epoch: 5 [11424/21600 (53%)]\tLoss: 0.000416\n",
            "Train Epoch: 5 [11520/21600 (53%)]\tLoss: 0.044076\n",
            "Train Epoch: 5 [11616/21600 (54%)]\tLoss: 0.017578\n",
            "Train Epoch: 5 [11712/21600 (54%)]\tLoss: 0.001536\n",
            "Train Epoch: 5 [11808/21600 (55%)]\tLoss: 0.000955\n",
            "Train Epoch: 5 [11904/21600 (55%)]\tLoss: 0.003059\n",
            "Train Epoch: 5 [12000/21600 (56%)]\tLoss: 0.001000\n",
            "Train Epoch: 5 [12096/21600 (56%)]\tLoss: 0.092061\n",
            "Train Epoch: 5 [12192/21600 (56%)]\tLoss: 0.000277\n",
            "Train Epoch: 5 [12288/21600 (57%)]\tLoss: 0.000835\n",
            "Train Epoch: 5 [12384/21600 (57%)]\tLoss: 0.203875\n",
            "Train Epoch: 5 [12480/21600 (58%)]\tLoss: 0.000207\n",
            "Train Epoch: 5 [12576/21600 (58%)]\tLoss: 0.001707\n",
            "Train Epoch: 5 [12672/21600 (59%)]\tLoss: 0.006348\n",
            "Train Epoch: 5 [12768/21600 (59%)]\tLoss: 0.004816\n",
            "Train Epoch: 5 [12864/21600 (60%)]\tLoss: 0.000798\n",
            "Train Epoch: 5 [12960/21600 (60%)]\tLoss: 0.001582\n",
            "Train Epoch: 5 [13056/21600 (60%)]\tLoss: 0.029003\n",
            "Train Epoch: 5 [13152/21600 (61%)]\tLoss: 0.007248\n",
            "Train Epoch: 5 [13248/21600 (61%)]\tLoss: 0.001217\n",
            "Train Epoch: 5 [13344/21600 (62%)]\tLoss: 0.005488\n",
            "Train Epoch: 5 [13440/21600 (62%)]\tLoss: 0.004402\n",
            "Train Epoch: 5 [13536/21600 (63%)]\tLoss: 0.002606\n",
            "Train Epoch: 5 [13632/21600 (63%)]\tLoss: 0.002683\n",
            "Train Epoch: 5 [13728/21600 (64%)]\tLoss: 0.012706\n",
            "Train Epoch: 5 [13824/21600 (64%)]\tLoss: 0.000546\n",
            "Train Epoch: 5 [13920/21600 (64%)]\tLoss: 0.004076\n",
            "Train Epoch: 5 [14016/21600 (65%)]\tLoss: 0.079840\n",
            "Train Epoch: 5 [14112/21600 (65%)]\tLoss: 0.005492\n",
            "Train Epoch: 5 [14208/21600 (66%)]\tLoss: 0.001191\n",
            "Train Epoch: 5 [14304/21600 (66%)]\tLoss: 0.000318\n",
            "Train Epoch: 5 [14400/21600 (67%)]\tLoss: 0.001568\n",
            "Train Epoch: 5 [14496/21600 (67%)]\tLoss: 0.001352\n",
            "Train Epoch: 5 [14592/21600 (68%)]\tLoss: 0.051767\n",
            "Train Epoch: 5 [14688/21600 (68%)]\tLoss: 0.002458\n",
            "Train Epoch: 5 [14784/21600 (68%)]\tLoss: 0.027029\n",
            "Train Epoch: 5 [14880/21600 (69%)]\tLoss: 0.000876\n",
            "Train Epoch: 5 [14976/21600 (69%)]\tLoss: 0.015119\n",
            "Train Epoch: 5 [15072/21600 (70%)]\tLoss: 0.001172\n",
            "Train Epoch: 5 [15168/21600 (70%)]\tLoss: 0.004941\n",
            "Train Epoch: 5 [15264/21600 (71%)]\tLoss: 0.001901\n",
            "Train Epoch: 5 [15360/21600 (71%)]\tLoss: 0.000276\n",
            "Train Epoch: 5 [15456/21600 (72%)]\tLoss: 0.006132\n",
            "Train Epoch: 5 [15552/21600 (72%)]\tLoss: 0.042076\n",
            "Train Epoch: 5 [15648/21600 (72%)]\tLoss: 0.000259\n",
            "Train Epoch: 5 [15744/21600 (73%)]\tLoss: 0.000399\n",
            "Train Epoch: 5 [15840/21600 (73%)]\tLoss: 0.018101\n",
            "Train Epoch: 5 [15936/21600 (74%)]\tLoss: 0.008560\n",
            "Train Epoch: 5 [16032/21600 (74%)]\tLoss: 0.015288\n",
            "Train Epoch: 5 [16128/21600 (75%)]\tLoss: 0.007873\n",
            "Train Epoch: 5 [16224/21600 (75%)]\tLoss: 0.002075\n",
            "Train Epoch: 5 [16320/21600 (76%)]\tLoss: 0.002632\n",
            "Train Epoch: 5 [16416/21600 (76%)]\tLoss: 0.013572\n",
            "Train Epoch: 5 [16512/21600 (76%)]\tLoss: 0.013301\n",
            "Train Epoch: 5 [16608/21600 (77%)]\tLoss: 0.000482\n",
            "Train Epoch: 5 [16704/21600 (77%)]\tLoss: 0.001141\n",
            "Train Epoch: 5 [16800/21600 (78%)]\tLoss: 0.000646\n",
            "Train Epoch: 5 [16896/21600 (78%)]\tLoss: 0.003581\n",
            "Train Epoch: 5 [16992/21600 (79%)]\tLoss: 0.018530\n",
            "Train Epoch: 5 [17088/21600 (79%)]\tLoss: 0.054408\n",
            "Train Epoch: 5 [17184/21600 (80%)]\tLoss: 0.001278\n",
            "Train Epoch: 5 [17280/21600 (80%)]\tLoss: 0.000441\n",
            "Train Epoch: 5 [17376/21600 (80%)]\tLoss: 0.005178\n",
            "Train Epoch: 5 [17472/21600 (81%)]\tLoss: 0.028632\n",
            "Train Epoch: 5 [17568/21600 (81%)]\tLoss: 0.044406\n",
            "Train Epoch: 5 [17664/21600 (82%)]\tLoss: 0.003007\n",
            "Train Epoch: 5 [17760/21600 (82%)]\tLoss: 0.001756\n",
            "Train Epoch: 5 [17856/21600 (83%)]\tLoss: 0.007369\n",
            "Train Epoch: 5 [17952/21600 (83%)]\tLoss: 0.001339\n",
            "Train Epoch: 5 [18048/21600 (84%)]\tLoss: 0.123557\n",
            "Train Epoch: 5 [18144/21600 (84%)]\tLoss: 0.136708\n",
            "Train Epoch: 5 [18240/21600 (84%)]\tLoss: 0.065518\n",
            "Train Epoch: 5 [18336/21600 (85%)]\tLoss: 0.012976\n",
            "Train Epoch: 5 [18432/21600 (85%)]\tLoss: 0.003220\n",
            "Train Epoch: 5 [18528/21600 (86%)]\tLoss: 0.002217\n",
            "Train Epoch: 5 [18624/21600 (86%)]\tLoss: 0.003402\n",
            "Train Epoch: 5 [18720/21600 (87%)]\tLoss: 0.000926\n",
            "Train Epoch: 5 [18816/21600 (87%)]\tLoss: 0.000247\n",
            "Train Epoch: 5 [18912/21600 (88%)]\tLoss: 0.020376\n",
            "Train Epoch: 5 [19008/21600 (88%)]\tLoss: 0.016247\n",
            "Train Epoch: 5 [19104/21600 (88%)]\tLoss: 0.000098\n",
            "Train Epoch: 5 [19200/21600 (89%)]\tLoss: 0.002135\n",
            "Train Epoch: 5 [19296/21600 (89%)]\tLoss: 0.006958\n",
            "Train Epoch: 5 [19392/21600 (90%)]\tLoss: 0.000679\n",
            "Train Epoch: 5 [19488/21600 (90%)]\tLoss: 0.004413\n",
            "Train Epoch: 5 [19584/21600 (91%)]\tLoss: 0.002568\n",
            "Train Epoch: 5 [19680/21600 (91%)]\tLoss: 0.012289\n",
            "Train Epoch: 5 [19776/21600 (92%)]\tLoss: 0.004508\n",
            "Train Epoch: 5 [19872/21600 (92%)]\tLoss: 0.001046\n",
            "Train Epoch: 5 [19968/21600 (92%)]\tLoss: 0.005092\n",
            "Train Epoch: 5 [20064/21600 (93%)]\tLoss: 0.000336\n",
            "Train Epoch: 5 [20160/21600 (93%)]\tLoss: 0.010246\n",
            "Train Epoch: 5 [20256/21600 (94%)]\tLoss: 0.004357\n",
            "Train Epoch: 5 [20352/21600 (94%)]\tLoss: 0.005148\n",
            "Train Epoch: 5 [20448/21600 (95%)]\tLoss: 0.000781\n",
            "Train Epoch: 5 [20544/21600 (95%)]\tLoss: 0.008028\n",
            "Train Epoch: 5 [20640/21600 (96%)]\tLoss: 0.000744\n",
            "Train Epoch: 5 [20736/21600 (96%)]\tLoss: 0.004451\n",
            "Train Epoch: 5 [20832/21600 (96%)]\tLoss: 0.000112\n",
            "Train Epoch: 5 [20928/21600 (97%)]\tLoss: 0.012206\n",
            "Train Epoch: 5 [21024/21600 (97%)]\tLoss: 0.005813\n",
            "Train Epoch: 5 [21120/21600 (98%)]\tLoss: 0.002010\n",
            "Train Epoch: 5 [21216/21600 (98%)]\tLoss: 0.000915\n",
            "Train Epoch: 5 [21312/21600 (99%)]\tLoss: 0.005029\n",
            "Train Epoch: 5 [21408/21600 (99%)]\tLoss: 0.000434\n",
            "Train Epoch: 5 [21504/21600 (100%)]\tLoss: 0.002083\n",
            "\n",
            "Validation set: Average loss: 0.0005, Accuracy: 5400/5400 (100%)\n",
            "\n",
            "\n",
            "Saved model to model_binary_5.pth.\n",
            "Train Epoch: 6 [0/21600 (0%)]\tLoss: 0.002462\n",
            "Train Epoch: 6 [96/21600 (0%)]\tLoss: 0.001616\n",
            "Train Epoch: 6 [192/21600 (1%)]\tLoss: 0.001578\n",
            "Train Epoch: 6 [288/21600 (1%)]\tLoss: 0.001137\n",
            "Train Epoch: 6 [384/21600 (2%)]\tLoss: 0.015527\n",
            "Train Epoch: 6 [480/21600 (2%)]\tLoss: 0.005302\n",
            "Train Epoch: 6 [576/21600 (3%)]\tLoss: 0.067329\n",
            "Train Epoch: 6 [672/21600 (3%)]\tLoss: 0.000453\n",
            "Train Epoch: 6 [768/21600 (4%)]\tLoss: 0.006437\n",
            "Train Epoch: 6 [864/21600 (4%)]\tLoss: 0.003126\n",
            "Train Epoch: 6 [960/21600 (4%)]\tLoss: 0.012170\n",
            "Train Epoch: 6 [1056/21600 (5%)]\tLoss: 0.004556\n",
            "Train Epoch: 6 [1152/21600 (5%)]\tLoss: 0.010849\n",
            "Train Epoch: 6 [1248/21600 (6%)]\tLoss: 0.020551\n",
            "Train Epoch: 6 [1344/21600 (6%)]\tLoss: 0.023939\n",
            "Train Epoch: 6 [1440/21600 (7%)]\tLoss: 0.001407\n",
            "Train Epoch: 6 [1536/21600 (7%)]\tLoss: 0.001291\n",
            "Train Epoch: 6 [1632/21600 (8%)]\tLoss: 0.012993\n",
            "Train Epoch: 6 [1728/21600 (8%)]\tLoss: 0.000566\n",
            "Train Epoch: 6 [1824/21600 (8%)]\tLoss: 0.087298\n",
            "Train Epoch: 6 [1920/21600 (9%)]\tLoss: 0.000811\n",
            "Train Epoch: 6 [2016/21600 (9%)]\tLoss: 0.000390\n",
            "Train Epoch: 6 [2112/21600 (10%)]\tLoss: 0.039876\n",
            "Train Epoch: 6 [2208/21600 (10%)]\tLoss: 0.008006\n",
            "Train Epoch: 6 [2304/21600 (11%)]\tLoss: 0.012258\n",
            "Train Epoch: 6 [2400/21600 (11%)]\tLoss: 0.001679\n",
            "Train Epoch: 6 [2496/21600 (12%)]\tLoss: 0.000559\n",
            "Train Epoch: 6 [2592/21600 (12%)]\tLoss: 0.013954\n",
            "Train Epoch: 6 [2688/21600 (12%)]\tLoss: 0.000054\n",
            "Train Epoch: 6 [2784/21600 (13%)]\tLoss: 0.004154\n",
            "Train Epoch: 6 [2880/21600 (13%)]\tLoss: 0.000795\n",
            "Train Epoch: 6 [2976/21600 (14%)]\tLoss: 0.009852\n",
            "Train Epoch: 6 [3072/21600 (14%)]\tLoss: 0.080863\n",
            "Train Epoch: 6 [3168/21600 (15%)]\tLoss: 0.001507\n",
            "Train Epoch: 6 [3264/21600 (15%)]\tLoss: 0.008506\n",
            "Train Epoch: 6 [3360/21600 (16%)]\tLoss: 0.005043\n",
            "Train Epoch: 6 [3456/21600 (16%)]\tLoss: 0.006175\n",
            "Train Epoch: 6 [3552/21600 (16%)]\tLoss: 0.013442\n",
            "Train Epoch: 6 [3648/21600 (17%)]\tLoss: 0.000927\n",
            "Train Epoch: 6 [3744/21600 (17%)]\tLoss: 0.108276\n",
            "Train Epoch: 6 [3840/21600 (18%)]\tLoss: 0.001152\n",
            "Train Epoch: 6 [3936/21600 (18%)]\tLoss: 0.003014\n",
            "Train Epoch: 6 [4032/21600 (19%)]\tLoss: 0.006297\n",
            "Train Epoch: 6 [4128/21600 (19%)]\tLoss: 0.004926\n",
            "Train Epoch: 6 [4224/21600 (20%)]\tLoss: 0.050841\n",
            "Train Epoch: 6 [4320/21600 (20%)]\tLoss: 0.014588\n",
            "Train Epoch: 6 [4416/21600 (20%)]\tLoss: 0.002115\n",
            "Train Epoch: 6 [4512/21600 (21%)]\tLoss: 0.001474\n",
            "Train Epoch: 6 [4608/21600 (21%)]\tLoss: 0.002687\n",
            "Train Epoch: 6 [4704/21600 (22%)]\tLoss: 0.005572\n",
            "Train Epoch: 6 [4800/21600 (22%)]\tLoss: 0.001614\n",
            "Train Epoch: 6 [4896/21600 (23%)]\tLoss: 0.076033\n",
            "Train Epoch: 6 [4992/21600 (23%)]\tLoss: 0.001114\n",
            "Train Epoch: 6 [5088/21600 (24%)]\tLoss: 0.000221\n",
            "Train Epoch: 6 [5184/21600 (24%)]\tLoss: 0.000122\n",
            "Train Epoch: 6 [5280/21600 (24%)]\tLoss: 0.006299\n",
            "Train Epoch: 6 [5376/21600 (25%)]\tLoss: 0.017291\n",
            "Train Epoch: 6 [5472/21600 (25%)]\tLoss: 0.004510\n",
            "Train Epoch: 6 [5568/21600 (26%)]\tLoss: 0.000719\n",
            "Train Epoch: 6 [5664/21600 (26%)]\tLoss: 0.000236\n",
            "Train Epoch: 6 [5760/21600 (27%)]\tLoss: 0.000560\n",
            "Train Epoch: 6 [5856/21600 (27%)]\tLoss: 0.104818\n",
            "Train Epoch: 6 [5952/21600 (28%)]\tLoss: 0.009476\n",
            "Train Epoch: 6 [6048/21600 (28%)]\tLoss: 0.001462\n",
            "Train Epoch: 6 [6144/21600 (28%)]\tLoss: 0.001149\n",
            "Train Epoch: 6 [6240/21600 (29%)]\tLoss: 0.011686\n",
            "Train Epoch: 6 [6336/21600 (29%)]\tLoss: 0.001207\n",
            "Train Epoch: 6 [6432/21600 (30%)]\tLoss: 0.004431\n",
            "Train Epoch: 6 [6528/21600 (30%)]\tLoss: 0.002768\n",
            "Train Epoch: 6 [6624/21600 (31%)]\tLoss: 0.026286\n",
            "Train Epoch: 6 [6720/21600 (31%)]\tLoss: 0.002208\n",
            "Train Epoch: 6 [6816/21600 (32%)]\tLoss: 0.000437\n",
            "Train Epoch: 6 [6912/21600 (32%)]\tLoss: 0.013434\n",
            "Train Epoch: 6 [7008/21600 (32%)]\tLoss: 0.003275\n",
            "Train Epoch: 6 [7104/21600 (33%)]\tLoss: 0.083202\n",
            "Train Epoch: 6 [7200/21600 (33%)]\tLoss: 0.001040\n",
            "Train Epoch: 6 [7296/21600 (34%)]\tLoss: 0.004489\n",
            "Train Epoch: 6 [7392/21600 (34%)]\tLoss: 0.061346\n",
            "Train Epoch: 6 [7488/21600 (35%)]\tLoss: 0.000740\n",
            "Train Epoch: 6 [7584/21600 (35%)]\tLoss: 0.009561\n",
            "Train Epoch: 6 [7680/21600 (36%)]\tLoss: 0.000731\n",
            "Train Epoch: 6 [7776/21600 (36%)]\tLoss: 0.002360\n",
            "Train Epoch: 6 [7872/21600 (36%)]\tLoss: 0.013676\n",
            "Train Epoch: 6 [7968/21600 (37%)]\tLoss: 0.003749\n",
            "Train Epoch: 6 [8064/21600 (37%)]\tLoss: 0.000473\n",
            "Train Epoch: 6 [8160/21600 (38%)]\tLoss: 0.231108\n",
            "Train Epoch: 6 [8256/21600 (38%)]\tLoss: 0.015340\n",
            "Train Epoch: 6 [8352/21600 (39%)]\tLoss: 0.003756\n",
            "Train Epoch: 6 [8448/21600 (39%)]\tLoss: 0.071571\n",
            "Train Epoch: 6 [8544/21600 (40%)]\tLoss: 0.000327\n",
            "Train Epoch: 6 [8640/21600 (40%)]\tLoss: 0.001344\n",
            "Train Epoch: 6 [8736/21600 (40%)]\tLoss: 0.043182\n",
            "Train Epoch: 6 [8832/21600 (41%)]\tLoss: 0.015227\n",
            "Train Epoch: 6 [8928/21600 (41%)]\tLoss: 0.001168\n",
            "Train Epoch: 6 [9024/21600 (42%)]\tLoss: 0.003456\n",
            "Train Epoch: 6 [9120/21600 (42%)]\tLoss: 0.012811\n",
            "Train Epoch: 6 [9216/21600 (43%)]\tLoss: 0.000226\n",
            "Train Epoch: 6 [9312/21600 (43%)]\tLoss: 0.000590\n",
            "Train Epoch: 6 [9408/21600 (44%)]\tLoss: 0.001994\n",
            "Train Epoch: 6 [9504/21600 (44%)]\tLoss: 0.001201\n",
            "Train Epoch: 6 [9600/21600 (44%)]\tLoss: 0.003753\n",
            "Train Epoch: 6 [9696/21600 (45%)]\tLoss: 0.003381\n",
            "Train Epoch: 6 [9792/21600 (45%)]\tLoss: 0.018631\n",
            "Train Epoch: 6 [9888/21600 (46%)]\tLoss: 0.002826\n",
            "Train Epoch: 6 [9984/21600 (46%)]\tLoss: 0.003372\n",
            "Train Epoch: 6 [10080/21600 (47%)]\tLoss: 0.005040\n",
            "Train Epoch: 6 [10176/21600 (47%)]\tLoss: 0.000309\n",
            "Train Epoch: 6 [10272/21600 (48%)]\tLoss: 0.006956\n",
            "Train Epoch: 6 [10368/21600 (48%)]\tLoss: 0.009622\n",
            "Train Epoch: 6 [10464/21600 (48%)]\tLoss: 0.067165\n",
            "Train Epoch: 6 [10560/21600 (49%)]\tLoss: 0.001226\n",
            "Train Epoch: 6 [10656/21600 (49%)]\tLoss: 0.000238\n",
            "Train Epoch: 6 [10752/21600 (50%)]\tLoss: 0.004406\n",
            "Train Epoch: 6 [10848/21600 (50%)]\tLoss: 0.003285\n",
            "Train Epoch: 6 [10944/21600 (51%)]\tLoss: 0.002446\n",
            "Train Epoch: 6 [11040/21600 (51%)]\tLoss: 0.010714\n",
            "Train Epoch: 6 [11136/21600 (52%)]\tLoss: 0.007791\n",
            "Train Epoch: 6 [11232/21600 (52%)]\tLoss: 0.011772\n",
            "Train Epoch: 6 [11328/21600 (52%)]\tLoss: 0.001842\n",
            "Train Epoch: 6 [11424/21600 (53%)]\tLoss: 0.036348\n",
            "Train Epoch: 6 [11520/21600 (53%)]\tLoss: 0.000925\n",
            "Train Epoch: 6 [11616/21600 (54%)]\tLoss: 0.001589\n",
            "Train Epoch: 6 [11712/21600 (54%)]\tLoss: 0.000364\n",
            "Train Epoch: 6 [11808/21600 (55%)]\tLoss: 0.039090\n",
            "Train Epoch: 6 [11904/21600 (55%)]\tLoss: 0.002166\n",
            "Train Epoch: 6 [12000/21600 (56%)]\tLoss: 0.000225\n",
            "Train Epoch: 6 [12096/21600 (56%)]\tLoss: 0.013166\n",
            "Train Epoch: 6 [12192/21600 (56%)]\tLoss: 0.006065\n",
            "Train Epoch: 6 [12288/21600 (57%)]\tLoss: 0.016556\n",
            "Train Epoch: 6 [12384/21600 (57%)]\tLoss: 0.002836\n",
            "Train Epoch: 6 [12480/21600 (58%)]\tLoss: 0.000278\n",
            "Train Epoch: 6 [12576/21600 (58%)]\tLoss: 0.000442\n",
            "Train Epoch: 6 [12672/21600 (59%)]\tLoss: 0.001317\n",
            "Train Epoch: 6 [12768/21600 (59%)]\tLoss: 0.045756\n",
            "Train Epoch: 6 [12864/21600 (60%)]\tLoss: 0.000233\n",
            "Train Epoch: 6 [12960/21600 (60%)]\tLoss: 0.018690\n",
            "Train Epoch: 6 [13056/21600 (60%)]\tLoss: 0.000218\n",
            "Train Epoch: 6 [13152/21600 (61%)]\tLoss: 0.004316\n",
            "Train Epoch: 6 [13248/21600 (61%)]\tLoss: 0.000796\n",
            "Train Epoch: 6 [13344/21600 (62%)]\tLoss: 0.000753\n",
            "Train Epoch: 6 [13440/21600 (62%)]\tLoss: 0.005070\n",
            "Train Epoch: 6 [13536/21600 (63%)]\tLoss: 0.001655\n",
            "Train Epoch: 6 [13632/21600 (63%)]\tLoss: 0.000754\n",
            "Train Epoch: 6 [13728/21600 (64%)]\tLoss: 0.000659\n",
            "Train Epoch: 6 [13824/21600 (64%)]\tLoss: 0.005122\n",
            "Train Epoch: 6 [13920/21600 (64%)]\tLoss: 0.000452\n",
            "Train Epoch: 6 [14016/21600 (65%)]\tLoss: 0.045571\n",
            "Train Epoch: 6 [14112/21600 (65%)]\tLoss: 0.013782\n",
            "Train Epoch: 6 [14208/21600 (66%)]\tLoss: 0.017222\n",
            "Train Epoch: 6 [14304/21600 (66%)]\tLoss: 0.000618\n",
            "Train Epoch: 6 [14400/21600 (67%)]\tLoss: 0.016256\n",
            "Train Epoch: 6 [14496/21600 (67%)]\tLoss: 0.000353\n",
            "Train Epoch: 6 [14592/21600 (68%)]\tLoss: 0.001282\n",
            "Train Epoch: 6 [14688/21600 (68%)]\tLoss: 0.000205\n",
            "Train Epoch: 6 [14784/21600 (68%)]\tLoss: 0.004323\n",
            "Train Epoch: 6 [14880/21600 (69%)]\tLoss: 0.002486\n",
            "Train Epoch: 6 [14976/21600 (69%)]\tLoss: 0.002212\n",
            "Train Epoch: 6 [15072/21600 (70%)]\tLoss: 0.000258\n",
            "Train Epoch: 6 [15168/21600 (70%)]\tLoss: 0.014148\n",
            "Train Epoch: 6 [15264/21600 (71%)]\tLoss: 0.001983\n",
            "Train Epoch: 6 [15360/21600 (71%)]\tLoss: 0.001235\n",
            "Train Epoch: 6 [15456/21600 (72%)]\tLoss: 0.002845\n",
            "Train Epoch: 6 [15552/21600 (72%)]\tLoss: 0.001792\n",
            "Train Epoch: 6 [15648/21600 (72%)]\tLoss: 0.028069\n",
            "Train Epoch: 6 [15744/21600 (73%)]\tLoss: 0.000983\n",
            "Train Epoch: 6 [15840/21600 (73%)]\tLoss: 0.007556\n",
            "Train Epoch: 6 [15936/21600 (74%)]\tLoss: 0.011350\n",
            "Train Epoch: 6 [16032/21600 (74%)]\tLoss: 0.015938\n",
            "Train Epoch: 6 [16128/21600 (75%)]\tLoss: 0.000873\n",
            "Train Epoch: 6 [16224/21600 (75%)]\tLoss: 0.005008\n",
            "Train Epoch: 6 [16320/21600 (76%)]\tLoss: 0.003571\n",
            "Train Epoch: 6 [16416/21600 (76%)]\tLoss: 0.000871\n",
            "Train Epoch: 6 [16512/21600 (76%)]\tLoss: 0.002472\n",
            "Train Epoch: 6 [16608/21600 (77%)]\tLoss: 0.003462\n",
            "Train Epoch: 6 [16704/21600 (77%)]\tLoss: 0.009615\n",
            "Train Epoch: 6 [16800/21600 (78%)]\tLoss: 0.000308\n",
            "Train Epoch: 6 [16896/21600 (78%)]\tLoss: 0.004332\n",
            "Train Epoch: 6 [16992/21600 (79%)]\tLoss: 0.003033\n",
            "Train Epoch: 6 [17088/21600 (79%)]\tLoss: 0.000236\n",
            "Train Epoch: 6 [17184/21600 (80%)]\tLoss: 0.012717\n",
            "Train Epoch: 6 [17280/21600 (80%)]\tLoss: 0.000385\n",
            "Train Epoch: 6 [17376/21600 (80%)]\tLoss: 0.007724\n",
            "Train Epoch: 6 [17472/21600 (81%)]\tLoss: 0.001506\n",
            "Train Epoch: 6 [17568/21600 (81%)]\tLoss: 0.000697\n",
            "Train Epoch: 6 [17664/21600 (82%)]\tLoss: 0.006117\n",
            "Train Epoch: 6 [17760/21600 (82%)]\tLoss: 0.000979\n",
            "Train Epoch: 6 [17856/21600 (83%)]\tLoss: 0.000457\n",
            "Train Epoch: 6 [17952/21600 (83%)]\tLoss: 0.000452\n",
            "Train Epoch: 6 [18048/21600 (84%)]\tLoss: 0.006118\n",
            "Train Epoch: 6 [18144/21600 (84%)]\tLoss: 0.002063\n",
            "Train Epoch: 6 [18240/21600 (84%)]\tLoss: 0.001892\n",
            "Train Epoch: 6 [18336/21600 (85%)]\tLoss: 0.003276\n",
            "Train Epoch: 6 [18432/21600 (85%)]\tLoss: 0.002134\n",
            "Train Epoch: 6 [18528/21600 (86%)]\tLoss: 0.005253\n",
            "Train Epoch: 6 [18624/21600 (86%)]\tLoss: 0.000156\n",
            "Train Epoch: 6 [18720/21600 (87%)]\tLoss: 0.017886\n",
            "Train Epoch: 6 [18816/21600 (87%)]\tLoss: 0.000122\n",
            "Train Epoch: 6 [18912/21600 (88%)]\tLoss: 0.000252\n",
            "Train Epoch: 6 [19008/21600 (88%)]\tLoss: 0.042013\n",
            "Train Epoch: 6 [19104/21600 (88%)]\tLoss: 0.038258\n",
            "Train Epoch: 6 [19200/21600 (89%)]\tLoss: 0.013076\n",
            "Train Epoch: 6 [19296/21600 (89%)]\tLoss: 0.000517\n",
            "Train Epoch: 6 [19392/21600 (90%)]\tLoss: 0.001826\n",
            "Train Epoch: 6 [19488/21600 (90%)]\tLoss: 0.000422\n",
            "Train Epoch: 6 [19584/21600 (91%)]\tLoss: 0.000061\n",
            "Train Epoch: 6 [19680/21600 (91%)]\tLoss: 0.003193\n",
            "Train Epoch: 6 [19776/21600 (92%)]\tLoss: 0.001927\n",
            "Train Epoch: 6 [19872/21600 (92%)]\tLoss: 0.000903\n",
            "Train Epoch: 6 [19968/21600 (92%)]\tLoss: 0.001128\n",
            "Train Epoch: 6 [20064/21600 (93%)]\tLoss: 0.017382\n",
            "Train Epoch: 6 [20160/21600 (93%)]\tLoss: 0.046098\n",
            "Train Epoch: 6 [20256/21600 (94%)]\tLoss: 0.000609\n",
            "Train Epoch: 6 [20352/21600 (94%)]\tLoss: 0.000964\n",
            "Train Epoch: 6 [20448/21600 (95%)]\tLoss: 0.021611\n",
            "Train Epoch: 6 [20544/21600 (95%)]\tLoss: 0.000448\n",
            "Train Epoch: 6 [20640/21600 (96%)]\tLoss: 0.007827\n",
            "Train Epoch: 6 [20736/21600 (96%)]\tLoss: 0.003420\n",
            "Train Epoch: 6 [20832/21600 (96%)]\tLoss: 0.000418\n",
            "Train Epoch: 6 [20928/21600 (97%)]\tLoss: 0.000757\n",
            "Train Epoch: 6 [21024/21600 (97%)]\tLoss: 0.004279\n",
            "Train Epoch: 6 [21120/21600 (98%)]\tLoss: 0.000399\n",
            "Train Epoch: 6 [21216/21600 (98%)]\tLoss: 0.004458\n",
            "Train Epoch: 6 [21312/21600 (99%)]\tLoss: 0.001448\n",
            "Train Epoch: 6 [21408/21600 (99%)]\tLoss: 0.001129\n",
            "Train Epoch: 6 [21504/21600 (100%)]\tLoss: 0.012672\n",
            "\n",
            "Validation set: Average loss: 0.0163, Accuracy: 5375/5400 (100%)\n",
            "\n",
            "\n",
            "Saved model to model_binary_6.pth.\n",
            "Train Epoch: 7 [0/21600 (0%)]\tLoss: 0.000424\n",
            "Train Epoch: 7 [96/21600 (0%)]\tLoss: 0.000199\n",
            "Train Epoch: 7 [192/21600 (1%)]\tLoss: 0.001293\n",
            "Train Epoch: 7 [288/21600 (1%)]\tLoss: 0.003110\n",
            "Train Epoch: 7 [384/21600 (2%)]\tLoss: 0.001616\n",
            "Train Epoch: 7 [480/21600 (2%)]\tLoss: 0.008605\n",
            "Train Epoch: 7 [576/21600 (3%)]\tLoss: 0.002035\n",
            "Train Epoch: 7 [672/21600 (3%)]\tLoss: 0.002451\n",
            "Train Epoch: 7 [768/21600 (4%)]\tLoss: 0.000434\n",
            "Train Epoch: 7 [864/21600 (4%)]\tLoss: 0.000272\n",
            "Train Epoch: 7 [960/21600 (4%)]\tLoss: 0.001363\n",
            "Train Epoch: 7 [1056/21600 (5%)]\tLoss: 0.000950\n",
            "Train Epoch: 7 [1152/21600 (5%)]\tLoss: 0.000093\n",
            "Train Epoch: 7 [1248/21600 (6%)]\tLoss: 0.036636\n",
            "Train Epoch: 7 [1344/21600 (6%)]\tLoss: 0.004357\n",
            "Train Epoch: 7 [1440/21600 (7%)]\tLoss: 0.000137\n",
            "Train Epoch: 7 [1536/21600 (7%)]\tLoss: 0.000027\n",
            "Train Epoch: 7 [1632/21600 (8%)]\tLoss: 0.000270\n",
            "Train Epoch: 7 [1728/21600 (8%)]\tLoss: 0.062420\n",
            "Train Epoch: 7 [1824/21600 (8%)]\tLoss: 0.000095\n",
            "Train Epoch: 7 [1920/21600 (9%)]\tLoss: 0.003618\n",
            "Train Epoch: 7 [2016/21600 (9%)]\tLoss: 0.001121\n",
            "Train Epoch: 7 [2112/21600 (10%)]\tLoss: 0.000650\n",
            "Train Epoch: 7 [2208/21600 (10%)]\tLoss: 0.000160\n",
            "Train Epoch: 7 [2304/21600 (11%)]\tLoss: 0.000433\n",
            "Train Epoch: 7 [2400/21600 (11%)]\tLoss: 0.000202\n",
            "Train Epoch: 7 [2496/21600 (12%)]\tLoss: 0.000472\n",
            "Train Epoch: 7 [2592/21600 (12%)]\tLoss: 0.000498\n",
            "Train Epoch: 7 [2688/21600 (12%)]\tLoss: 0.000530\n",
            "Train Epoch: 7 [2784/21600 (13%)]\tLoss: 0.004172\n",
            "Train Epoch: 7 [2880/21600 (13%)]\tLoss: 0.000992\n",
            "Train Epoch: 7 [2976/21600 (14%)]\tLoss: 0.000279\n",
            "Train Epoch: 7 [3072/21600 (14%)]\tLoss: 0.000166\n",
            "Train Epoch: 7 [3168/21600 (15%)]\tLoss: 0.000254\n",
            "Train Epoch: 7 [3264/21600 (15%)]\tLoss: 0.000381\n",
            "Train Epoch: 7 [3360/21600 (16%)]\tLoss: 0.032792\n",
            "Train Epoch: 7 [3456/21600 (16%)]\tLoss: 0.000253\n",
            "Train Epoch: 7 [3552/21600 (16%)]\tLoss: 0.000025\n",
            "Train Epoch: 7 [3648/21600 (17%)]\tLoss: 0.000611\n",
            "Train Epoch: 7 [3744/21600 (17%)]\tLoss: 0.000120\n",
            "Train Epoch: 7 [3840/21600 (18%)]\tLoss: 0.000144\n",
            "Train Epoch: 7 [3936/21600 (18%)]\tLoss: 0.047001\n",
            "Train Epoch: 7 [4032/21600 (19%)]\tLoss: 0.000480\n",
            "Train Epoch: 7 [4128/21600 (19%)]\tLoss: 0.010650\n",
            "Train Epoch: 7 [4224/21600 (20%)]\tLoss: 0.002668\n",
            "Train Epoch: 7 [4320/21600 (20%)]\tLoss: 0.000965\n",
            "Train Epoch: 7 [4416/21600 (20%)]\tLoss: 0.000096\n",
            "Train Epoch: 7 [4512/21600 (21%)]\tLoss: 0.005982\n",
            "Train Epoch: 7 [4608/21600 (21%)]\tLoss: 0.000461\n",
            "Train Epoch: 7 [4704/21600 (22%)]\tLoss: 0.040041\n",
            "Train Epoch: 7 [4800/21600 (22%)]\tLoss: 0.003782\n",
            "Train Epoch: 7 [4896/21600 (23%)]\tLoss: 0.001191\n",
            "Train Epoch: 7 [4992/21600 (23%)]\tLoss: 0.001752\n",
            "Train Epoch: 7 [5088/21600 (24%)]\tLoss: 0.000374\n",
            "Train Epoch: 7 [5184/21600 (24%)]\tLoss: 0.062846\n",
            "Train Epoch: 7 [5280/21600 (24%)]\tLoss: 0.000534\n",
            "Train Epoch: 7 [5376/21600 (25%)]\tLoss: 0.000424\n",
            "Train Epoch: 7 [5472/21600 (25%)]\tLoss: 0.000298\n",
            "Train Epoch: 7 [5568/21600 (26%)]\tLoss: 0.000297\n",
            "Train Epoch: 7 [5664/21600 (26%)]\tLoss: 0.000078\n",
            "Train Epoch: 7 [5760/21600 (27%)]\tLoss: 0.000669\n",
            "Train Epoch: 7 [5856/21600 (27%)]\tLoss: 0.000374\n",
            "Train Epoch: 7 [5952/21600 (28%)]\tLoss: 0.007570\n",
            "Train Epoch: 7 [6048/21600 (28%)]\tLoss: 0.000145\n",
            "Train Epoch: 7 [6144/21600 (28%)]\tLoss: 0.000075\n",
            "Train Epoch: 7 [6240/21600 (29%)]\tLoss: 0.059258\n",
            "Train Epoch: 7 [6336/21600 (29%)]\tLoss: 0.010892\n",
            "Train Epoch: 7 [6432/21600 (30%)]\tLoss: 0.000054\n",
            "Train Epoch: 7 [6528/21600 (30%)]\tLoss: 0.000209\n",
            "Train Epoch: 7 [6624/21600 (31%)]\tLoss: 0.002977\n",
            "Train Epoch: 7 [6720/21600 (31%)]\tLoss: 0.001639\n",
            "Train Epoch: 7 [6816/21600 (32%)]\tLoss: 0.000314\n",
            "Train Epoch: 7 [6912/21600 (32%)]\tLoss: 0.000433\n",
            "Train Epoch: 7 [7008/21600 (32%)]\tLoss: 0.000766\n",
            "Train Epoch: 7 [7104/21600 (33%)]\tLoss: 0.001810\n",
            "Train Epoch: 7 [7200/21600 (33%)]\tLoss: 0.023495\n",
            "Train Epoch: 7 [7296/21600 (34%)]\tLoss: 0.000454\n",
            "Train Epoch: 7 [7392/21600 (34%)]\tLoss: 0.007245\n",
            "Train Epoch: 7 [7488/21600 (35%)]\tLoss: 0.008917\n",
            "Train Epoch: 7 [7584/21600 (35%)]\tLoss: 0.040564\n",
            "Train Epoch: 7 [7680/21600 (36%)]\tLoss: 0.006115\n",
            "Train Epoch: 7 [7776/21600 (36%)]\tLoss: 0.111691\n",
            "Train Epoch: 7 [7872/21600 (36%)]\tLoss: 0.000614\n",
            "Train Epoch: 7 [7968/21600 (37%)]\tLoss: 0.000305\n",
            "Train Epoch: 7 [8064/21600 (37%)]\tLoss: 0.000608\n",
            "Train Epoch: 7 [8160/21600 (38%)]\tLoss: 0.000856\n",
            "Train Epoch: 7 [8256/21600 (38%)]\tLoss: 0.002058\n",
            "Train Epoch: 7 [8352/21600 (39%)]\tLoss: 0.019757\n",
            "Train Epoch: 7 [8448/21600 (39%)]\tLoss: 0.018590\n",
            "Train Epoch: 7 [8544/21600 (40%)]\tLoss: 0.001259\n",
            "Train Epoch: 7 [8640/21600 (40%)]\tLoss: 0.008279\n",
            "Train Epoch: 7 [8736/21600 (40%)]\tLoss: 0.092569\n",
            "Train Epoch: 7 [8832/21600 (41%)]\tLoss: 0.022122\n",
            "Train Epoch: 7 [8928/21600 (41%)]\tLoss: 0.009271\n",
            "Train Epoch: 7 [9024/21600 (42%)]\tLoss: 0.000145\n",
            "Train Epoch: 7 [9120/21600 (42%)]\tLoss: 0.008080\n",
            "Train Epoch: 7 [9216/21600 (43%)]\tLoss: 0.008562\n",
            "Train Epoch: 7 [9312/21600 (43%)]\tLoss: 0.009156\n",
            "Train Epoch: 7 [9408/21600 (44%)]\tLoss: 0.000182\n",
            "Train Epoch: 7 [9504/21600 (44%)]\tLoss: 0.065777\n",
            "Train Epoch: 7 [9600/21600 (44%)]\tLoss: 0.000509\n",
            "Train Epoch: 7 [9696/21600 (45%)]\tLoss: 0.000112\n",
            "Train Epoch: 7 [9792/21600 (45%)]\tLoss: 0.006083\n",
            "Train Epoch: 7 [9888/21600 (46%)]\tLoss: 0.000226\n",
            "Train Epoch: 7 [9984/21600 (46%)]\tLoss: 0.000533\n",
            "Train Epoch: 7 [10080/21600 (47%)]\tLoss: 0.009072\n",
            "Train Epoch: 7 [10176/21600 (47%)]\tLoss: 0.009856\n",
            "Train Epoch: 7 [10272/21600 (48%)]\tLoss: 0.004874\n",
            "Train Epoch: 7 [10368/21600 (48%)]\tLoss: 0.098325\n",
            "Train Epoch: 7 [10464/21600 (48%)]\tLoss: 0.185486\n",
            "Train Epoch: 7 [10560/21600 (49%)]\tLoss: 0.001621\n",
            "Train Epoch: 7 [10656/21600 (49%)]\tLoss: 0.003768\n",
            "Train Epoch: 7 [10752/21600 (50%)]\tLoss: 0.008622\n",
            "Train Epoch: 7 [10848/21600 (50%)]\tLoss: 0.000582\n",
            "Train Epoch: 7 [10944/21600 (51%)]\tLoss: 0.000199\n",
            "Train Epoch: 7 [11040/21600 (51%)]\tLoss: 0.002390\n",
            "Train Epoch: 7 [11136/21600 (52%)]\tLoss: 0.029129\n",
            "Train Epoch: 7 [11232/21600 (52%)]\tLoss: 0.067932\n",
            "Train Epoch: 7 [11328/21600 (52%)]\tLoss: 0.000981\n",
            "Train Epoch: 7 [11424/21600 (53%)]\tLoss: 0.000862\n",
            "Train Epoch: 7 [11520/21600 (53%)]\tLoss: 0.000216\n",
            "Train Epoch: 7 [11616/21600 (54%)]\tLoss: 0.002939\n",
            "Train Epoch: 7 [11712/21600 (54%)]\tLoss: 0.000388\n",
            "Train Epoch: 7 [11808/21600 (55%)]\tLoss: 0.000073\n",
            "Train Epoch: 7 [11904/21600 (55%)]\tLoss: 0.054912\n",
            "Train Epoch: 7 [12000/21600 (56%)]\tLoss: 0.001697\n",
            "Train Epoch: 7 [12096/21600 (56%)]\tLoss: 0.003484\n",
            "Train Epoch: 7 [12192/21600 (56%)]\tLoss: 0.000129\n",
            "Train Epoch: 7 [12288/21600 (57%)]\tLoss: 0.000671\n",
            "Train Epoch: 7 [12384/21600 (57%)]\tLoss: 0.000146\n",
            "Train Epoch: 7 [12480/21600 (58%)]\tLoss: 0.005219\n",
            "Train Epoch: 7 [12576/21600 (58%)]\tLoss: 0.000202\n",
            "Train Epoch: 7 [12672/21600 (59%)]\tLoss: 0.001192\n",
            "Train Epoch: 7 [12768/21600 (59%)]\tLoss: 0.000195\n",
            "Train Epoch: 7 [12864/21600 (60%)]\tLoss: 0.007279\n",
            "Train Epoch: 7 [12960/21600 (60%)]\tLoss: 0.043885\n",
            "Train Epoch: 7 [13056/21600 (60%)]\tLoss: 0.001799\n",
            "Train Epoch: 7 [13152/21600 (61%)]\tLoss: 0.000214\n",
            "Train Epoch: 7 [13248/21600 (61%)]\tLoss: 0.001456\n",
            "Train Epoch: 7 [13344/21600 (62%)]\tLoss: 0.001634\n",
            "Train Epoch: 7 [13440/21600 (62%)]\tLoss: 0.003198\n",
            "Train Epoch: 7 [13536/21600 (63%)]\tLoss: 0.036609\n",
            "Train Epoch: 7 [13632/21600 (63%)]\tLoss: 0.006040\n",
            "Train Epoch: 7 [13728/21600 (64%)]\tLoss: 0.002308\n",
            "Train Epoch: 7 [13824/21600 (64%)]\tLoss: 0.011591\n",
            "Train Epoch: 7 [13920/21600 (64%)]\tLoss: 0.000924\n",
            "Train Epoch: 7 [14016/21600 (65%)]\tLoss: 0.000842\n",
            "Train Epoch: 7 [14112/21600 (65%)]\tLoss: 0.000150\n",
            "Train Epoch: 7 [14208/21600 (66%)]\tLoss: 0.000692\n",
            "Train Epoch: 7 [14304/21600 (66%)]\tLoss: 0.000208\n",
            "Train Epoch: 7 [14400/21600 (67%)]\tLoss: 0.000957\n",
            "Train Epoch: 7 [14496/21600 (67%)]\tLoss: 0.000526\n",
            "Train Epoch: 7 [14592/21600 (68%)]\tLoss: 0.013910\n",
            "Train Epoch: 7 [14688/21600 (68%)]\tLoss: 0.004379\n",
            "Train Epoch: 7 [14784/21600 (68%)]\tLoss: 0.001142\n",
            "Train Epoch: 7 [14880/21600 (69%)]\tLoss: 0.000081\n",
            "Train Epoch: 7 [14976/21600 (69%)]\tLoss: 0.001840\n",
            "Train Epoch: 7 [15072/21600 (70%)]\tLoss: 0.002078\n",
            "Train Epoch: 7 [15168/21600 (70%)]\tLoss: 0.011320\n",
            "Train Epoch: 7 [15264/21600 (71%)]\tLoss: 0.002076\n",
            "Train Epoch: 7 [15360/21600 (71%)]\tLoss: 0.066267\n",
            "Train Epoch: 7 [15456/21600 (72%)]\tLoss: 0.001739\n",
            "Train Epoch: 7 [15552/21600 (72%)]\tLoss: 0.001849\n",
            "Train Epoch: 7 [15648/21600 (72%)]\tLoss: 0.000046\n",
            "Train Epoch: 7 [15744/21600 (73%)]\tLoss: 0.000245\n",
            "Train Epoch: 7 [15840/21600 (73%)]\tLoss: 0.002221\n",
            "Train Epoch: 7 [15936/21600 (74%)]\tLoss: 0.000840\n",
            "Train Epoch: 7 [16032/21600 (74%)]\tLoss: 0.000376\n",
            "Train Epoch: 7 [16128/21600 (75%)]\tLoss: 0.072402\n",
            "Train Epoch: 7 [16224/21600 (75%)]\tLoss: 0.003887\n",
            "Train Epoch: 7 [16320/21600 (76%)]\tLoss: 0.015181\n",
            "Train Epoch: 7 [16416/21600 (76%)]\tLoss: 0.004588\n",
            "Train Epoch: 7 [16512/21600 (76%)]\tLoss: 0.000782\n",
            "Train Epoch: 7 [16608/21600 (77%)]\tLoss: 0.000217\n",
            "Train Epoch: 7 [16704/21600 (77%)]\tLoss: 0.008517\n",
            "Train Epoch: 7 [16800/21600 (78%)]\tLoss: 0.002511\n",
            "Train Epoch: 7 [16896/21600 (78%)]\tLoss: 0.001126\n",
            "Train Epoch: 7 [16992/21600 (79%)]\tLoss: 0.028666\n",
            "Train Epoch: 7 [17088/21600 (79%)]\tLoss: 0.001886\n",
            "Train Epoch: 7 [17184/21600 (80%)]\tLoss: 0.000799\n",
            "Train Epoch: 7 [17280/21600 (80%)]\tLoss: 0.000387\n",
            "Train Epoch: 7 [17376/21600 (80%)]\tLoss: 0.089581\n",
            "Train Epoch: 7 [17472/21600 (81%)]\tLoss: 0.004257\n",
            "Train Epoch: 7 [17568/21600 (81%)]\tLoss: 0.001012\n",
            "Train Epoch: 7 [17664/21600 (82%)]\tLoss: 0.111792\n",
            "Train Epoch: 7 [17760/21600 (82%)]\tLoss: 0.047357\n",
            "Train Epoch: 7 [17856/21600 (83%)]\tLoss: 0.003793\n",
            "Train Epoch: 7 [17952/21600 (83%)]\tLoss: 0.000265\n",
            "Train Epoch: 7 [18048/21600 (84%)]\tLoss: 0.002936\n",
            "Train Epoch: 7 [18144/21600 (84%)]\tLoss: 0.000809\n",
            "Train Epoch: 7 [18240/21600 (84%)]\tLoss: 0.008106\n",
            "Train Epoch: 7 [18336/21600 (85%)]\tLoss: 0.005472\n",
            "Train Epoch: 7 [18432/21600 (85%)]\tLoss: 0.000179\n",
            "Train Epoch: 7 [18528/21600 (86%)]\tLoss: 0.008363\n",
            "Train Epoch: 7 [18624/21600 (86%)]\tLoss: 0.001934\n",
            "Train Epoch: 7 [18720/21600 (87%)]\tLoss: 0.000422\n",
            "Train Epoch: 7 [18816/21600 (87%)]\tLoss: 0.000740\n",
            "Train Epoch: 7 [18912/21600 (88%)]\tLoss: 0.000081\n",
            "Train Epoch: 7 [19008/21600 (88%)]\tLoss: 0.000178\n",
            "Train Epoch: 7 [19104/21600 (88%)]\tLoss: 0.000990\n",
            "Train Epoch: 7 [19200/21600 (89%)]\tLoss: 0.001163\n",
            "Train Epoch: 7 [19296/21600 (89%)]\tLoss: 0.004789\n",
            "Train Epoch: 7 [19392/21600 (90%)]\tLoss: 0.000548\n",
            "Train Epoch: 7 [19488/21600 (90%)]\tLoss: 0.023679\n",
            "Train Epoch: 7 [19584/21600 (91%)]\tLoss: 0.007691\n",
            "Train Epoch: 7 [19680/21600 (91%)]\tLoss: 0.002259\n",
            "Train Epoch: 7 [19776/21600 (92%)]\tLoss: 0.000383\n",
            "Train Epoch: 7 [19872/21600 (92%)]\tLoss: 0.002462\n",
            "Train Epoch: 7 [19968/21600 (92%)]\tLoss: 0.002422\n",
            "Train Epoch: 7 [20064/21600 (93%)]\tLoss: 0.022891\n",
            "Train Epoch: 7 [20160/21600 (93%)]\tLoss: 0.000426\n",
            "Train Epoch: 7 [20256/21600 (94%)]\tLoss: 0.000707\n",
            "Train Epoch: 7 [20352/21600 (94%)]\tLoss: 0.000356\n",
            "Train Epoch: 7 [20448/21600 (95%)]\tLoss: 0.000380\n",
            "Train Epoch: 7 [20544/21600 (95%)]\tLoss: 0.014482\n",
            "Train Epoch: 7 [20640/21600 (96%)]\tLoss: 0.014334\n",
            "Train Epoch: 7 [20736/21600 (96%)]\tLoss: 0.000091\n",
            "Train Epoch: 7 [20832/21600 (96%)]\tLoss: 0.000107\n",
            "Train Epoch: 7 [20928/21600 (97%)]\tLoss: 0.009980\n",
            "Train Epoch: 7 [21024/21600 (97%)]\tLoss: 0.034406\n",
            "Train Epoch: 7 [21120/21600 (98%)]\tLoss: 0.000615\n",
            "Train Epoch: 7 [21216/21600 (98%)]\tLoss: 0.001065\n",
            "Train Epoch: 7 [21312/21600 (99%)]\tLoss: 0.001453\n",
            "Train Epoch: 7 [21408/21600 (99%)]\tLoss: 0.000092\n",
            "Train Epoch: 7 [21504/21600 (100%)]\tLoss: 0.008675\n",
            "\n",
            "Validation set: Average loss: 0.0003, Accuracy: 5400/5400 (100%)\n",
            "\n",
            "\n",
            "Saved model to model_binary_7.pth.\n",
            "Train Epoch: 8 [0/21600 (0%)]\tLoss: 0.063528\n",
            "Train Epoch: 8 [96/21600 (0%)]\tLoss: 0.004095\n",
            "Train Epoch: 8 [192/21600 (1%)]\tLoss: 0.000049\n",
            "Train Epoch: 8 [288/21600 (1%)]\tLoss: 0.007283\n",
            "Train Epoch: 8 [384/21600 (2%)]\tLoss: 0.084647\n",
            "Train Epoch: 8 [480/21600 (2%)]\tLoss: 0.000470\n",
            "Train Epoch: 8 [576/21600 (3%)]\tLoss: 0.036192\n",
            "Train Epoch: 8 [672/21600 (3%)]\tLoss: 0.000632\n",
            "Train Epoch: 8 [768/21600 (4%)]\tLoss: 0.002052\n",
            "Train Epoch: 8 [864/21600 (4%)]\tLoss: 0.017920\n",
            "Train Epoch: 8 [960/21600 (4%)]\tLoss: 0.000354\n",
            "Train Epoch: 8 [1056/21600 (5%)]\tLoss: 0.000427\n",
            "Train Epoch: 8 [1152/21600 (5%)]\tLoss: 0.002681\n",
            "Train Epoch: 8 [1248/21600 (6%)]\tLoss: 0.000197\n",
            "Train Epoch: 8 [1344/21600 (6%)]\tLoss: 0.015094\n",
            "Train Epoch: 8 [1440/21600 (7%)]\tLoss: 0.000099\n",
            "Train Epoch: 8 [1536/21600 (7%)]\tLoss: 0.000305\n",
            "Train Epoch: 8 [1632/21600 (8%)]\tLoss: 0.003777\n",
            "Train Epoch: 8 [1728/21600 (8%)]\tLoss: 0.000823\n",
            "Train Epoch: 8 [1824/21600 (8%)]\tLoss: 0.001572\n",
            "Train Epoch: 8 [1920/21600 (9%)]\tLoss: 0.001327\n",
            "Train Epoch: 8 [2016/21600 (9%)]\tLoss: 0.003457\n",
            "Train Epoch: 8 [2112/21600 (10%)]\tLoss: 0.001978\n",
            "Train Epoch: 8 [2208/21600 (10%)]\tLoss: 0.001046\n",
            "Train Epoch: 8 [2304/21600 (11%)]\tLoss: 0.006420\n",
            "Train Epoch: 8 [2400/21600 (11%)]\tLoss: 0.013048\n",
            "Train Epoch: 8 [2496/21600 (12%)]\tLoss: 0.000229\n",
            "Train Epoch: 8 [2592/21600 (12%)]\tLoss: 0.006278\n",
            "Train Epoch: 8 [2688/21600 (12%)]\tLoss: 0.008670\n",
            "Train Epoch: 8 [2784/21600 (13%)]\tLoss: 0.003469\n",
            "Train Epoch: 8 [2880/21600 (13%)]\tLoss: 0.025697\n",
            "Train Epoch: 8 [2976/21600 (14%)]\tLoss: 0.000697\n",
            "Train Epoch: 8 [3072/21600 (14%)]\tLoss: 0.001684\n",
            "Train Epoch: 8 [3168/21600 (15%)]\tLoss: 0.000266\n",
            "Train Epoch: 8 [3264/21600 (15%)]\tLoss: 0.002260\n",
            "Train Epoch: 8 [3360/21600 (16%)]\tLoss: 0.000408\n",
            "Train Epoch: 8 [3456/21600 (16%)]\tLoss: 0.000655\n",
            "Train Epoch: 8 [3552/21600 (16%)]\tLoss: 0.029522\n",
            "Train Epoch: 8 [3648/21600 (17%)]\tLoss: 0.050386\n",
            "Train Epoch: 8 [3744/21600 (17%)]\tLoss: 0.002015\n",
            "Train Epoch: 8 [3840/21600 (18%)]\tLoss: 0.000958\n",
            "Train Epoch: 8 [3936/21600 (18%)]\tLoss: 0.001841\n",
            "Train Epoch: 8 [4032/21600 (19%)]\tLoss: 0.010910\n",
            "Train Epoch: 8 [4128/21600 (19%)]\tLoss: 0.000258\n",
            "Train Epoch: 8 [4224/21600 (20%)]\tLoss: 0.000201\n",
            "Train Epoch: 8 [4320/21600 (20%)]\tLoss: 0.003588\n",
            "Train Epoch: 8 [4416/21600 (20%)]\tLoss: 0.000318\n",
            "Train Epoch: 8 [4512/21600 (21%)]\tLoss: 0.000240\n",
            "Train Epoch: 8 [4608/21600 (21%)]\tLoss: 0.018338\n",
            "Train Epoch: 8 [4704/21600 (22%)]\tLoss: 0.000119\n",
            "Train Epoch: 8 [4800/21600 (22%)]\tLoss: 0.063031\n",
            "Train Epoch: 8 [4896/21600 (23%)]\tLoss: 0.004793\n",
            "Train Epoch: 8 [4992/21600 (23%)]\tLoss: 0.000145\n",
            "Train Epoch: 8 [5088/21600 (24%)]\tLoss: 0.000881\n",
            "Train Epoch: 8 [5184/21600 (24%)]\tLoss: 0.002009\n",
            "Train Epoch: 8 [5280/21600 (24%)]\tLoss: 0.009771\n",
            "Train Epoch: 8 [5376/21600 (25%)]\tLoss: 0.035006\n",
            "Train Epoch: 8 [5472/21600 (25%)]\tLoss: 0.000606\n",
            "Train Epoch: 8 [5568/21600 (26%)]\tLoss: 0.002257\n",
            "Train Epoch: 8 [5664/21600 (26%)]\tLoss: 0.000777\n",
            "Train Epoch: 8 [5760/21600 (27%)]\tLoss: 0.000674\n",
            "Train Epoch: 8 [5856/21600 (27%)]\tLoss: 0.003001\n",
            "Train Epoch: 8 [5952/21600 (28%)]\tLoss: 0.062664\n",
            "Train Epoch: 8 [6048/21600 (28%)]\tLoss: 0.000533\n",
            "Train Epoch: 8 [6144/21600 (28%)]\tLoss: 0.000198\n",
            "Train Epoch: 8 [6240/21600 (29%)]\tLoss: 0.001790\n",
            "Train Epoch: 8 [6336/21600 (29%)]\tLoss: 0.000503\n",
            "Train Epoch: 8 [6432/21600 (30%)]\tLoss: 0.000085\n",
            "Train Epoch: 8 [6528/21600 (30%)]\tLoss: 0.001004\n",
            "Train Epoch: 8 [6624/21600 (31%)]\tLoss: 0.034086\n",
            "Train Epoch: 8 [6720/21600 (31%)]\tLoss: 0.000758\n",
            "Train Epoch: 8 [6816/21600 (32%)]\tLoss: 0.001831\n",
            "Train Epoch: 8 [6912/21600 (32%)]\tLoss: 0.000127\n",
            "Train Epoch: 8 [7008/21600 (32%)]\tLoss: 0.000965\n",
            "Train Epoch: 8 [7104/21600 (33%)]\tLoss: 0.006991\n",
            "Train Epoch: 8 [7200/21600 (33%)]\tLoss: 0.000393\n",
            "Train Epoch: 8 [7296/21600 (34%)]\tLoss: 0.000223\n",
            "Train Epoch: 8 [7392/21600 (34%)]\tLoss: 0.000375\n",
            "Train Epoch: 8 [7488/21600 (35%)]\tLoss: 0.002304\n",
            "Train Epoch: 8 [7584/21600 (35%)]\tLoss: 0.015122\n",
            "Train Epoch: 8 [7680/21600 (36%)]\tLoss: 0.018319\n",
            "Train Epoch: 8 [7776/21600 (36%)]\tLoss: 0.000340\n",
            "Train Epoch: 8 [7872/21600 (36%)]\tLoss: 0.000821\n",
            "Train Epoch: 8 [7968/21600 (37%)]\tLoss: 0.000244\n",
            "Train Epoch: 8 [8064/21600 (37%)]\tLoss: 0.012456\n",
            "Train Epoch: 8 [8160/21600 (38%)]\tLoss: 0.023545\n",
            "Train Epoch: 8 [8256/21600 (38%)]\tLoss: 0.001051\n",
            "Train Epoch: 8 [8352/21600 (39%)]\tLoss: 0.003481\n",
            "Train Epoch: 8 [8448/21600 (39%)]\tLoss: 0.000096\n",
            "Train Epoch: 8 [8544/21600 (40%)]\tLoss: 0.036685\n",
            "Train Epoch: 8 [8640/21600 (40%)]\tLoss: 0.041052\n",
            "Train Epoch: 8 [8736/21600 (40%)]\tLoss: 0.000688\n",
            "Train Epoch: 8 [8832/21600 (41%)]\tLoss: 0.003455\n",
            "Train Epoch: 8 [8928/21600 (41%)]\tLoss: 0.000687\n",
            "Train Epoch: 8 [9024/21600 (42%)]\tLoss: 0.009678\n",
            "Train Epoch: 8 [9120/21600 (42%)]\tLoss: 0.000720\n",
            "Train Epoch: 8 [9216/21600 (43%)]\tLoss: 0.000379\n",
            "Train Epoch: 8 [9312/21600 (43%)]\tLoss: 0.003971\n",
            "Train Epoch: 8 [9408/21600 (44%)]\tLoss: 0.002552\n",
            "Train Epoch: 8 [9504/21600 (44%)]\tLoss: 0.000809\n",
            "Train Epoch: 8 [9600/21600 (44%)]\tLoss: 0.034633\n",
            "Train Epoch: 8 [9696/21600 (45%)]\tLoss: 0.000377\n",
            "Train Epoch: 8 [9792/21600 (45%)]\tLoss: 0.002367\n",
            "Train Epoch: 8 [9888/21600 (46%)]\tLoss: 0.044869\n",
            "Train Epoch: 8 [9984/21600 (46%)]\tLoss: 0.000525\n",
            "Train Epoch: 8 [10080/21600 (47%)]\tLoss: 0.000082\n",
            "Train Epoch: 8 [10176/21600 (47%)]\tLoss: 0.002082\n",
            "Train Epoch: 8 [10272/21600 (48%)]\tLoss: 0.001000\n",
            "Train Epoch: 8 [10368/21600 (48%)]\tLoss: 0.000206\n",
            "Train Epoch: 8 [10464/21600 (48%)]\tLoss: 0.007153\n",
            "Train Epoch: 8 [10560/21600 (49%)]\tLoss: 0.024137\n",
            "Train Epoch: 8 [10656/21600 (49%)]\tLoss: 0.007403\n",
            "Train Epoch: 8 [10752/21600 (50%)]\tLoss: 0.000728\n",
            "Train Epoch: 8 [10848/21600 (50%)]\tLoss: 0.000246\n",
            "Train Epoch: 8 [10944/21600 (51%)]\tLoss: 0.000704\n",
            "Train Epoch: 8 [11040/21600 (51%)]\tLoss: 0.002229\n",
            "Train Epoch: 8 [11136/21600 (52%)]\tLoss: 0.003111\n",
            "Train Epoch: 8 [11232/21600 (52%)]\tLoss: 0.019224\n",
            "Train Epoch: 8 [11328/21600 (52%)]\tLoss: 0.000663\n",
            "Train Epoch: 8 [11424/21600 (53%)]\tLoss: 0.000406\n",
            "Train Epoch: 8 [11520/21600 (53%)]\tLoss: 0.099281\n",
            "Train Epoch: 8 [11616/21600 (54%)]\tLoss: 0.046568\n",
            "Train Epoch: 8 [11712/21600 (54%)]\tLoss: 0.000458\n",
            "Train Epoch: 8 [11808/21600 (55%)]\tLoss: 0.004824\n",
            "Train Epoch: 8 [11904/21600 (55%)]\tLoss: 0.000565\n",
            "Train Epoch: 8 [12000/21600 (56%)]\tLoss: 0.000175\n",
            "Train Epoch: 8 [12096/21600 (56%)]\tLoss: 0.111753\n",
            "Train Epoch: 8 [12192/21600 (56%)]\tLoss: 0.000647\n",
            "Train Epoch: 8 [12288/21600 (57%)]\tLoss: 0.000964\n",
            "Train Epoch: 8 [12384/21600 (57%)]\tLoss: 0.040645\n",
            "Train Epoch: 8 [12480/21600 (58%)]\tLoss: 0.002874\n",
            "Train Epoch: 8 [12576/21600 (58%)]\tLoss: 0.010864\n",
            "Train Epoch: 8 [12672/21600 (59%)]\tLoss: 0.157975\n",
            "Train Epoch: 8 [12768/21600 (59%)]\tLoss: 0.000806\n",
            "Train Epoch: 8 [12864/21600 (60%)]\tLoss: 0.009707\n",
            "Train Epoch: 8 [12960/21600 (60%)]\tLoss: 0.006004\n",
            "Train Epoch: 8 [13056/21600 (60%)]\tLoss: 0.005182\n",
            "Train Epoch: 8 [13152/21600 (61%)]\tLoss: 0.002769\n",
            "Train Epoch: 8 [13248/21600 (61%)]\tLoss: 0.006572\n",
            "Train Epoch: 8 [13344/21600 (62%)]\tLoss: 0.021521\n",
            "Train Epoch: 8 [13440/21600 (62%)]\tLoss: 0.002605\n",
            "Train Epoch: 8 [13536/21600 (63%)]\tLoss: 0.000440\n",
            "Train Epoch: 8 [13632/21600 (63%)]\tLoss: 0.000043\n",
            "Train Epoch: 8 [13728/21600 (64%)]\tLoss: 0.001224\n",
            "Train Epoch: 8 [13824/21600 (64%)]\tLoss: 0.006071\n",
            "Train Epoch: 8 [13920/21600 (64%)]\tLoss: 0.000820\n",
            "Train Epoch: 8 [14016/21600 (65%)]\tLoss: 0.000170\n",
            "Train Epoch: 8 [14112/21600 (65%)]\tLoss: 0.001112\n",
            "Train Epoch: 8 [14208/21600 (66%)]\tLoss: 0.000114\n",
            "Train Epoch: 8 [14304/21600 (66%)]\tLoss: 0.003885\n",
            "Train Epoch: 8 [14400/21600 (67%)]\tLoss: 0.000341\n",
            "Train Epoch: 8 [14496/21600 (67%)]\tLoss: 0.000159\n",
            "Train Epoch: 8 [14592/21600 (68%)]\tLoss: 0.000051\n",
            "Train Epoch: 8 [14688/21600 (68%)]\tLoss: 0.053116\n",
            "Train Epoch: 8 [14784/21600 (68%)]\tLoss: 0.001133\n",
            "Train Epoch: 8 [14880/21600 (69%)]\tLoss: 0.003027\n",
            "Train Epoch: 8 [14976/21600 (69%)]\tLoss: 0.015192\n",
            "Train Epoch: 8 [15072/21600 (70%)]\tLoss: 0.000738\n",
            "Train Epoch: 8 [15168/21600 (70%)]\tLoss: 0.001890\n",
            "Train Epoch: 8 [15264/21600 (71%)]\tLoss: 0.000405\n",
            "Train Epoch: 8 [15360/21600 (71%)]\tLoss: 0.000292\n",
            "Train Epoch: 8 [15456/21600 (72%)]\tLoss: 0.004909\n",
            "Train Epoch: 8 [15552/21600 (72%)]\tLoss: 0.000995\n",
            "Train Epoch: 8 [15648/21600 (72%)]\tLoss: 0.002729\n",
            "Train Epoch: 8 [15744/21600 (73%)]\tLoss: 0.000778\n",
            "Train Epoch: 8 [15840/21600 (73%)]\tLoss: 0.048958\n",
            "Train Epoch: 8 [15936/21600 (74%)]\tLoss: 0.001645\n",
            "Train Epoch: 8 [16032/21600 (74%)]\tLoss: 0.035919\n",
            "Train Epoch: 8 [16128/21600 (75%)]\tLoss: 0.000056\n",
            "Train Epoch: 8 [16224/21600 (75%)]\tLoss: 0.000394\n",
            "Train Epoch: 8 [16320/21600 (76%)]\tLoss: 0.004311\n",
            "Train Epoch: 8 [16416/21600 (76%)]\tLoss: 0.001071\n",
            "Train Epoch: 8 [16512/21600 (76%)]\tLoss: 0.000764\n",
            "Train Epoch: 8 [16608/21600 (77%)]\tLoss: 0.001398\n",
            "Train Epoch: 8 [16704/21600 (77%)]\tLoss: 0.000118\n",
            "Train Epoch: 8 [16800/21600 (78%)]\tLoss: 0.000467\n",
            "Train Epoch: 8 [16896/21600 (78%)]\tLoss: 0.001821\n",
            "Train Epoch: 8 [16992/21600 (79%)]\tLoss: 0.000400\n",
            "Train Epoch: 8 [17088/21600 (79%)]\tLoss: 0.014399\n",
            "Train Epoch: 8 [17184/21600 (80%)]\tLoss: 0.001560\n",
            "Train Epoch: 8 [17280/21600 (80%)]\tLoss: 0.002219\n",
            "Train Epoch: 8 [17376/21600 (80%)]\tLoss: 0.019364\n",
            "Train Epoch: 8 [17472/21600 (81%)]\tLoss: 0.000418\n",
            "Train Epoch: 8 [17568/21600 (81%)]\tLoss: 0.005446\n",
            "Train Epoch: 8 [17664/21600 (82%)]\tLoss: 0.124952\n",
            "Train Epoch: 8 [17760/21600 (82%)]\tLoss: 0.000332\n",
            "Train Epoch: 8 [17856/21600 (83%)]\tLoss: 0.000579\n",
            "Train Epoch: 8 [17952/21600 (83%)]\tLoss: 0.001094\n",
            "Train Epoch: 8 [18048/21600 (84%)]\tLoss: 0.000262\n",
            "Train Epoch: 8 [18144/21600 (84%)]\tLoss: 0.074299\n",
            "Train Epoch: 8 [18240/21600 (84%)]\tLoss: 0.005662\n",
            "Train Epoch: 8 [18336/21600 (85%)]\tLoss: 0.018952\n",
            "Train Epoch: 8 [18432/21600 (85%)]\tLoss: 0.001976\n",
            "Train Epoch: 8 [18528/21600 (86%)]\tLoss: 0.012227\n",
            "Train Epoch: 8 [18624/21600 (86%)]\tLoss: 0.008543\n",
            "Train Epoch: 8 [18720/21600 (87%)]\tLoss: 0.000272\n",
            "Train Epoch: 8 [18816/21600 (87%)]\tLoss: 0.003766\n",
            "Train Epoch: 8 [18912/21600 (88%)]\tLoss: 0.000185\n",
            "Train Epoch: 8 [19008/21600 (88%)]\tLoss: 0.067221\n",
            "Train Epoch: 8 [19104/21600 (88%)]\tLoss: 0.010211\n",
            "Train Epoch: 8 [19200/21600 (89%)]\tLoss: 0.062852\n",
            "Train Epoch: 8 [19296/21600 (89%)]\tLoss: 0.000343\n",
            "Train Epoch: 8 [19392/21600 (90%)]\tLoss: 0.000035\n",
            "Train Epoch: 8 [19488/21600 (90%)]\tLoss: 0.016427\n",
            "Train Epoch: 8 [19584/21600 (91%)]\tLoss: 0.000179\n",
            "Train Epoch: 8 [19680/21600 (91%)]\tLoss: 0.000408\n",
            "Train Epoch: 8 [19776/21600 (92%)]\tLoss: 0.000329\n",
            "Train Epoch: 8 [19872/21600 (92%)]\tLoss: 0.031707\n",
            "Train Epoch: 8 [19968/21600 (92%)]\tLoss: 0.080666\n",
            "Train Epoch: 8 [20064/21600 (93%)]\tLoss: 0.000547\n",
            "Train Epoch: 8 [20160/21600 (93%)]\tLoss: 0.009144\n",
            "Train Epoch: 8 [20256/21600 (94%)]\tLoss: 0.041094\n",
            "Train Epoch: 8 [20352/21600 (94%)]\tLoss: 0.006489\n",
            "Train Epoch: 8 [20448/21600 (95%)]\tLoss: 0.011939\n",
            "Train Epoch: 8 [20544/21600 (95%)]\tLoss: 0.001830\n",
            "Train Epoch: 8 [20640/21600 (96%)]\tLoss: 0.018971\n",
            "Train Epoch: 8 [20736/21600 (96%)]\tLoss: 0.074552\n",
            "Train Epoch: 8 [20832/21600 (96%)]\tLoss: 0.002435\n",
            "Train Epoch: 8 [20928/21600 (97%)]\tLoss: 0.000185\n",
            "Train Epoch: 8 [21024/21600 (97%)]\tLoss: 0.000912\n",
            "Train Epoch: 8 [21120/21600 (98%)]\tLoss: 0.025364\n",
            "Train Epoch: 8 [21216/21600 (98%)]\tLoss: 0.002570\n",
            "Train Epoch: 8 [21312/21600 (99%)]\tLoss: 0.000221\n",
            "Train Epoch: 8 [21408/21600 (99%)]\tLoss: 0.022270\n",
            "Train Epoch: 8 [21504/21600 (100%)]\tLoss: 0.002290\n",
            "\n",
            "Validation set: Average loss: 0.0003, Accuracy: 5400/5400 (100%)\n",
            "\n",
            "\n",
            "Saved model to model_binary_8.pth.\n",
            "Train Epoch: 9 [0/21600 (0%)]\tLoss: 0.001867\n",
            "Train Epoch: 9 [96/21600 (0%)]\tLoss: 0.021704\n",
            "Train Epoch: 9 [192/21600 (1%)]\tLoss: 0.000130\n",
            "Train Epoch: 9 [288/21600 (1%)]\tLoss: 0.000492\n",
            "Train Epoch: 9 [384/21600 (2%)]\tLoss: 0.001145\n",
            "Train Epoch: 9 [480/21600 (2%)]\tLoss: 0.000234\n",
            "Train Epoch: 9 [576/21600 (3%)]\tLoss: 0.004016\n",
            "Train Epoch: 9 [672/21600 (3%)]\tLoss: 0.000062\n",
            "Train Epoch: 9 [768/21600 (4%)]\tLoss: 0.000157\n",
            "Train Epoch: 9 [864/21600 (4%)]\tLoss: 0.001077\n",
            "Train Epoch: 9 [960/21600 (4%)]\tLoss: 0.003503\n",
            "Train Epoch: 9 [1056/21600 (5%)]\tLoss: 0.001163\n",
            "Train Epoch: 9 [1152/21600 (5%)]\tLoss: 0.005831\n",
            "Train Epoch: 9 [1248/21600 (6%)]\tLoss: 0.000175\n",
            "Train Epoch: 9 [1344/21600 (6%)]\tLoss: 0.000261\n",
            "Train Epoch: 9 [1440/21600 (7%)]\tLoss: 0.000965\n",
            "Train Epoch: 9 [1536/21600 (7%)]\tLoss: 0.000455\n",
            "Train Epoch: 9 [1632/21600 (8%)]\tLoss: 0.000540\n",
            "Train Epoch: 9 [1728/21600 (8%)]\tLoss: 0.002022\n",
            "Train Epoch: 9 [1824/21600 (8%)]\tLoss: 0.038804\n",
            "Train Epoch: 9 [1920/21600 (9%)]\tLoss: 0.002184\n",
            "Train Epoch: 9 [2016/21600 (9%)]\tLoss: 0.000422\n",
            "Train Epoch: 9 [2112/21600 (10%)]\tLoss: 0.000301\n",
            "Train Epoch: 9 [2208/21600 (10%)]\tLoss: 0.007191\n",
            "Train Epoch: 9 [2304/21600 (11%)]\tLoss: 0.016980\n",
            "Train Epoch: 9 [2400/21600 (11%)]\tLoss: 0.001214\n",
            "Train Epoch: 9 [2496/21600 (12%)]\tLoss: 0.001693\n",
            "Train Epoch: 9 [2592/21600 (12%)]\tLoss: 0.001865\n",
            "Train Epoch: 9 [2688/21600 (12%)]\tLoss: 0.029843\n",
            "Train Epoch: 9 [2784/21600 (13%)]\tLoss: 0.002414\n",
            "Train Epoch: 9 [2880/21600 (13%)]\tLoss: 0.011274\n",
            "Train Epoch: 9 [2976/21600 (14%)]\tLoss: 0.002031\n",
            "Train Epoch: 9 [3072/21600 (14%)]\tLoss: 0.098843\n",
            "Train Epoch: 9 [3168/21600 (15%)]\tLoss: 0.000103\n",
            "Train Epoch: 9 [3264/21600 (15%)]\tLoss: 0.004753\n",
            "Train Epoch: 9 [3360/21600 (16%)]\tLoss: 0.004900\n",
            "Train Epoch: 9 [3456/21600 (16%)]\tLoss: 0.014589\n",
            "Train Epoch: 9 [3552/21600 (16%)]\tLoss: 0.004302\n",
            "Train Epoch: 9 [3648/21600 (17%)]\tLoss: 0.056445\n",
            "Train Epoch: 9 [3744/21600 (17%)]\tLoss: 0.004363\n",
            "Train Epoch: 9 [3840/21600 (18%)]\tLoss: 0.000990\n",
            "Train Epoch: 9 [3936/21600 (18%)]\tLoss: 0.000710\n",
            "Train Epoch: 9 [4032/21600 (19%)]\tLoss: 0.033370\n",
            "Train Epoch: 9 [4128/21600 (19%)]\tLoss: 0.001703\n",
            "Train Epoch: 9 [4224/21600 (20%)]\tLoss: 0.006019\n",
            "Train Epoch: 9 [4320/21600 (20%)]\tLoss: 0.000161\n",
            "Train Epoch: 9 [4416/21600 (20%)]\tLoss: 0.000248\n",
            "Train Epoch: 9 [4512/21600 (21%)]\tLoss: 0.000872\n",
            "Train Epoch: 9 [4608/21600 (21%)]\tLoss: 0.000090\n",
            "Train Epoch: 9 [4704/21600 (22%)]\tLoss: 0.000689\n",
            "Train Epoch: 9 [4800/21600 (22%)]\tLoss: 0.002180\n",
            "Train Epoch: 9 [4896/21600 (23%)]\tLoss: 0.000183\n",
            "Train Epoch: 9 [4992/21600 (23%)]\tLoss: 0.037899\n",
            "Train Epoch: 9 [5088/21600 (24%)]\tLoss: 0.000907\n",
            "Train Epoch: 9 [5184/21600 (24%)]\tLoss: 0.020076\n",
            "Train Epoch: 9 [5280/21600 (24%)]\tLoss: 0.000238\n",
            "Train Epoch: 9 [5376/21600 (25%)]\tLoss: 0.004684\n",
            "Train Epoch: 9 [5472/21600 (25%)]\tLoss: 0.023697\n",
            "Train Epoch: 9 [5568/21600 (26%)]\tLoss: 0.003261\n",
            "Train Epoch: 9 [5664/21600 (26%)]\tLoss: 0.000401\n",
            "Train Epoch: 9 [5760/21600 (27%)]\tLoss: 0.000090\n",
            "Train Epoch: 9 [5856/21600 (27%)]\tLoss: 0.000067\n",
            "Train Epoch: 9 [5952/21600 (28%)]\tLoss: 0.019122\n",
            "Train Epoch: 9 [6048/21600 (28%)]\tLoss: 0.004696\n",
            "Train Epoch: 9 [6144/21600 (28%)]\tLoss: 0.000217\n",
            "Train Epoch: 9 [6240/21600 (29%)]\tLoss: 0.001240\n",
            "Train Epoch: 9 [6336/21600 (29%)]\tLoss: 0.001445\n",
            "Train Epoch: 9 [6432/21600 (30%)]\tLoss: 0.000285\n",
            "Train Epoch: 9 [6528/21600 (30%)]\tLoss: 0.000034\n",
            "Train Epoch: 9 [6624/21600 (31%)]\tLoss: 0.001624\n",
            "Train Epoch: 9 [6720/21600 (31%)]\tLoss: 0.000131\n",
            "Train Epoch: 9 [6816/21600 (32%)]\tLoss: 0.000790\n",
            "Train Epoch: 9 [6912/21600 (32%)]\tLoss: 0.001088\n",
            "Train Epoch: 9 [7008/21600 (32%)]\tLoss: 0.000174\n",
            "Train Epoch: 9 [7104/21600 (33%)]\tLoss: 0.001381\n",
            "Train Epoch: 9 [7200/21600 (33%)]\tLoss: 0.002317\n",
            "Train Epoch: 9 [7296/21600 (34%)]\tLoss: 0.000111\n",
            "Train Epoch: 9 [7392/21600 (34%)]\tLoss: 0.000638\n",
            "Train Epoch: 9 [7488/21600 (35%)]\tLoss: 0.008421\n",
            "Train Epoch: 9 [7584/21600 (35%)]\tLoss: 0.000051\n",
            "Train Epoch: 9 [7680/21600 (36%)]\tLoss: 0.021219\n",
            "Train Epoch: 9 [7776/21600 (36%)]\tLoss: 0.005936\n",
            "Train Epoch: 9 [7872/21600 (36%)]\tLoss: 0.023382\n",
            "Train Epoch: 9 [7968/21600 (37%)]\tLoss: 0.000407\n",
            "Train Epoch: 9 [8064/21600 (37%)]\tLoss: 0.002077\n",
            "Train Epoch: 9 [8160/21600 (38%)]\tLoss: 0.000191\n",
            "Train Epoch: 9 [8256/21600 (38%)]\tLoss: 0.000327\n",
            "Train Epoch: 9 [8352/21600 (39%)]\tLoss: 0.000459\n",
            "Train Epoch: 9 [8448/21600 (39%)]\tLoss: 0.000060\n",
            "Train Epoch: 9 [8544/21600 (40%)]\tLoss: 0.000296\n",
            "Train Epoch: 9 [8640/21600 (40%)]\tLoss: 0.002391\n",
            "Train Epoch: 9 [8736/21600 (40%)]\tLoss: 0.000470\n",
            "Train Epoch: 9 [8832/21600 (41%)]\tLoss: 0.002179\n",
            "Train Epoch: 9 [8928/21600 (41%)]\tLoss: 0.000655\n",
            "Train Epoch: 9 [9024/21600 (42%)]\tLoss: 0.037438\n",
            "Train Epoch: 9 [9120/21600 (42%)]\tLoss: 0.001280\n",
            "Train Epoch: 9 [9216/21600 (43%)]\tLoss: 0.010901\n",
            "Train Epoch: 9 [9312/21600 (43%)]\tLoss: 0.004254\n",
            "Train Epoch: 9 [9408/21600 (44%)]\tLoss: 0.013626\n",
            "Train Epoch: 9 [9504/21600 (44%)]\tLoss: 0.000376\n",
            "Train Epoch: 9 [9600/21600 (44%)]\tLoss: 0.001245\n",
            "Train Epoch: 9 [9696/21600 (45%)]\tLoss: 0.039734\n",
            "Train Epoch: 9 [9792/21600 (45%)]\tLoss: 0.006084\n",
            "Train Epoch: 9 [9888/21600 (46%)]\tLoss: 0.007164\n",
            "Train Epoch: 9 [9984/21600 (46%)]\tLoss: 0.021153\n",
            "Train Epoch: 9 [10080/21600 (47%)]\tLoss: 0.000365\n",
            "Train Epoch: 9 [10176/21600 (47%)]\tLoss: 0.025153\n",
            "Train Epoch: 9 [10272/21600 (48%)]\tLoss: 0.004619\n",
            "Train Epoch: 9 [10368/21600 (48%)]\tLoss: 0.000494\n",
            "Train Epoch: 9 [10464/21600 (48%)]\tLoss: 0.000690\n",
            "Train Epoch: 9 [10560/21600 (49%)]\tLoss: 0.001210\n",
            "Train Epoch: 9 [10656/21600 (49%)]\tLoss: 0.000247\n",
            "Train Epoch: 9 [10752/21600 (50%)]\tLoss: 0.000234\n",
            "Train Epoch: 9 [10848/21600 (50%)]\tLoss: 0.000274\n",
            "Train Epoch: 9 [10944/21600 (51%)]\tLoss: 0.000240\n",
            "Train Epoch: 9 [11040/21600 (51%)]\tLoss: 0.000158\n",
            "Train Epoch: 9 [11136/21600 (52%)]\tLoss: 0.000613\n",
            "Train Epoch: 9 [11232/21600 (52%)]\tLoss: 0.005418\n",
            "Train Epoch: 9 [11328/21600 (52%)]\tLoss: 0.045320\n",
            "Train Epoch: 9 [11424/21600 (53%)]\tLoss: 0.005786\n",
            "Train Epoch: 9 [11520/21600 (53%)]\tLoss: 0.004694\n",
            "Train Epoch: 9 [11616/21600 (54%)]\tLoss: 0.000080\n",
            "Train Epoch: 9 [11712/21600 (54%)]\tLoss: 0.013851\n",
            "Train Epoch: 9 [11808/21600 (55%)]\tLoss: 0.000441\n",
            "Train Epoch: 9 [11904/21600 (55%)]\tLoss: 0.000143\n",
            "Train Epoch: 9 [12000/21600 (56%)]\tLoss: 0.000238\n",
            "Train Epoch: 9 [12096/21600 (56%)]\tLoss: 0.000093\n",
            "Train Epoch: 9 [12192/21600 (56%)]\tLoss: 0.033624\n",
            "Train Epoch: 9 [12288/21600 (57%)]\tLoss: 0.000462\n",
            "Train Epoch: 9 [12384/21600 (57%)]\tLoss: 0.070540\n",
            "Train Epoch: 9 [12480/21600 (58%)]\tLoss: 0.000274\n",
            "Train Epoch: 9 [12576/21600 (58%)]\tLoss: 0.003182\n",
            "Train Epoch: 9 [12672/21600 (59%)]\tLoss: 0.025960\n",
            "Train Epoch: 9 [12768/21600 (59%)]\tLoss: 0.054351\n",
            "Train Epoch: 9 [12864/21600 (60%)]\tLoss: 0.000041\n",
            "Train Epoch: 9 [12960/21600 (60%)]\tLoss: 0.018609\n",
            "Train Epoch: 9 [13056/21600 (60%)]\tLoss: 0.003480\n",
            "Train Epoch: 9 [13152/21600 (61%)]\tLoss: 0.001497\n",
            "Train Epoch: 9 [13248/21600 (61%)]\tLoss: 0.000823\n",
            "Train Epoch: 9 [13344/21600 (62%)]\tLoss: 0.008760\n",
            "Train Epoch: 9 [13440/21600 (62%)]\tLoss: 0.010517\n",
            "Train Epoch: 9 [13536/21600 (63%)]\tLoss: 0.064437\n",
            "Train Epoch: 9 [13632/21600 (63%)]\tLoss: 0.001023\n",
            "Train Epoch: 9 [13728/21600 (64%)]\tLoss: 0.000176\n",
            "Train Epoch: 9 [13824/21600 (64%)]\tLoss: 0.046535\n",
            "Train Epoch: 9 [13920/21600 (64%)]\tLoss: 0.000237\n",
            "Train Epoch: 9 [14016/21600 (65%)]\tLoss: 0.002386\n",
            "Train Epoch: 9 [14112/21600 (65%)]\tLoss: 0.002395\n",
            "Train Epoch: 9 [14208/21600 (66%)]\tLoss: 0.002470\n",
            "Train Epoch: 9 [14304/21600 (66%)]\tLoss: 0.001353\n",
            "Train Epoch: 9 [14400/21600 (67%)]\tLoss: 0.000917\n",
            "Train Epoch: 9 [14496/21600 (67%)]\tLoss: 0.002382\n",
            "Train Epoch: 9 [14592/21600 (68%)]\tLoss: 0.004588\n",
            "Train Epoch: 9 [14688/21600 (68%)]\tLoss: 0.000889\n",
            "Train Epoch: 9 [14784/21600 (68%)]\tLoss: 0.044092\n",
            "Train Epoch: 9 [14880/21600 (69%)]\tLoss: 0.000050\n",
            "Train Epoch: 9 [14976/21600 (69%)]\tLoss: 0.000375\n",
            "Train Epoch: 9 [15072/21600 (70%)]\tLoss: 0.000501\n",
            "Train Epoch: 9 [15168/21600 (70%)]\tLoss: 0.004254\n",
            "Train Epoch: 9 [15264/21600 (71%)]\tLoss: 0.000106\n",
            "Train Epoch: 9 [15360/21600 (71%)]\tLoss: 0.000242\n",
            "Train Epoch: 9 [15456/21600 (72%)]\tLoss: 0.000112\n",
            "Train Epoch: 9 [15552/21600 (72%)]\tLoss: 0.001320\n",
            "Train Epoch: 9 [15648/21600 (72%)]\tLoss: 0.000443\n",
            "Train Epoch: 9 [15744/21600 (73%)]\tLoss: 0.012977\n",
            "Train Epoch: 9 [15840/21600 (73%)]\tLoss: 0.000099\n",
            "Train Epoch: 9 [15936/21600 (74%)]\tLoss: 0.004453\n",
            "Train Epoch: 9 [16032/21600 (74%)]\tLoss: 0.002736\n",
            "Train Epoch: 9 [16128/21600 (75%)]\tLoss: 0.006366\n",
            "Train Epoch: 9 [16224/21600 (75%)]\tLoss: 0.010003\n",
            "Train Epoch: 9 [16320/21600 (76%)]\tLoss: 0.000854\n",
            "Train Epoch: 9 [16416/21600 (76%)]\tLoss: 0.006091\n",
            "Train Epoch: 9 [16512/21600 (76%)]\tLoss: 0.000064\n",
            "Train Epoch: 9 [16608/21600 (77%)]\tLoss: 0.003337\n",
            "Train Epoch: 9 [16704/21600 (77%)]\tLoss: 0.000175\n",
            "Train Epoch: 9 [16800/21600 (78%)]\tLoss: 0.058207\n",
            "Train Epoch: 9 [16896/21600 (78%)]\tLoss: 0.000066\n",
            "Train Epoch: 9 [16992/21600 (79%)]\tLoss: 0.009885\n",
            "Train Epoch: 9 [17088/21600 (79%)]\tLoss: 0.000070\n",
            "Train Epoch: 9 [17184/21600 (80%)]\tLoss: 0.000157\n",
            "Train Epoch: 9 [17280/21600 (80%)]\tLoss: 0.000078\n",
            "Train Epoch: 9 [17376/21600 (80%)]\tLoss: 0.000172\n",
            "Train Epoch: 9 [17472/21600 (81%)]\tLoss: 0.000029\n",
            "Train Epoch: 9 [17568/21600 (81%)]\tLoss: 0.015157\n",
            "Train Epoch: 9 [17664/21600 (82%)]\tLoss: 0.001563\n",
            "Train Epoch: 9 [17760/21600 (82%)]\tLoss: 0.000171\n",
            "Train Epoch: 9 [17856/21600 (83%)]\tLoss: 0.000304\n",
            "Train Epoch: 9 [17952/21600 (83%)]\tLoss: 0.000472\n",
            "Train Epoch: 9 [18048/21600 (84%)]\tLoss: 0.000581\n",
            "Train Epoch: 9 [18144/21600 (84%)]\tLoss: 0.000084\n",
            "Train Epoch: 9 [18240/21600 (84%)]\tLoss: 0.002852\n",
            "Train Epoch: 9 [18336/21600 (85%)]\tLoss: 0.001150\n",
            "Train Epoch: 9 [18432/21600 (85%)]\tLoss: 0.140224\n",
            "Train Epoch: 9 [18528/21600 (86%)]\tLoss: 0.002011\n",
            "Train Epoch: 9 [18624/21600 (86%)]\tLoss: 0.000099\n",
            "Train Epoch: 9 [18720/21600 (87%)]\tLoss: 0.002739\n",
            "Train Epoch: 9 [18816/21600 (87%)]\tLoss: 0.000493\n",
            "Train Epoch: 9 [18912/21600 (88%)]\tLoss: 0.026642\n",
            "Train Epoch: 9 [19008/21600 (88%)]\tLoss: 0.001336\n",
            "Train Epoch: 9 [19104/21600 (88%)]\tLoss: 0.000138\n",
            "Train Epoch: 9 [19200/21600 (89%)]\tLoss: 0.000601\n",
            "Train Epoch: 9 [19296/21600 (89%)]\tLoss: 0.000165\n",
            "Train Epoch: 9 [19392/21600 (90%)]\tLoss: 0.004628\n",
            "Train Epoch: 9 [19488/21600 (90%)]\tLoss: 0.000086\n",
            "Train Epoch: 9 [19584/21600 (91%)]\tLoss: 0.000488\n",
            "Train Epoch: 9 [19680/21600 (91%)]\tLoss: 0.000111\n",
            "Train Epoch: 9 [19776/21600 (92%)]\tLoss: 0.000296\n",
            "Train Epoch: 9 [19872/21600 (92%)]\tLoss: 0.000341\n",
            "Train Epoch: 9 [19968/21600 (92%)]\tLoss: 0.001571\n",
            "Train Epoch: 9 [20064/21600 (93%)]\tLoss: 0.000036\n",
            "Train Epoch: 9 [20160/21600 (93%)]\tLoss: 0.002576\n",
            "Train Epoch: 9 [20256/21600 (94%)]\tLoss: 0.000352\n",
            "Train Epoch: 9 [20352/21600 (94%)]\tLoss: 0.010076\n",
            "Train Epoch: 9 [20448/21600 (95%)]\tLoss: 0.000180\n",
            "Train Epoch: 9 [20544/21600 (95%)]\tLoss: 0.182325\n",
            "Train Epoch: 9 [20640/21600 (96%)]\tLoss: 0.002943\n",
            "Train Epoch: 9 [20736/21600 (96%)]\tLoss: 0.000146\n",
            "Train Epoch: 9 [20832/21600 (96%)]\tLoss: 0.000036\n",
            "Train Epoch: 9 [20928/21600 (97%)]\tLoss: 0.007498\n",
            "Train Epoch: 9 [21024/21600 (97%)]\tLoss: 0.000096\n",
            "Train Epoch: 9 [21120/21600 (98%)]\tLoss: 0.005746\n",
            "Train Epoch: 9 [21216/21600 (98%)]\tLoss: 0.000487\n",
            "Train Epoch: 9 [21312/21600 (99%)]\tLoss: 0.000439\n",
            "Train Epoch: 9 [21408/21600 (99%)]\tLoss: 0.000099\n",
            "Train Epoch: 9 [21504/21600 (100%)]\tLoss: 0.000017\n",
            "\n",
            "Validation set: Average loss: 0.0002, Accuracy: 5400/5400 (100%)\n",
            "\n",
            "\n",
            "Saved model to model_binary_9.pth.\n",
            "Train Epoch: 10 [0/21600 (0%)]\tLoss: 0.002283\n",
            "Train Epoch: 10 [96/21600 (0%)]\tLoss: 0.002914\n",
            "Train Epoch: 10 [192/21600 (1%)]\tLoss: 0.005534\n",
            "Train Epoch: 10 [288/21600 (1%)]\tLoss: 0.000900\n",
            "Train Epoch: 10 [384/21600 (2%)]\tLoss: 0.028385\n",
            "Train Epoch: 10 [480/21600 (2%)]\tLoss: 0.000849\n",
            "Train Epoch: 10 [576/21600 (3%)]\tLoss: 0.017716\n",
            "Train Epoch: 10 [672/21600 (3%)]\tLoss: 0.000056\n",
            "Train Epoch: 10 [768/21600 (4%)]\tLoss: 0.003498\n",
            "Train Epoch: 10 [864/21600 (4%)]\tLoss: 0.000696\n",
            "Train Epoch: 10 [960/21600 (4%)]\tLoss: 0.000183\n",
            "Train Epoch: 10 [1056/21600 (5%)]\tLoss: 0.019616\n",
            "Train Epoch: 10 [1152/21600 (5%)]\tLoss: 0.054338\n",
            "Train Epoch: 10 [1248/21600 (6%)]\tLoss: 0.024569\n",
            "Train Epoch: 10 [1344/21600 (6%)]\tLoss: 0.000028\n",
            "Train Epoch: 10 [1440/21600 (7%)]\tLoss: 0.001124\n",
            "Train Epoch: 10 [1536/21600 (7%)]\tLoss: 0.001357\n",
            "Train Epoch: 10 [1632/21600 (8%)]\tLoss: 0.000526\n",
            "Train Epoch: 10 [1728/21600 (8%)]\tLoss: 0.001013\n",
            "Train Epoch: 10 [1824/21600 (8%)]\tLoss: 0.001359\n",
            "Train Epoch: 10 [1920/21600 (9%)]\tLoss: 0.000921\n",
            "Train Epoch: 10 [2016/21600 (9%)]\tLoss: 0.000134\n",
            "Train Epoch: 10 [2112/21600 (10%)]\tLoss: 0.062602\n",
            "Train Epoch: 10 [2208/21600 (10%)]\tLoss: 0.000520\n",
            "Train Epoch: 10 [2304/21600 (11%)]\tLoss: 0.002269\n",
            "Train Epoch: 10 [2400/21600 (11%)]\tLoss: 0.000628\n",
            "Train Epoch: 10 [2496/21600 (12%)]\tLoss: 0.000034\n",
            "Train Epoch: 10 [2592/21600 (12%)]\tLoss: 0.000125\n",
            "Train Epoch: 10 [2688/21600 (12%)]\tLoss: 0.000383\n",
            "Train Epoch: 10 [2784/21600 (13%)]\tLoss: 0.000055\n",
            "Train Epoch: 10 [2880/21600 (13%)]\tLoss: 0.052409\n",
            "Train Epoch: 10 [2976/21600 (14%)]\tLoss: 0.039972\n",
            "Train Epoch: 10 [3072/21600 (14%)]\tLoss: 0.000100\n",
            "Train Epoch: 10 [3168/21600 (15%)]\tLoss: 0.004716\n",
            "Train Epoch: 10 [3264/21600 (15%)]\tLoss: 0.000804\n",
            "Train Epoch: 10 [3360/21600 (16%)]\tLoss: 0.054625\n",
            "Train Epoch: 10 [3456/21600 (16%)]\tLoss: 0.207672\n",
            "Train Epoch: 10 [3552/21600 (16%)]\tLoss: 0.000148\n",
            "Train Epoch: 10 [3648/21600 (17%)]\tLoss: 0.051849\n",
            "Train Epoch: 10 [3744/21600 (17%)]\tLoss: 0.005386\n",
            "Train Epoch: 10 [3840/21600 (18%)]\tLoss: 0.000656\n",
            "Train Epoch: 10 [3936/21600 (18%)]\tLoss: 0.005522\n",
            "Train Epoch: 10 [4032/21600 (19%)]\tLoss: 0.002554\n",
            "Train Epoch: 10 [4128/21600 (19%)]\tLoss: 0.000292\n",
            "Train Epoch: 10 [4224/21600 (20%)]\tLoss: 0.047645\n",
            "Train Epoch: 10 [4320/21600 (20%)]\tLoss: 0.000664\n",
            "Train Epoch: 10 [4416/21600 (20%)]\tLoss: 0.000689\n",
            "Train Epoch: 10 [4512/21600 (21%)]\tLoss: 0.000426\n",
            "Train Epoch: 10 [4608/21600 (21%)]\tLoss: 0.000109\n",
            "Train Epoch: 10 [4704/21600 (22%)]\tLoss: 0.000147\n",
            "Train Epoch: 10 [4800/21600 (22%)]\tLoss: 0.027926\n",
            "Train Epoch: 10 [4896/21600 (23%)]\tLoss: 0.038330\n",
            "Train Epoch: 10 [4992/21600 (23%)]\tLoss: 0.000020\n",
            "Train Epoch: 10 [5088/21600 (24%)]\tLoss: 0.000289\n",
            "Train Epoch: 10 [5184/21600 (24%)]\tLoss: 0.001637\n",
            "Train Epoch: 10 [5280/21600 (24%)]\tLoss: 0.000468\n",
            "Train Epoch: 10 [5376/21600 (25%)]\tLoss: 0.011865\n",
            "Train Epoch: 10 [5472/21600 (25%)]\tLoss: 0.015067\n",
            "Train Epoch: 10 [5568/21600 (26%)]\tLoss: 0.002840\n",
            "Train Epoch: 10 [5664/21600 (26%)]\tLoss: 0.000149\n",
            "Train Epoch: 10 [5760/21600 (27%)]\tLoss: 0.010104\n",
            "Train Epoch: 10 [5856/21600 (27%)]\tLoss: 0.000950\n",
            "Train Epoch: 10 [5952/21600 (28%)]\tLoss: 0.050316\n",
            "Train Epoch: 10 [6048/21600 (28%)]\tLoss: 0.003963\n",
            "Train Epoch: 10 [6144/21600 (28%)]\tLoss: 0.002125\n",
            "Train Epoch: 10 [6240/21600 (29%)]\tLoss: 0.004486\n",
            "Train Epoch: 10 [6336/21600 (29%)]\tLoss: 0.000760\n",
            "Train Epoch: 10 [6432/21600 (30%)]\tLoss: 0.000582\n",
            "Train Epoch: 10 [6528/21600 (30%)]\tLoss: 0.017389\n",
            "Train Epoch: 10 [6624/21600 (31%)]\tLoss: 0.008554\n",
            "Train Epoch: 10 [6720/21600 (31%)]\tLoss: 0.000096\n",
            "Train Epoch: 10 [6816/21600 (32%)]\tLoss: 0.000146\n",
            "Train Epoch: 10 [6912/21600 (32%)]\tLoss: 0.000096\n",
            "Train Epoch: 10 [7008/21600 (32%)]\tLoss: 0.001636\n",
            "Train Epoch: 10 [7104/21600 (33%)]\tLoss: 0.000121\n",
            "Train Epoch: 10 [7200/21600 (33%)]\tLoss: 0.001562\n",
            "Train Epoch: 10 [7296/21600 (34%)]\tLoss: 0.027756\n",
            "Train Epoch: 10 [7392/21600 (34%)]\tLoss: 0.000228\n",
            "Train Epoch: 10 [7488/21600 (35%)]\tLoss: 0.003352\n",
            "Train Epoch: 10 [7584/21600 (35%)]\tLoss: 0.034561\n",
            "Train Epoch: 10 [7680/21600 (36%)]\tLoss: 0.001530\n",
            "Train Epoch: 10 [7776/21600 (36%)]\tLoss: 0.006289\n",
            "Train Epoch: 10 [7872/21600 (36%)]\tLoss: 0.000204\n",
            "Train Epoch: 10 [7968/21600 (37%)]\tLoss: 0.000612\n",
            "Train Epoch: 10 [8064/21600 (37%)]\tLoss: 0.001954\n",
            "Train Epoch: 10 [8160/21600 (38%)]\tLoss: 0.002016\n",
            "Train Epoch: 10 [8256/21600 (38%)]\tLoss: 0.004615\n",
            "Train Epoch: 10 [8352/21600 (39%)]\tLoss: 0.005615\n",
            "Train Epoch: 10 [8448/21600 (39%)]\tLoss: 0.111106\n",
            "Train Epoch: 10 [8544/21600 (40%)]\tLoss: 0.000134\n",
            "Train Epoch: 10 [8640/21600 (40%)]\tLoss: 0.001585\n",
            "Train Epoch: 10 [8736/21600 (40%)]\tLoss: 0.025346\n",
            "Train Epoch: 10 [8832/21600 (41%)]\tLoss: 0.000203\n",
            "Train Epoch: 10 [8928/21600 (41%)]\tLoss: 0.000132\n",
            "Train Epoch: 10 [9024/21600 (42%)]\tLoss: 0.000207\n",
            "Train Epoch: 10 [9120/21600 (42%)]\tLoss: 0.018183\n",
            "Train Epoch: 10 [9216/21600 (43%)]\tLoss: 0.000865\n",
            "Train Epoch: 10 [9312/21600 (43%)]\tLoss: 0.005716\n",
            "Train Epoch: 10 [9408/21600 (44%)]\tLoss: 0.001282\n",
            "Train Epoch: 10 [9504/21600 (44%)]\tLoss: 0.002012\n",
            "Train Epoch: 10 [9600/21600 (44%)]\tLoss: 0.000077\n",
            "Train Epoch: 10 [9696/21600 (45%)]\tLoss: 0.162277\n",
            "Train Epoch: 10 [9792/21600 (45%)]\tLoss: 0.019599\n",
            "Train Epoch: 10 [9888/21600 (46%)]\tLoss: 0.000420\n",
            "Train Epoch: 10 [9984/21600 (46%)]\tLoss: 0.000039\n",
            "Train Epoch: 10 [10080/21600 (47%)]\tLoss: 0.009120\n",
            "Train Epoch: 10 [10176/21600 (47%)]\tLoss: 0.001789\n",
            "Train Epoch: 10 [10272/21600 (48%)]\tLoss: 0.002062\n",
            "Train Epoch: 10 [10368/21600 (48%)]\tLoss: 0.003805\n",
            "Train Epoch: 10 [10464/21600 (48%)]\tLoss: 0.007269\n",
            "Train Epoch: 10 [10560/21600 (49%)]\tLoss: 0.033605\n",
            "Train Epoch: 10 [10656/21600 (49%)]\tLoss: 0.000062\n",
            "Train Epoch: 10 [10752/21600 (50%)]\tLoss: 0.000127\n",
            "Train Epoch: 10 [10848/21600 (50%)]\tLoss: 0.044902\n",
            "Train Epoch: 10 [10944/21600 (51%)]\tLoss: 0.000770\n",
            "Train Epoch: 10 [11040/21600 (51%)]\tLoss: 0.000239\n",
            "Train Epoch: 10 [11136/21600 (52%)]\tLoss: 0.000039\n",
            "Train Epoch: 10 [11232/21600 (52%)]\tLoss: 0.000170\n",
            "Train Epoch: 10 [11328/21600 (52%)]\tLoss: 0.000688\n",
            "Train Epoch: 10 [11424/21600 (53%)]\tLoss: 0.007183\n",
            "Train Epoch: 10 [11520/21600 (53%)]\tLoss: 0.000762\n",
            "Train Epoch: 10 [11616/21600 (54%)]\tLoss: 0.001371\n",
            "Train Epoch: 10 [11712/21600 (54%)]\tLoss: 0.000255\n",
            "Train Epoch: 10 [11808/21600 (55%)]\tLoss: 0.000135\n",
            "Train Epoch: 10 [11904/21600 (55%)]\tLoss: 0.105351\n",
            "Train Epoch: 10 [12000/21600 (56%)]\tLoss: 0.000441\n",
            "Train Epoch: 10 [12096/21600 (56%)]\tLoss: 0.000169\n",
            "Train Epoch: 10 [12192/21600 (56%)]\tLoss: 0.040249\n",
            "Train Epoch: 10 [12288/21600 (57%)]\tLoss: 0.008194\n",
            "Train Epoch: 10 [12384/21600 (57%)]\tLoss: 0.003964\n",
            "Train Epoch: 10 [12480/21600 (58%)]\tLoss: 0.000264\n",
            "Train Epoch: 10 [12576/21600 (58%)]\tLoss: 0.000555\n",
            "Train Epoch: 10 [12672/21600 (59%)]\tLoss: 0.001454\n",
            "Train Epoch: 10 [12768/21600 (59%)]\tLoss: 0.002812\n",
            "Train Epoch: 10 [12864/21600 (60%)]\tLoss: 0.014667\n",
            "Train Epoch: 10 [12960/21600 (60%)]\tLoss: 0.001301\n",
            "Train Epoch: 10 [13056/21600 (60%)]\tLoss: 0.003553\n",
            "Train Epoch: 10 [13152/21600 (61%)]\tLoss: 0.000085\n",
            "Train Epoch: 10 [13248/21600 (61%)]\tLoss: 0.003418\n",
            "Train Epoch: 10 [13344/21600 (62%)]\tLoss: 0.000533\n",
            "Train Epoch: 10 [13440/21600 (62%)]\tLoss: 0.024535\n",
            "Train Epoch: 10 [13536/21600 (63%)]\tLoss: 0.003268\n",
            "Train Epoch: 10 [13632/21600 (63%)]\tLoss: 0.008692\n",
            "Train Epoch: 10 [13728/21600 (64%)]\tLoss: 0.000387\n",
            "Train Epoch: 10 [13824/21600 (64%)]\tLoss: 0.005814\n",
            "Train Epoch: 10 [13920/21600 (64%)]\tLoss: 0.000046\n",
            "Train Epoch: 10 [14016/21600 (65%)]\tLoss: 0.001342\n",
            "Train Epoch: 10 [14112/21600 (65%)]\tLoss: 0.000378\n",
            "Train Epoch: 10 [14208/21600 (66%)]\tLoss: 0.012793\n",
            "Train Epoch: 10 [14304/21600 (66%)]\tLoss: 0.000075\n",
            "Train Epoch: 10 [14400/21600 (67%)]\tLoss: 0.005987\n",
            "Train Epoch: 10 [14496/21600 (67%)]\tLoss: 0.000470\n",
            "Train Epoch: 10 [14592/21600 (68%)]\tLoss: 0.000162\n",
            "Train Epoch: 10 [14688/21600 (68%)]\tLoss: 0.000059\n",
            "Train Epoch: 10 [14784/21600 (68%)]\tLoss: 0.005536\n",
            "Train Epoch: 10 [14880/21600 (69%)]\tLoss: 0.000623\n",
            "Train Epoch: 10 [14976/21600 (69%)]\tLoss: 0.000120\n",
            "Train Epoch: 10 [15072/21600 (70%)]\tLoss: 0.007941\n",
            "Train Epoch: 10 [15168/21600 (70%)]\tLoss: 0.020723\n",
            "Train Epoch: 10 [15264/21600 (71%)]\tLoss: 0.000069\n",
            "Train Epoch: 10 [15360/21600 (71%)]\tLoss: 0.000029\n",
            "Train Epoch: 10 [15456/21600 (72%)]\tLoss: 0.000485\n",
            "Train Epoch: 10 [15552/21600 (72%)]\tLoss: 0.001541\n",
            "Train Epoch: 10 [15648/21600 (72%)]\tLoss: 0.001664\n",
            "Train Epoch: 10 [15744/21600 (73%)]\tLoss: 0.003757\n",
            "Train Epoch: 10 [15840/21600 (73%)]\tLoss: 0.001516\n",
            "Train Epoch: 10 [15936/21600 (74%)]\tLoss: 0.001589\n",
            "Train Epoch: 10 [16032/21600 (74%)]\tLoss: 0.002308\n",
            "Train Epoch: 10 [16128/21600 (75%)]\tLoss: 0.001493\n",
            "Train Epoch: 10 [16224/21600 (75%)]\tLoss: 0.000092\n",
            "Train Epoch: 10 [16320/21600 (76%)]\tLoss: 0.001488\n",
            "Train Epoch: 10 [16416/21600 (76%)]\tLoss: 0.117076\n",
            "Train Epoch: 10 [16512/21600 (76%)]\tLoss: 0.027152\n",
            "Train Epoch: 10 [16608/21600 (77%)]\tLoss: 0.000337\n",
            "Train Epoch: 10 [16704/21600 (77%)]\tLoss: 0.000116\n",
            "Train Epoch: 10 [16800/21600 (78%)]\tLoss: 0.000067\n",
            "Train Epoch: 10 [16896/21600 (78%)]\tLoss: 0.000925\n",
            "Train Epoch: 10 [16992/21600 (79%)]\tLoss: 0.002386\n",
            "Train Epoch: 10 [17088/21600 (79%)]\tLoss: 0.026443\n",
            "Train Epoch: 10 [17184/21600 (80%)]\tLoss: 0.030310\n",
            "Train Epoch: 10 [17280/21600 (80%)]\tLoss: 0.000033\n",
            "Train Epoch: 10 [17376/21600 (80%)]\tLoss: 0.000528\n",
            "Train Epoch: 10 [17472/21600 (81%)]\tLoss: 0.056435\n",
            "Train Epoch: 10 [17568/21600 (81%)]\tLoss: 0.000108\n",
            "Train Epoch: 10 [17664/21600 (82%)]\tLoss: 0.000106\n",
            "Train Epoch: 10 [17760/21600 (82%)]\tLoss: 0.000245\n",
            "Train Epoch: 10 [17856/21600 (83%)]\tLoss: 0.000029\n",
            "Train Epoch: 10 [17952/21600 (83%)]\tLoss: 0.000322\n",
            "Train Epoch: 10 [18048/21600 (84%)]\tLoss: 0.000239\n",
            "Train Epoch: 10 [18144/21600 (84%)]\tLoss: 0.026275\n",
            "Train Epoch: 10 [18240/21600 (84%)]\tLoss: 0.049404\n",
            "Train Epoch: 10 [18336/21600 (85%)]\tLoss: 0.000091\n",
            "Train Epoch: 10 [18432/21600 (85%)]\tLoss: 0.002418\n",
            "Train Epoch: 10 [18528/21600 (86%)]\tLoss: 0.000259\n",
            "Train Epoch: 10 [18624/21600 (86%)]\tLoss: 0.000239\n",
            "Train Epoch: 10 [18720/21600 (87%)]\tLoss: 0.001361\n",
            "Train Epoch: 10 [18816/21600 (87%)]\tLoss: 0.024478\n",
            "Train Epoch: 10 [18912/21600 (88%)]\tLoss: 0.000049\n",
            "Train Epoch: 10 [19008/21600 (88%)]\tLoss: 0.000269\n",
            "Train Epoch: 10 [19104/21600 (88%)]\tLoss: 0.005738\n",
            "Train Epoch: 10 [19200/21600 (89%)]\tLoss: 0.003797\n",
            "Train Epoch: 10 [19296/21600 (89%)]\tLoss: 0.005793\n",
            "Train Epoch: 10 [19392/21600 (90%)]\tLoss: 0.000034\n",
            "Train Epoch: 10 [19488/21600 (90%)]\tLoss: 0.000750\n",
            "Train Epoch: 10 [19584/21600 (91%)]\tLoss: 0.000050\n",
            "Train Epoch: 10 [19680/21600 (91%)]\tLoss: 0.013040\n",
            "Train Epoch: 10 [19776/21600 (92%)]\tLoss: 0.000141\n",
            "Train Epoch: 10 [19872/21600 (92%)]\tLoss: 0.000075\n",
            "Train Epoch: 10 [19968/21600 (92%)]\tLoss: 0.003505\n",
            "Train Epoch: 10 [20064/21600 (93%)]\tLoss: 0.001109\n",
            "Train Epoch: 10 [20160/21600 (93%)]\tLoss: 0.009727\n",
            "Train Epoch: 10 [20256/21600 (94%)]\tLoss: 0.001174\n",
            "Train Epoch: 10 [20352/21600 (94%)]\tLoss: 0.005496\n",
            "Train Epoch: 10 [20448/21600 (95%)]\tLoss: 0.012119\n",
            "Train Epoch: 10 [20544/21600 (95%)]\tLoss: 0.025087\n",
            "Train Epoch: 10 [20640/21600 (96%)]\tLoss: 0.000409\n",
            "Train Epoch: 10 [20736/21600 (96%)]\tLoss: 0.010940\n",
            "Train Epoch: 10 [20832/21600 (96%)]\tLoss: 0.002181\n",
            "Train Epoch: 10 [20928/21600 (97%)]\tLoss: 0.000763\n",
            "Train Epoch: 10 [21024/21600 (97%)]\tLoss: 0.000491\n",
            "Train Epoch: 10 [21120/21600 (98%)]\tLoss: 0.001965\n",
            "Train Epoch: 10 [21216/21600 (98%)]\tLoss: 0.033493\n",
            "Train Epoch: 10 [21312/21600 (99%)]\tLoss: 0.000202\n",
            "Train Epoch: 10 [21408/21600 (99%)]\tLoss: 0.029426\n",
            "Train Epoch: 10 [21504/21600 (100%)]\tLoss: 0.009952\n",
            "\n",
            "Validation set: Average loss: 0.0001, Accuracy: 5400/5400 (100%)\n",
            "\n",
            "\n",
            "Saved model to model_binary_10.pth.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net()\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqZEO0qX44-b",
        "outputId": "f7359e91-edb9-4318-9452-ed505de1d508"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (layers): Sequential(\n",
            "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (7): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (9): ReLU(inplace=True)\n",
            "    (10): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (12): ReLU(inplace=True)\n",
            "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): Dropout(p=0.5, inplace=False)\n",
            "    (1): Linear(in_features=133760, out_features=512, bias=True)\n",
            "    (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): Dropout(p=0.5, inplace=False)\n",
            "    (5): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (7): ReLU(inplace=True)\n",
            "    (8): Dropout(p=0.5, inplace=False)\n",
            "    (9): Linear(in_features=512, out_features=128, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rcParams['figure.figsize'] = [20, 20]\n",
        "plt.plot(losses_list)\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vXsR7oN6404h",
        "outputId": "8b4efab1-5f52-42f1-cb8f-bd9e535e9aff"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABIUAAARsCAYAAAAABybnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzd/+v/+z3H8fv9c85ZZmZ+2Idk5qD8gEKdVppfrEhIIlLIbyc/sYjyoz/Akh+U5XtIipWIrIxFoc9hxjY/SPPDqH0mOsPCzufhh8/79Xqv9XmfveV5e7/uz/O8XOq0c7ZP5/VsP167P+73XmsVAAAAAMdy79IfAAAAAMDdE4UAAAAADkgUAgAAADggUQgAAADggEQhAAAAgAMShQAAAAAO6OlLf8Anev3rX7+effbZS38GAAAAwCvGCy+88JG11v1P/u9HRaFnn322Hjx4cOnPAAAAAHjF6O5/fNJ/7/kYAAAAwAGJQgAAAAAHJAoBAAAAHJAoBAAAAHBAohAAAADAAYlCAAAAAAckCgEAAAAckCgEAAAAcECiEAAAAMABiUIAAAAAByQKAQAAAByQKAQAAABwQKIQAAAAwAGJQgAAAAAHJAoBAAAAHJAoBAAAAHBAohAAAADAAYlCAAAAAAckCgEAAAAckCgEAAAAcECiEAAAAMABiUIAAAAAByQKAQAAAByQKAQAAABwQKIQAAAAwAGJQgAAAAAHJAoBAAAAHJAoBAAAAHBAohAAAADAAYlCAAAAAAckCgEAAAAckCgEAAAAcECiEAAAAMABiUIAAAAAByQKAQAAAByQKAQAAABwQKIQAAAAwAGJQgAAAAAHJAoBAAAAHJAoBAAAAHBAohAAAADAAYlCAAAAAAckCgEAAAAckCgEAAAAcECiEAAAAMABiUIAAAAABxSNQt39we7+m+5+T3c/SP7WFL/73n+u7/7ZP6uP/fdLl/4UAAAAgBs9fQe/8bVrrY/cwe+M8E//9rH607//l3pprUt/CgAAAMCNPB8LWaIQAAAAMFg6Cq2q+oPufqG7nw//1gjdj/9TEgIAAAAmSz8f+5q11oe6+7Or6p3d/XdrrXd/4h+4ikXPV1W98Y1vDH8OAAAAAFXhSaG11oeu/vPDVfWOqnrTE/7M29daz621nrt//37yc+6U12MAAADAZLEo1N2v6e7Xnv6+qr6+qv429XtT9On9GAAAAMBgyedjn1NV77iKJE9X1a+ttX4/+HuzmBQCAAAABotFobXWP1TVV6T+/VOZEwIAAAD2wEn6kGVUCAAAABhMFNqYlUIAAADAHohCIa6PAQAAAJOJQhszKAQAAADsgSgUYlAIAAAAmEwU2lhfLRVa3o8BAAAAg4lCG7NoGgAAANgDUSjEnBAAAAAwmSi0MYNCAAAAwB6IQiFWCgEAAACTiUJbs1QIAAAA2AFRKGTZKgQAAAAMJgptzJwQAAAAsAeiUIpBIQAAAGAwUWhjVgoBAAAAeyAKhRgUAgAAACYThTbWV1uFnKQHAAAAJhOFNub5GAAAALAHolCIk/QAAADAZKLQxgwKAQAAAHsgCoXYKQQAAABMJgptzE4hAAAAYA9EoRCDQgAAAMBkotDG2lYhAAAAYAdEoZBlqRAAAAAwmCi0tatBIU0IAAAAmEwUAgAAADggUWhjNgoBAAAAeyAKbazdpAcAAAB2QBQKsVMIAAAAmEwU2pg5IQAAAGAPRKGQVUaFAAAAgLlEoY1ZKQQAAADsgSgUYqcQAAAAMJkotDGTQgAAAMAeiEIhBoUAAACAyUShjfXV/bHl/RgAAAAwmCi0Mc/HAAAAgD0QhULMCQEAAACTiUIAAAAAByQKhVgpBAAAAEwmCm2sLRUCAAAAdkAUijEqBAAAAMwlCm3MnBAAAACwB6JQiJ1CAAAAwGSi0MasFAIAAAD2QBQKMSgEAAAATCYKbayvtgp5PgYAAABMJgptzPMxAAAAYA9EoZDlARkAAAAwmCi0MYNCAAAAwB6IQiF2CgEAAACTiUIbs1MIAAAA2ANRKMSkEAAAADCZKLQ5o0IAAADAfKJQiOtjAAAAwGSi0MbsFAIAAAD2QBQKsVMIAAAAmEwU2phBIQAAAGAPRKGNtfdjAAAAwA6IQiGejwEAAACTiUIbMycEAAAA7IEoFOIkPQAAADCZKLQxK4UAAACAPRCFQuwUAgAAACYThTZmUggAAADYA1EoxKAQAAAAMJkotLF2fwwAAADYAVEoZFkqBAAAAAwmCm3talBIEgIAAAAmE4U25vEYAAAAsAeiUIjXYwAAAMBkotDG2k16AAAAYAdEoRijQgAAAMBcotDGzAkBAAAAeyAKhdgpBAAAAEwmCm3MSiEAAABgD0ShEINCAAAAwGSi0MbaViEAAABgB0ShEDuFAAAAgMlEoY2ddgotVQgAAAAYTBTamMdjAAAAwB6IQiHmhAAAAIDJRKGtGRUCAAAAdkAUCrFSCAAAAJhMFNqYk/QAAADAHohCIctWIQAAAGAwUWhjbVAIAAAA2AFRKMWgEAAAADCYKLSx06CQJgQAAABMJgptrL0fAwAAAHZAFApxkh4AAACYTBTamEEhAAAAYA9EoRAn6QEAAIDJRKGNGRQCAAAA9kAUCrFTCAAAAJhMFNqYnUIAAADAHohCIQaFAAAAgMlEoc0ZFQIAAADmE4VClqVCAAAAwGCi0MZOO4UkIQAAAGAyUWhjHo8BAAAAeyAKpRgVAgAAAAYThTbWbtIDAAAAOyAKhSyjQgAAAMBgotDGzAkBAAAAeyAKhbhIDwAAAEwmCm3MSiEAAABgD0ShEJNCAAAAwGSi0MbaViEAAABgB0ShEINCAAAAwGSi0MZOO4WW92MAAADAYKIQAAAAwAGJQiHmhAAAAIDJRKGNOUkPAAAA7IEoFGKlEAAAADCZKLQxJ+kBAACAPRCFYowKAQAAAHOJQhuzUwgAAADYA1EoxE4hAAAAYDJRaGMmhQAAAIA9EIVCDAoBAAAAk4lCGztdH/N8DAAAAJhMFNqY52MAAADAHohCIcsDMgAAAGAwUWhjBoUAAACAPRCFQuwUAgAAACYThTZmpxAAAACwB6JQiEEhAAAAYDJRaHNGhQAAAID5RKGQZakQAAAAMJgotDE7hQAAAIA9EIUAAAAADkgU2thpUMjrMQAAAGAyUWhj7f0YAAAAsAOiUMhylB4AAAAYTBTamDkhAAAAYA9EoRA7hQAAAIDJRKGNWSkEAAAA7IEoFGJSCAAAAJhMFNpY2yoEAAAA7IAoFGJQCAAAAJhMFNrYaafQ8n4MAAAAGEwUAgAAADggUSjEnBAAAAAwmSi0MSfpAQAAgD0QhVKMCgEAAACDiUIba6NCAAAAwA6IQiHLqBAAAAAwmCi0MXNCAAAAwB6IQiHLoBAAAAAwmCi0MSuFAAAAgD0QhUIMCgEAAACTiUIb66utQp6PAQAAAJOJQhvzfAwAAADYA1EoxEl6AAAAYDJRaGMGhQAAAIA9EIVC7BQCAAAAJhOFtmZUCAAAANgBUSjEoBAAAAAwmSi0sTYqBAAAAOyAKJRiqRAAAAAwmCi0sTYoBAAAAOyAKBRiTggAAACYTBTa2GlQyOsxAAAAYDJRaGPt/RgAAACwA6JQyDIqBAAAAAwmCm3MnBAAAACwB6JQiDkhAAAAYDJRaGNWCgEAAAB7IAqFWCkEAAAATCYKbaxtFQIAAAB2QBQKMSgEAAAATCYKbc2gEAAAALADolDIslQIAAAAGEwU2pjrYwAAAMAeiEIb04QAAACAPRCFQrweAwAAACYThTbW3o8BAAAAOyAKhSxH6QEAAIDBRKGNmRMCAAAA9kAUCrFTCAAAAJhMFNqYlUIAAADAHohCIQaFAAAAgMlEoY21rUIAAADADohCIXYKAQAAAJOJQhs77RRykh4AAACYTBQCAAAAOCBRKMTzMQAAAGCyeBTq7qe6+6+6+3fSvzWBk/QAAADAHtzFpNAPVtUH7uB3AAAAALilaBTq7jdU1TdV1c8mf2cSJ+kBAACAPUhPCv1kVf1oVT266Q909/Pd/aC7Hzx8+DD8OXdnWSoEAAAADBaLQt39zVX14bXWCy/359Zab19rPbfWeu7+/fupz7kzdgoBAAAAe5CcFHpzVX1Ld3+wqn69qt7S3b8S/L1RDAoBAAAAk8Wi0Frrx9Zab1hrPVtV31VVf7jW+p7U701hUAgAAADYg7u4PnZIBoUAAACAyZ6+ix9Za/1RVf3RXfzWpfXVUiHPxwAAAIDJTAptzPMxAAAAYA9EoZDlARkAAAAwmCi0MSfpAQAAgD0QhULsFAIAAAAmE4U21kaFAAAAgB0QhUIMCgEAAACTiUIAAAAAByQKpVgqBAAAAAwmCgV0ez4GAAAAzCYKBVg1DQAAAEwnCoV4PQYAAABMJgoFOEsPAAAATCcKhSxbhQAAAIDBRKEAc0IAAADAdKJQiJ1CAAAAwGSiUICVQgAAAMB0olCIQSEAAABgMlEooG0VAgAAAIYThULsFAIAAAAmE4US2kl6AAAAYDZRKMDjMQAAAGA6USjFoBAAAAAwmCgU4CQ9AAAAMJ0oFGJQCAAAAJhMFApwkh4AAACYThQKWW7SAwAAAIOJQgF2CgEAAADTiUIhBoUAAACAyUShAINCAAAAwHSiUIhBIQAAAGAyUSiguz0fAwAAAEYThQI8HwMAAACmE4VClgdkAAAAwGCiUIJRIQAAAGA4USjETiEAAABgMlEowKAQAAAAMJ0oBAAAAHBAolBAt1khAAAAYDZRKGRZKgQAAAAMJgoFGBQCAAAAphOFQswJAQAAAJOJQgFdTtIDAAAAs4lCARZNAwAAANOJQiHLAzIAAABgMFEowJwQAAAAMJ0oFGKnEAAAADCZKBRgpRAAAAAwnSgUYlAIAAAAmEwUijAqBAAAAMwmCoXYKQQAAABMJgoF2CkEAAAATCcKxRgVAgAAAOYShQK6PB8DAAAAZhOFAjwfAwAAAKYThUJMCgEAAACTiUIB7SQ9AAAAMJwoFLIsmgYAAAAGE4UC7BQCAAAAphOFQuwUAgAAACYThQIMCgEAAADTiUIhBoUAAACAyUShgO72fAwAAAAYTRQCAAAAOCBRKMRJegAAAGAyUSjASXoAAABgOlEoxaAQAAAAMJgoFGBSCAAAAJhOFAoxKAQAAABMJgoFdBkVAgAAAGYThULWMisEAAAAzCUKBdgpBAAAAEwnCoWYEwIAAAAmE4UCuqq8HgMAAAAmE4UC2vsxAAAAYDhRKMSgEAAAADCZKBRgTggAAACYThQKcZIeAAAAmEwUSjAqBAAAAAwnCoWYEwIAAAAmE4UCDAoBAAAA04lCKUaFAAAAgMFEoYBus0IAAADAbKJQyDIqBAAAAAwmCgV0VblIDwAAAEwmCgV4PQYAAABMJwqFmBQCAAAAJhOFAtpRegAAAGA4USjEomkAAABgMlEowE4hAAAAYDpRKMROIQAAAGAyUQgAAADggEShEINCAAAAwGSiUEBbKgQAAAAMJwqF2CkEAAAATCYKBTyeE1KFAAAAgLlEoQCvxwAAAIDpRKEQz8cAAACAyUShAJNCAAAAwHSiUIhBIQAAAGAyUSigy6gQAAAAMJsoFLIsFQIAAAAGE4UC7BQCAAAAphOFQswJAQAAAJOJQgEGhQAAAIDpRKEQK4UAAACAyUShhG7PxwAAAIDRRKEAz8cAAACA6UShECfpAQAAgMlEoQAn6QEAAIDpRCEAAACAAxKFAgwKAQAAANOJQiFWCgEAAACTiUIBbakQAAAAMJwoFLLKqBAAAAAwlygU0OX5GAAAADCbKBTg9RgAAAAwnSgUYlIIAAAAmEwUCmhH6QEAAIDhRKEQi6YBAACAyUShBINCAAAAwHCiUIidQgAAAMBkolCAQSEAAABgOlEoxKAQAAAAMJkoFNBGhQAAAIDhRKEUo0IAAADAYKJQQFc7SQ8AAACMJgoFeD4GAAAATCcKhThJDwAAAEwmCgWYFAIAAACmE4VCDAoBAAAAk4lCAV1GhQAAAIDZRKGQZakQAAAAMJgoFGCnEAAAADCdKBRiTggAAACYTBQCAAAAOCBRKMRKIQAAAGAyUSiguz0fAwAAAEYThQLsmQYAAACmE4VSvB8DAAAABhOFApykBwAAAKYThULMCQEAAACTiUIBBoUAAACA6UShECuFAAAAgMlEoYC2VAgAAAAYThQKWbYKAQAAAIOJQgHmhAAAAIDpRKEQO4UAAACAyUShgG5RCAAAAJhNFIrwgAwAAACYTRQKMSgEAAAATCYKBbhIDwAAAEwnCoUsS4UAAACAwUShAINCAAAAwHSiEAAAAMABiUIBdgoBAAAA04lCIVYKAQAAAJOJQgFtqxAAAAAwnCgUssqoEAAAADCXKBTQ7fkYAAAAMJsoFGDRNAAAADCdKBRiUAgAAACYTBQKsGgaAAAAmE4UClmWCgEAAACDiUIJBoUAAACA4UShEHNCAAAAwGSiUIBBIQAAAGA6USjFqBAAAAAwmCgU0G1WCAAAAJhNFAoxKAQAAABMJgoFdDlJDwAAAMwmCgV4PQYAAABMJwqFmBMCAAAAJhOFAgwKAQAAANOJQiFWCgEAAACTiUIBTtIDAAAA04lCIctWIQAAAGAwUSjAnBAAAAAwnSgUYqcQAAAAMJkolNCiEAAAADCbKBTQHpABAAAAw4lCAAAAAAckCgW4SA8AAABMJwqFLEuFAAAAgMFEoQCDQgAAAMB0olCIOSEAAABgMlEowE4hAAAAYDpRKMRKIQAAAGAyUSigbRUCAAAAhotFoe7+tO7+i+7+6+5+X3f/eOq3Jlq2CgEAAACDPR38d/9XVb1lrfXv3f1MVf1Jd//eWuvPgr85QrfnYwAAAMBssSi01lpV9e9X//jM1V+HSCUWTQMAAADTRXcKdfdT3f2eqvpwVb1zrfXnT/gzz3f3g+5+8PDhw+Tn3KlD1C8AAABgt6JRaK310lrrK6vqDVX1pu7+8if8mbevtZ5baz13//795OfcIaNCAAAAwGx3cn1srfVvVfWuqvqGu/i9CewUAgAAACZLXh+7392fdfX3r66qr6uqv0v93iR2CgEAAADTJa+PfW5V/VJ3P1WP49NvrLV+J/h7wxgVAgAAAOZKXh97b1V9VerfP5lBIQAAAGC6O9kpdER2CgEAAACTiUIBdgoBAAAA04lCIQaFAAAAgMlEoYCuruX9GAAAADCYKBTg+RgAAAAwnSgUYk4IAAAAmEwUCjAoBAAAAEwnCoVYKQQAAABMJgoFtKVCAAAAwHCiUIjrYwAAAMBkohAAAADAAYlCIeaEAAAAgMlEoQArhQAAAIDpRKEUo0IAAADAYKJQQFdrQgAAAMBoolCA52MAAADAdKJQiJP0AAAAwGSiUIBBIQAAAGA6USjEnBAAAAAwmSgU0F3l9RgAAAAwmSgU0N21zAoBAAAAg4lCAV0mhQAAAIDZRKGEtlMIAAAAmE0UCmhVCAAAABhOFAroLjuFAAAAgNFEoQA7hQAAAIDpRKGA9noMAAAAGE4UCujqWkaFAAAAgMFEoQCTQgAAAMB0olCAnUIAAADAdKJQQvelvwAAAADgZYlCAackZK8QAAAAMJUoFHAaFNKEAAAAgKlEoYC+mhXShAAAAICpRKGA60khWQgAAACYSRQKOO8UuuhXAAAAANxMFAqwUwgAAACYThQK6D7tFFKFAAAAgJlEoSCTQgAAAMBUolDA6fkYAAAAwFSiUMD5JL1JIQAAAGAoUSjgvGjaTiEAAABgKFEo4HySXhMCAAAAhhKFAq4nhQAAAABmEoUCrncKyUIAAADATKJQgEkhAAAAYDpRKKDb9TEAAABgNlEo4HrRtCoEAAAAzCQKBZyfj2lCAAAAwFCiUMB5UuiiXwEAAABwM1Eo4HqnkCwEAAAAzCQKBbg+BgAAAEwnCgVcL5q+6GcAAAAA3EgUSjg9HzMrBAAAAAwlCgWcJoU0IQAAAGAqUSjATiEAAABgOlEooOt0fezCHwIAAABwA1Eo4HpSSBUCAAAAZhKFAlwfAwAAAKYThQLsFAIAAACmE4UCrncKyUIAAADATKJQwmlSSBMCAAAAhhKFAvpT/xEAAACAixKFArqdpAcAAABmE4UCztfHrJoGAAAAhhKFAtpOIQAAAGA4USjASXoAAABgOlEowEl6AAAAYDpRKMCkEAAAADCdKBRkUAgAAACYShQKOJ2kNysEAAAATCUKBZyTkCYEAAAADCUKBdgpBAAAAEwnCgVcXx+78IcAAAAA3EAUCrieFFKFAAAAgJlEoQA7hQAAAIDpRKGA86SQKAQAAAAMJQpFXO0U8nwMAAAAGEoUCjApBAAAAEwnCgX0p/4jAAAAABclCgV0O0kPAAAAzCYKBZyvj9kpBAAAAAwlCgXYKQQAAABMd6so1N2v6e57V3//Jd39Ld39TPbT9uschS77GQAAAAA3uu2k0Lur6tO6+/Oq6g+q6nur6hdTH7V3fTpJb1QIAAAAGOq2UajXWv9ZVd9WVT+91vqOqvqy3GftnEkhAAAAYLhbR6Hu/uqq+u6q+t2r/+6pzCft33nRtCoEAAAADHXbKPTWqvqxqnrHWut93f1FVfWu3Gft2+kkvVkhAAAAYKqnb/OH1lp/XFV/XFV1tXD6I2utH0h+2J6ZFAIAAACmu+31sV/r7s/s7tdU1d9W1fu7+0eyn7Zfro8BAAAA0932+diXrrVerKpvrarfq6ovrMcXyHiC6+tjF/4QAAAAgBvcNgo9093P1OMo9Ntrrf8pgzA3Ok8KqUIAAADAULeNQj9TVR+sqtdU1bu7+wuq6sXUR+2dNdMAAADAdLddNP1TVfVTn/Bf/WN3f23mk14BzpNCl/0MAAAAgJvcdtH067r7bd394Oqvn6jHU0M8wXmnkFkhAAAAYKjbPh/7+ar6aFV959VfL1bVL6Q+au/a+zEAAABguFs9H6uqL15rffsn/POPd/d7Eh/0SqAJAQAAANPddlLoY939Nad/6O43V9XHMp+0f/fuOUkPAAAAzHbbSaHvr6pf7u7XXf3zv1bV92U+af9Ok0KPVCEAAABgqNteH/vrqvqK7v7Mq39+sbvfWlXvTX7cXp12CklCAAAAwFS3fT5WVY9j0Frrxat//KHA97xCnJ6PyUIAAADATP+nKPRJ+lP/kWMyKQQAAABM9/+JQprHDc61zP9DAAAAwFAvu1Oouz9aT04bXVWvjnzRK0BfjQotVQgAAAAY6mWj0FrrtXf1Ia8kp0khK4UAAACAqf4/z8e4wXmnkCgEAAAADCUKBfTp+tiFvwMAAADgJqJQwPWkkCwEAAAAzCQKBUlCAAAAwFSiUICdQgAAAMB0olBAX98fu+h3AAAAANxEFAowKQQAAABMJwoFnKPQZT8DAAAA4EaiUMD5JL0qBAAAAAwlCgVcTwqpQgAAAMBMolDAec20JgQAAAAMJQoF2CkEAAAATCcKRZx2CslCAAAAwEyiUMBpUggAAABgKlEowE4hAAAAYDpRKKCvRoVcHwMAAACmEoUCTAoBAAAA04lCAefrY6IQAAAAMJQoFNCn62MX/g4AAACAm4hCAdeTQrIQAAAAMJMoFCQJAQAAAFOJQgF93jR90c8AAAAAuJEoFOAkPQAAADCdKBTgJD0AAAAwnSgUcF40fdnPAAAAALiRKBRwPkmvCgEAAABDiUIB15NCqhAAAAAwkygUYKcQAAAAMJ0olGCnEAAAADCcKBTQ5yokCwEAAAAziUIBro8BAAAA04lCAXYKAQAAANOJQgHdp5P0qhAAAAAwkygUcJ4UuuhXAAAAANxMFApoe6YBAACA4UShgNP1MU0IAAAAmEoUSjhPCslCAAAAwEyiUMDp+RgAAADAVKJQgJP0AAAAwHSiUMD5JL2tQgAAAMBQolCASSEAAABgOlEo4HyS/rKfAQAAAHAjUSjgfJJeFQIAAACGEoUCrieFVCEAAABgJlEo4ByFNCEAAABgKFEo4Pr5mCoEAAAAzCQKBZgUAgAAAKYThQLOJ+kv+hUAAAAANxOFArpdHwMAAABmE4UCrieFVCEAAABgJlEowE4hAAAAYDpRKOD8fOzC3wEAAABwE1EoyagQAAAAMJQoFNJtUggAAACYSxQK6TIoBAAAAMwlCoV0t+tjAAAAwFiiUIhJIQAAAGAyUSjETiEAAABgMlEopKtNCgEAAABjiUIpXXYKAQAAAGOJQiFd5f0YAAAAMJYoFGKnEAAAADCZKBTyeKeQLAQAAADMJAqFdDtJDwAAAMwlCoV0eT4GAAAAzCUKhXQ7SQ8AAADMJQqFPJ4UUoUAAACAmUShFDuFAAAAgMFEoZC+9AcAAAAAvAxRKOTxTiGjQgAAAMBMolBIt+tjAAAAwFyiUEiXnUIAAADAXKJQSHe7PgYAAACMJQqFmBQCAAAAJhOFQuwUAgAAACYThWLapBAAAAAwligU0l1lVggAAACYShQKsVMIAAAAmEwUCukWhQAAAIC5RKGQLifpAQAAgLlEoRCTQgAAAMBkolBIlzXTAAAAwFyiUEi3k/QAAADAXLEo1N2f393v6u73d/f7uvsHU781lZ1CAAAAwFRPB//dH6+qH15r/WV3v7aqXujud6613h/8zTHa+zEAAABgsNik0Frrn9daf3n19x+tqg9U1eelfm+a7kt/AQAAAMDN7mSnUHc/W1VfVVV//oT/7fnuftDdDx4+fHgXn3NnDAoBAAAAU8WjUHd/RlX9ZlW9da314if/72utt6+1nltrPXf//v3059yZLqNCAAAAwFzRKNTdz9TjIPSra63fSv7WRMv5MQAAAGCo5PWxrqqfq6oPrLXelvqdqewUAgAAACZLTgq9uaq+t6re0t3vufrrG4O/N445IQAAAGCq2En6tdafVB13sU5XlddjAAAAwFR3cn3siNr7MQAAAGAwUSjIoBAAAAAwlSgUYk4IAAAAmEwUCnKSHgAAAJhKFEppz8cAAACAuUShEM/HAAAAgMlEoSSjQgAAAMBQolCIk/QAAADAZKJQ0DIqBAAAAAwlCoV0VTk+BgAAAEwlCoV4PQYAAABMJgoFmRQCAAAAphKFQtpRegAAAGAwUSjIomkAAABgKlEoxAUIwwgAACAASURBVE4hAAAAYDJRKMhOIQAAAGAqUShIEwIAAACmEoVC2vsxAAAAYDBRKMjzMQAAAGAqUSjEnBAAAAAwmSgUZVQIAAAAmEkUCun2fAwAAACYSxQKsWcaAAAAmEwUCjIoBAAAAEwlCoW0VdMAAADAYKJQ0LJUCAAAABhKFArp9nwMAAAAmEsUAgAAADggUSiky0l6AAAAYC5RKMVNegAAAGAwUSjIoBAAAAAwlSgUYk4IAAAAmEwUCnKSHgAAAJhKFAqxUggAAACYTBQK0YQAAACAyUShIK/HAAAAgKlEoZD2fgwAAAAYTBQKWo7SAwAAAEOJQiFdno8BAAAAc4lCIV6PAQAAAJOJQkEmhQAAAICpRKGQdpQeAAAAGEwUCrJoGgAAAJhKFEoxKAQAAAAMJgoF2SkEAAAATCUKhXSVx2MAAADAWKJQiJP0AAAAwGSiUJJRIQAAAGAoUSjESXoAAABgMlEoyEl6AAAAYCpRKKTb9TEAAABgLlEoxKJpAAAAYDJRKMigEAAAADCVKBRi0TQAAAAwmSgUtCwVAgAAAIYShUK6PR8DAAAA5hKFAAAAAA5IFAryegwAAACYShQKaTfpAQAAgMFEoSCDQgAAAMBUolCIOSEAAABgMlEoyVIhAAAAYChRKMRJegAAAGAyUSjE8zEAAABgMlEoyOsxAAAAYCpRKMRJegAAAGAyUSho2SoEAAAADCUKhXR5PgYAAADMJQqFeD0GAAAATCYKBZkUAgAAAKYShWKMCgEAAABziUJBBoUAAACAqUShkO6q5f0YAAAAMJQoFOLxGAAAADCZKAQAAABwQKJQiJP0AAAAwGSiUJCVQgAAAMBUolBI2yoEAAAADCYKBS1H6QEAAIChRKGQxyfpL/0VAAAAAE8mCoVYNA0AAABMJgoFGRQCAAAAphKFQiyaBgAAACYThYKWpUIAAADAUKJQSns+BgAAAMwlCoV4PAYAAABMJgolGRUCAAAAhhKFQtpNegAAAGAwUSjIoBAAAAAwlSgU0uX6GAAAADCXKBTSro8BAAAAg4lCIfe6y6AQAAAAMJUoFNJV9UgVAgAAAIYShULapBAAAAAwmCgUcq8tmgYAAADmEoVCuqseaUIAAADAUKJQyL3uWu6PAQAAAEOJQiEmhQAAAIDJRKEQi6YBAACAyUShEIumAQAAgMlEoZCurkeiEAAAADCUKBRyr8uaaQAAAGAsUSiku+uRTdMAAADAUKJQSJsUAgAAAAYThULuuT4GAAAADCYKhXSVRdMAAADAWKJQyL17JoUAAACAuUShkG6TQgAAAMBcolBIl0khAAAAYC5RKOReVy33xwAAAIChRKGQx8/HLv0VAAAAAE8mCoU8PkmvCgEAAAAziUIh3W1SCAAAABhLFArpq/80LQQAAABMJAqF3OvHWUgTAgAAACYShUKumlA9UoUAAACAgUShkHtXUUgSAgAAACYShUL6alTIpBAAAAAwkSgUcno+pgkBAAAAE4lCIRZNAwAAAJOJQiGnk/SejwEAAAATiUIh9+wUAgAAAAYThULa9TEAAABgMFEo5HR9bD268IcAAAAAPIEoFHLvPClkVggAAACYRxQKOe0UeumRKAQAAADMIwqFvOrpx//X/s9LohAAAAAwjygU8umveqqqqv7jvz9+4S8BgP9l7z4D5KbOhY8/ml3bBEIICaSTOAmQegMkkFwSksClh3QCaZdA2ntTCQkkMTUJvRhMx6aZDqYbWNu493Vfr70ua3vt3XXd4vX2NkXvhxlpJI2kkWZGM9rd/++L1zMa6YxmRjp69JznAAAAAJkICgXkkNHlIiLSOxAvcUsAAAAAAAAyERQKyMFjyBQCAAAAAADhRVAoIFqmUM8AQSEAAAAAABA+BIUCMqqMQtMAAAAAACC8CAoFpCzClPQAAAAAACC8CAoFRAsKxRKJErcEAAAAAAAgE0GhgJSngkIJlUwhAAAAAAAQPgSFAqJnClFTCAAAAAAAhBBBoYBQUwgAAAAAAIQZQaGAaMPH4gwfAwAAAAAAIURQKCARMoUAAAAAAECIERQKSDlBIQAAAAAAEGIEhQJCTSEAAAAAABBmBIUCos8+RlAIAAAAAACEEEGhgJApBAAAAAAAwoygUEDKFIJCAAAAAAAgvAgKBYRMIQAAAAAAEGYEhQKiKIqURRSCQgAAAAAAIJQICgWoTFEoNA0AAAAAAEKJoFCQFBFVCAoBAAAAAIDwISgUoIgiQkwIAAAAAACEEUGhACmiSEIlKgQAAAAAAMKHoFCAFEWEmBAAAAAAAAgjgkIBYvQYAAAAAAAIK4JCAVIUhUwhAAAAAAAQSgSFAqQw+xgAAAAAAAgpgkIBUoSaQgAAAAAAIJwICgUoOXyMqBAAAAAAAAgfgkIBSg4fAwAAAAAACB+CQgFi+BgAAAAAAAgrgkIBiigKhaYBAAAAAEAoERQKkKKIJIgJAQAAAACAECIoFCiF4WMAAAAAACCUCAoFSFFEKDUNAAAAAADCiKBQgCIKhaYBAAAAAEA4ERQKkCKKJIgKAQAAAACAECIoFCCFTCEAAAAAABBSBIUCpAgVhQAAAAAAQDgRFAqQojD7GAAAAAAACCeCQgFSFBGVXCEAAAAAABBCBIUCRE0hAAAAAAAQVgSFAqSIIipRIQAAAAAAEEIEhQKUHD4GAAAAAAAQPgSFAqQIw8cAAAAAAEA4ERQKUERRyBQCAAAAAAChRFAoSIpIglQhAAAAAAAQQgSFAqSIUFQIAAAAAACEEkGhACmKIipRIQAAAAAAEEIEhQJEoWkAAAAAABBWBIUCFFEUagoBAAAAAIBQIigUIEUhUwgAAAAAAIQTQaGAERMCAAAAAABhRFAoQIqikCkEAAAAAABCiaBQgCLMSQ8AAAAAAEKKoFCAFEUkQUwIAAAAAACEEEGhACmiiMr4MQAAAAAAEEIEhQKkKAweAwAAAAAA4URQKECKMCU9AAAAAAAIJ4JCAVIUhUwhAAAAAAAQSgSFAqQoQk0hAAAAAAAQSoEFhRRFeVxRlGZFUWqC2kbYMXwMAAAAAACEVZCZQk+IyDkBrj/0ksPHiAoBAAAAAIDwCSwopKrqQhFpC2r9QwGZQgAAAAAAIKxKXlNIUZT/pyjKKkVRVrW0tJS6OQUVURSCQgAAAAAAIJRKHhRSVfVhVVVPVFX1xCOPPLLUzSksRSRBVAgAAAAAAIRQyYNCw5kiQkUhAAAAAAAQSgSFApQcPkZYCAAAAAAAhE+QU9I/LyKVIvIpRVF2KYry66C2FVaRiEiCmBAAAAAAAAih8qBWrKrqT4Na91ARURSJExUCAAAAAAAhxPCxAJVFFApNAwAAAACAUCIoFKAyMoUAAAAAAEBIERQKUCRCUAgAAAAAAIQTQaEAlSkMHwMAAAAAAOFEUChAZWQKAQAAAACAkCIoFKBIRBEShQAAAAAAQBgRFApQRBGJExUCAAAAAAAhRFAoQMw+BgAAAAAAwoqgUIAiEUX2dw/K8dfPlOqd7aVuDgAAAAAAgI6gUIDKFEX6onFp743KxAV1pW4OAAAAAACAjqBQgCIRpdRNAAAAAAAAsEVQKEBl7F0AAAAAABBShC0CVKaQKQQAAAAAAMKJoFCAjMPHmJkeAAAAAACECUGhAEUMmUJtPYPS2R8tYWsAAAAAAADSCAoFqMyQKbSivk1OvGF2CVsDAAAAAACQRlAoQBFLTaHBeKJELQEAAAAAADAjKBQgZh8DAAAAAABhRdgiQNZMIQAAAAAAgLAgKBQgQkIAAAAAACCsCAoFiUwhAAAAAAAQUgSFAhQhJgQAAAAAAEKKoFCAFAaQAQAAAACAkCIoFCBGjwEAAAAAgLAiKBQgho8BAAAAAICwIigUIIVUIQAAAAAAEFIEhQJETAgAAAAAAIQVQaEAUWgaAAAAAACEFUGhAJEpBAAAAAAAwoqgUIAoNA0AAAAAAMKKoFCAGD4GAAAAAADCiqBQgBg+BgAAAAAAwoqgUICYkh4AAAAAAIQVQaEAERICAAAAAABhRVAoQBSaBgAAAAAAYUVQKEAMHwMAAAAAAGFFUChAxIQAAAAAAEBYERQKUFNnf6mbAAAAAAAAYIugUICicbXUTQAAAAAAALBFUAgAAAAAAGAEIigEAAAAAAAwAhEUChB1pgEAAAAAQFgRFAoSUSEAAAAAABBSBIUCpBAVAgAAAAAAIUVQCAAAAAAAYAQiKBQghUQhAAAAAAAQUgSFAkRMCAAAAAAAhBVBoQCppW4AAAAAAACAA4JCAVKJCgEAAAAAgJAiKBQglVwhAAAAAAAQUgSFAkSmEAAAAAAACCuCQgFSiQoBAAAAAICQIigUIGJCAAAAAAAgrAgKBYiYEAAAAAAACCuCQgEiUwgAAAAAAIQVQaEAJYgKAQAAAACAkCIoBAAAAAAAMAIRFAoQs48BAAAAAICwIigUIEJCAAAAAAAgrAgKBeij7zk447ENezpK0BIAAAAAAAAzgkIB+tXXPp7xWHPXQAlaAgAAAAAAYEZQKECRiJL5IGPKAAAAAABACBAUAgAAAAAAGIEIChWZSqoQAAAAAAAIAYJCRcYs9QAAAAAAIAwIChUZQSEAAAAAABAGBIWKjJgQAAAAAAAIA4JCRaaSKgQAAAAAAEKAoFCRJYgJAQAAAACAECAoFLDF/zxNfnDCh0vdDAAAAAAAABOCQgH7yOEHy3sOGW14hFQhAAAAAABQegSFiqxQJYVi8YRcMnmFrG5oK8wKAQAAAADAiEJQqAgUw9+FyhPa094v82tb5C8vrC3QGgEAAAAAwEhCUKgIFENUqNCTjxnXDQAAAAAA4BVBoSJQDJEb1SFXqG8wLluaujyv02k9AAAAAAAAXhAUKgLT8DGHWM5lU6rkrAkLpWcg5nPdpAoBAAAAAAD/CAqFxIodyYLRA7GEp+ULPQwNAAAAAACMLASFisFYU8hhkUhqiFk84S3aoy1FTSEAAAAAAJALgkJFYBzipTqk+EQiyWUSPlOAiAkBAAAAAIBcEBQqgk8ccYj+t1PMJxUT8h0UAgAAAAAAyAVBoSK44MSP6H87zRrme/gYwSMAAAAAAJAHgkJFYJqS3jFTSHF93ipdU4gBZAAAAAAAwD+CQkXmGBRKfRJeM4U0hIQAAAAAAEAuCAqFRFkq4yfmMSi0p70vyOYAAAAAAIBhjqBQkTlOSR/xXlNodcMBueixFcn/kCoEAAAAAAByQFCoyBynpNczhRJZ17GtuaugbQIAAAAAACMPQaEi00JCtfu65KnKev3xMh+zjxmLS5MoBAAAAAAAclFe6gaMNNF4MhPo7LsXiojIRf/9MVEURR8+5qWmkDEQxOxjAAAAAAAgF2QKFVnfYNz0/x9NrBQRkVRMyHemEAAAAAAAQC4IChVZryUotLrhgIiIlKWiQlomkRtCQgAAAAAAIF8EhYrMGhTSaIEehzrU5mUNUSECRAAAAAAAIBcEhYqsdzDm+rzfoBAAAAAAAEAuCAoV2UDUfXiYKl4KTRtmHyNABAAAAAAAckBQqMgGs9QM8lBnmkAQAAAAAADIG0GhInutard0DzgPIVO9jB8zUKgqBAAAAAAAclBe6gaMRA/N32b6/9hxFXLoQcmPwktIiCnpAQAAAABAvsgUKgG7EWRd/ansIS/Dx4x/Ex8CAAAAAAA5ICgUMgkPw8cIBAEAAAAAgHwRFAoZT1PSU0cIAAAAAADkiaBQyHirKRR4MwAAAAAAwDBHUKgE3II6noaPmdZFhAgAAAAAAPhHUChkPA0fM8SBvIaEVFWVeMLfdPcAAAAAAGD4IigUOp4GkPle65+fr5JPXjXNf3MAAAAAAMCwRFCoBNxCOl6SeSLGTCGP8aG31u31tiAAAAAAABgRCAqFjDZ8bPbGJpm4oM52GeoIAQAAAACAfBEUKoGFW1scn1NTw8d+89QquXX65qzrisVVmbSgTqLxRMHaBwAAAAAAhj+CQiVQu6/L8TlPhaaN62rqklumb5anKhvyb5hjm1T5ys2z5cWVOwPbBgAAAAAAKC6CQiUQjTtHfrxMSR+x+dR6B2L5NMlVPKFKU+eAjHt1XWDbAAAAAAAAxUVQaAhScph9DAAAAAAAwIigUMh4GT5W7JiQ1iQKXAMAAAAAMHwQFCqSS746Vt4xqizrcl6GjxVbCJsEAAAAAADyRFCoSP793c/JphvOybpcLKFKIpGOwlz12vqMZSJFzthRhagQAAAAAADDDUGhIjvy0DGuz//j5XVy8eQV+v+fW97oexu3z9gsd7ydfTp7r7RMIQaPAQAAAAAwfBAUKrLxFxyXdZlFW1szHuvqj0rj/l4RSU4R7+bB+XXywLw6h+e2eWilGcPHAAAAAAAYfggKFVlZjkO/vv/AEvnGHfNERPIazHX7jFrfr2H4GAAAAAAAww9BoSKL5LjH61p6RCSZJTRxvn0WUFD04WOMHwMAAAAAYNggKFRkuWYKabY0dcvyHW0ZjweZy0OeEAAAAAAAww9BoSKLRPILCsUSiQK1xLtsNYwAAAAAAMDQQ1CoyPKdTl4pwRxgCX32McaPAQAAAAAwXBAUKrKyPDOFnGJKXf1R2d89kNe6HZEoBAAAAADAsENQqMjyrSk07tX1to8/smiHfOnG2Xmt20khZh/b2tQlL67aWYDWAAAAAACAQiAoVGS5zj6mqd7Z7nnZlq4BmVfb7LrM+l0dssKmcLWRXlIoj3jWmRMWyj9eXpf7CgAAAAAAQEERFCqyXIaPdfRGc9rWTx6ulF9OXimJhHOmz3fuXywXTqp0XQ+jxwAAAAAAGH4IChVZLsPHfvv0qpy2VdfSIyL5B3WYfQwAkK91u9pl+fb9pW4GAAAADAgKFZmSQ1Codl9XXtuM22QKLdraIrG4t+nt07OPAQCQm+/ev0R+/PCyUjcDAAAABgSFiiyX4WP90XhO29LiTwlLps/ira1y0WMr5IF5dZ7WU4hC0wAAAAAAIFwIChVZLsPHBmLeMnqcWEd/NXX2i4hIw/4ejyvIa/PAiFK9s13qWrpL3QwAAAAAyKq81A0YafKdfSyb7oFYxmPWTCG/iAkB3n3vgSUiIlJ/63klbgkAAAAAuCNTqMhyGT7mx8T5mUPC4vkGhbSaQhQVAgAAAABg2CAoVGSjyoLd5YOG4tFaDEe1jD7zGyKiphAAAAAAAMMPQaEiCzooZDfTmHX4mD7FvMfMn/TsY6QKAQAAAAAwXBAUKrIx5aUPCmm8BnnUPIefAQAAAACA8CEoVGRBZwrN3dyc8Zi1plC2EM/rVbtlR2t6ZjJiQgAAAAAADD8EhYrMWmj6irOOLej6G9t6Mx5zCuo4FY6+bMpaOXvCwgK2CgAAAAAAhA1BoRL70ZeOCmzdSirqkzF8zEPmj7FgdSFnH2MoGgAAAAAA4UBQqMSKMc27TZkhERF5efUuT693mn3s7tlbfGcUERMCAAAAACAcykvdgJGuGPN5JZyiQl5f7zBZ2d2zt+a1XgAAAAAAUDpkCpVaUTKFrIWmM4NE82szC1TryxcwvYdEoaHvhrc2ythxFaVuBgAAAAAgTwSFSuC1P3xV/zsS4Pgxbc1eEoUumbxS1jQesH2ukIEcagoNfY8t3lHqJgAAAAAACoCgUAl8+gPv0v8uyvAxa6aQQ1ymoy9qG7QpZBxHW9VgLCFjx1XIvXMYggYA8GZHa4/0R+OlbgYAAMCwQVCoBIzJQUoRKk1bAz2PLNpuu1x5RHEIABVw+FhqVZv3dYqIyKMObQEAwCgaT8hp4+fLn55bU+qmAAAADBsEhUrAFBRyWe7ikz9WkO0ZZpcXEZG6lh7b5cojEdvwT3pK+vwDWFo9o+/evyTvdeVixY42WbilpSTbBgDkLp4aC71oa2uJWwIAADB8EBQqAcUQCjLGWeZdcap5uTyDMNrLrcPHnJSXKbbDx5xmH8tFqUsKXTipUn7x+IrSNgIAAAAAgBAgKFQC5kyh9H8+fsQhBd1ONJ6MwHgNCpVFFNui1Fp2T9dATL9TCwBh9sC8bXLXrC2lbsaIt6q+jRpAAAAAIUZQqAQiNuPHyiPJP/7vm58w/b8QvGbnKGI/Xb3x9be/vbkwjQKAAN3xdi2F7Ets14Fe+dHEShn3yrqCrK/UmaYAAADDEUGhErAL94wpT34U/+/ryaBQpIBBIa/ZPQnVvtNtfGzSAn+Foe+evUXGjqvQ/983GCfbCABCav2uDmnpGijIuroHYiIismlvV0HWZ3fTApmufHW9/OWFqlI3AwAADBEEhUrAmCgUS1WBHjOqTEREysuSH0kiocrnP3xYQbbndfiYqqr2QaE8OuJ3zzbfqT/hhlly2ZS1+v+LMfsagmFXfwrA0Pad+xfLufcsKnUzbHHI8eb5FY0yde2eUjcDAAAMEQSFSsAYCDn84NHywxM+LI9dfKKIpIeNxVVVzv/ih2X2377heb3fPe5Dto97TcyJJ1TbAFBl3X7PbfDizep0Z5XAwtDFR1dY+7sH5DPXzpCqxgOlbgpGuNbuwmQKFRqHHAAAgMIjKFRikYgid/34eDnho4eLSHIGMJHkBbeiKHL0+w71vK7R5fYf54DHIp9Ow8durNjkuQ0YObhAK6xl29ukLxqXRxb5G6JZCP1RhnUi/LiJAAAAUHgEhUKmPJL7R+IUFDrQG/X0+oSqmoaavVa1y9f2u/qjMnZchby4cqfn1zB8bOjiAm34+PS1M+TS56lBAntT1+6W+taeUjeDQDQAAC52HeiVrn5v132AEUGhkNHqSzsFeNyMLrN/TVvvoKfXJ1Tz4LHxb/ubznlfR7+IiExaWOfrdUh6rWqXnHTT7CGTsTE0Wgmj6ev3ytJtrbbPVazfW+TWpH3rnkVy3H9mlmz7cPeXF9YWpc5QR19Unq6sl4TDMVBNBN4EAACGrFNumyffe2BJqZuBIai81A0Yya4469iMxxRFkX9/57Py1aOP8L0+p0BSd3/M0+vjCXOhaa+ZIKqqiqIoegFtggX2OnqjUl6myCFj7H92V79WI72DcemLxuWdDsuECYlCwQhyv/7+2TUiIlJ/63mG7ZX+g9y4t7PUTZBXVu+Sg0aVyXlf+GCpmxJKfR6HIefjofl1MnFBnRx+yGj59hcya+Qx+xgAAO62t5Q+sxdDT/ivPIcp40WZ1SVf+3hO63TKFPI++5j5AnFPR79sa84+lXCy/pGIyNCMCr1WtUu++NHD5WPvPUT+/cYGeaN6j6y59syCb+e462fKuw8eJWuvO8v2+Ugqqub18yo1LtCGh9gQyUwL2uUvVYuIyHlfcD42I1jaRAvbmrttnx8ih0YAAIAhheFjw4hWpNoqFvfWk/7lEyvljWrzNLZn3LUw6+u0IEZkaMaE5K9TquXb9y0WEZEnltZLW4+34Xa5aHep76RnWg2RIRJcoA0P0fgQ+cJh2ItE3GvMccgBAAAoPIJCw0iZQ9HmeML7Rd91Uzd4Wm6/YcpiraOuWDJdChVc2dnWK2PHVUjN7o6CrM9Iy4zq8jjEzkksnpAdeRZiHWqZQhgeBmMEhTA0hGGoIxC0WRub5PjrZ0p/EYZsAiidrU1dEuPGHEKCoNAw4nSXddO+7EPA8nFTxaZkXaHU/7V++xdvmFWQ9c+rbRYRkSk+ZjXzqlAjZ+54u1ZOGz9fdrb15rwO7eOLF+jCp7V7QL54wyzZuCeYei0Er4aHQTokcFHUQEyWbXHEwUhwU8VGae+Nyt7U5B0Ahp/61h45c8JCuf3t2lI3BRARgkLDSsQhU2jWxqZAt/vE0npp743qHfZC15q5f+62gq5PRGRVfZuISMFm+lq+I7m+FkMGlV/a51eoNs3b3CxtPYPy6OLtBVmfFTGh4SGaGl46ymH4KbxbWd827O7uF+J3XqhzghaIdjjVAQAwJLSmrhdWNxwocUuAJIJCw8gmyww+//fNTwSyHVVV5bdPrTI9Fk0k9A57oYMFzV25B1qcrKxPHoQLFYDR6wHl8eaVAgeF9GF9EswVVFhiQv3RuKeC6LAXTQ0fK49wOsjHzrZeuWBipVz16vpSN6WgwvI7FxG9MQSkh66dbb0MAwQAIGS4ChhGOvvNRYx/cfLYQLaTUEXWNLabHvvyTXNkb3sy1Tno/t72lm75w7OrZSCW/x35Qg3VKkTYRR8+VqgxbanVvLJmV95jltt7B6W5y5zKbtex7x6ISX2etZX8uvylajnjroXy5+er5OwJmYXRuwdisutA7sP6iqVU2Q9aoenyLEV+4U6rS7ZxbzDDNUulEMNEvQamswWyCSUMbTW7O+Trt8+TJ5bWl7opAIACaesZlLoW+1lDMXQQFBpGrBd1TlPU5yvmULh6TWMy+8bPXUA/F8Lasle+ul6mrd+XV8qlNpyhcJlCycblc/1U6OFjxiEbL6/elde6jr9+lnz5pjmW9Wf66cPL5NTx8/Pall8Lt7SIiMib1XuktikzY+jHkyrllNvmFbVNuSjVzXNtSvpsMz9haEkkVBk7rkIeXZTf8NEwJXWEqS3wr2F/Mji/MjV8GxiOGvf36v0SYCQ4/c75cvqdC0rdDOSJoNAQMrrc/eOyDv/Itnyusk1m5qffnkv9jbLUxWtVY3veaeiJQgWFtPWpIrvb++TpZQ2eXtfU2S8dfckML+2aPFaANm3e1ylbmtJR+97Bwtc5sdv16wOYIS6bWNx9f20IqND2cMGF9vCkZUHeOn2z/tizyxtcC8/bHU8LUQ/I7zomzN4id83aEkhbACBI3xw/T37x+IpSNwMhN5zOZgd6o9kXQugRFBpCXv7dya7Pl5UpsvmGc/T/jwkoKOQ05ErrxPsZbtAfTUjj/vTQno17OuXXT6x0nSZbCwrd8XatvFG9x/O27GjvJd8kCWPG00WPLpdrX6+R9t7BrK/7ys1z5LRUZo1imZL+3HsWyY1vbcypPefcvUgeW7zDtn0FE5IzmlPmGrxhFrnCCFvAwu5jvTOvOAAAIABJREFUvfq1GvnWvYscX2MXjy7V1+PeOVszHgv7VzWRUAt2owEiO1p7pK0n+3kUCJOwH6cAwA5BoSHkk0e+0/X5REKVg0aV6f8PavhYtuFNqiqeAiKabS3pIT9/f7la5mxultp9zoWDjbOs3T6jVqoavQ0jqzFksWg1K7QOfELNf4iVSPJO+/5UJ9Zrx0Dr9GqJXlrmy6a9nfKoIbATNkFcBKuqKhv2+Ms2imbJFPKiozeq19YZabTvKTM6FYbicUf2DMQKcsxxkkuwzzZTKI+fl9/XZls+7LOPfeE/M+Wb48M/VLVU/J4zThs/X06/c34wjQGGmfbeQRk7rkIWbWXo2lAQ0tMYRjCCQkOAlhmTrSOsDTv6wkcOE5HgaoTcMm2T6/OqiJxxV2bBXyeVdftl7LgK2dPepwecyl2mxy4zvK/d7X3ygweXmts3fZP84+XqjNdtNcxQpdcUMlyFXPFS5mu8Smf55LwKPdhVrMyNN6r3yPkPLc2+oIMgmvnE0no5797FUlm3v/Ard3Hc9TPl8hdz//z9UFVVVje0FW0Gnu/ev1i+dY9bdkh+7VBVVR5fvEO6B2KOyzyzrEH2tPfltZ2hwuvneu3UGrnipWpZ3RBMfZVcAijWlm/c02lbp8vz+rSAY4G6v4X8yeQSgM6meyAmO9tGxve8WBiWgJGmPxqXlhxm3a3ZnRwaPHFBXaGbBB8GYwkZO65Cnq6sd12OhDKEDUGhIUCLgUQceveXnXGMiKQzeJ7+9Vdk+l++Hlh7Xli50/V5VRVp7fZ+QntkUTIbZmV9m56tMRhL2Kbh90fjMndzc8bj+zr65ZNXTZP1uzpk0oLt8uKqzDvwdrVnClZo2vh3jtc/2udrrSm0tK41x1al2TXp0uerZHXDgZyDEwlVlf5oXB6aXyfj364tSJBDq/+zs4izhWntzncoolcvr94l5z9UKdPW79Mfm7Z+r0xbvzeQ7a3b1eE6I1a+QaF5tc1y/Vsb5aYK+6GObT2Dcs3rNcO+xoLfwEdzZ/IY2TMQl+um1sgrBc4aSuQQkLF+F7517yL5/gNLcm6DFnwvdFZhIYJDzyxvlPPuXSyLt+Z/fIU3hQoOAsPZzx9dLifdNNv36wo+gy1y0pWaCXrC7Mwh0ECYERQaAuyGIxx56Bj97+OOereIpIMJh71jlHzmg+8qTuNs5BMc0E5m33tgidwyPTMjyekOyH/fMkfiCVWeXlafdd3ZHhMR15pGbowXP373guJwQn+9andObTG6d+42xyF92uai8YSvbA5VRP79xga5bcZmuX/eNlMx62JlwRSCdX8v3dbqWow3X1qA5t45W/XOwx+eXSMVAQWFssn3k9I+d61gupUWaDgwxGuDTFxQJ2PHVThmRPkNfBgP609VNsjleWQq2slt+Fh+23y9ardc+eq6gq3PqpDr25jKEmpo6yncSgEgT7nOrFuIjPXhIBZPyOtVu4dUP7RQRuJ7RuEQFBoCjjr8HRmPrbz6DP3vT3/gUBER+eEJH85Y7vhUwEhEZMN/zg6gdZnyOSQZs2SmWDKSFBHp7nceoiJiHlp2w1sbJWaoExM1rFs7bjoFhY69ZroMxMwzdr1ZvUfGjquwzYKKGKak19btdHB+dc0u+f0zqx3XEcRdnraeQblu6gbb57TsrKteXS9fvXWu9LgMAzJS1WQhUI3xItfveWllfZt0WIYJTF+/V5ZvD34YmXV3/+zR5a7FeDVXv7Ze3swhu6h3IPm9qm3qkitfXZ/xfP3+Xrn0+SrPn0O+tO9pZ19Ujr9+ZsGHMw2XPsozqRkFswW3vNYUCppaghJZl01ZK8+vcM8kzUchM47eWJv87ZK9UjxhK8YelH9NrZHXqgpToxAwOtCTrBs0c8O+jOe07u9I/95MWrhdLpuytmjZ3/kq5BlopAcEkR+CQkPAc7/9b7n/ZyeYikiLiPzghA/L6LKIfPCwd0j9refJ922CQu9IveaSr46VUTaFp2sCCBTlekK69vUaU4ZOZ39MdvkcRmQMCj22eIcs2pYeGhC3KSTsdjfdmi303PJGERHZYlMEu3pXu4iYL4CdDs5/e7Faptc4n9DX7+4I5G6707T0WhBq9qYmEUkO0XNi/GytHXzzezc/t3hrq2MAJRpPyAUTK+XiyebhRb9/do38+OFljm3xy+l7mevwqWeXN8qfn68SkeR3d+y4Ck+v6xlMB3vs6gZs2tspb1TvyQiKBkX7niZUkfbeqNw/d5uv13NRnZtC/K4HYwm5e/YW29+s9Xvt5bic7bdw5l0LfLWv0PXRCrm6HofjIYI33I8ZT1Y2yF+n5J/5xwUerLQab3aTkGh1REd4TEiaOvtFpPTZyV6vhQr5cY3UgOCSba2+JjiCPYJCQ8D733WQfPsLH8p4fMKPj5ctN53raR1nfvb9MsqmePM7x5Tn3T6rXA9Jnf0xabZcJK+qN6fRZlt3mfUuveEFxiykCbO2yBUvVYvbhFPWbY0Zlfy59FsyiPZ29OkBF2Phar8HZy1T6Ia3NgZyt90pgcGu1pIT050Xy8uMF4DWzuz/PrZcD6BYaUGpjXs6A+3MOK27EBeuT6eySLxwC7oZdRmy4l5evUv2dw/IDx9ckldBdDtBT6Ft/N5NX79XFmwZWjOjbNrbmSoMnv+6EgnVcyZWVeMBufzFatfP57nlDXL37K3y4PzMYbWZQaHs28y2zNbm7uwr8blN0/JZjvBBzD4WksSuEWWkZAxpqhoPSO9gLpmfpd1PzZ39eRdjj8UT8lRl/bCf3TORUGXx1taSXpRrNxaLNVnJUNYfjcuEWVsyRgQMdUF/8oOxhO+b9UHrHojJzx9dLr99alWpmzLkERQa5owd3mINayjkReZlU9b6Wr6hzXywMnY+jUGhWEKVl1fvch2qZX0fB5Uns65+9cQq07CpvR39+t/xRLrj43c3FHvYiba5WMJ7Z63dMMTL+vaM7zeMnRKnNhW7KKPX7Wnf3cb9vXLFS9Xyp+eqZE1je8GnMff79p9d3iCr6tOBDeNv7LHFO2TsuArpM9WX0pZLZn9dPMQKTp97zyI5/6HKgqzriaX1cv5DlTK/tjlrMOLXT66SV9bskjaXu1990eRvd8A2Uyj1h48LBS9fhebO/uwL+VifH+E7quRvS1OX3DVry4i4wzvcM4TsHOgZlB88uFT+8oK/voxI6TOFTh0/X867d3Fe63h+5U65buoGeXRRZmbLcPJkZb3872PLZYZNFnixTE9NXlHq702pGY8yqqrK/XO3yn5L2YeHF26Xe+Zslacrvd/Q86qU+z/ovvc/X1knp9w2z9TH86vQgTitTEitzSgO+ENQaIj5+jFHyDmf+0Cpm+GqlGn582udsxDsLsbdDqDW5Q8alf65vGXImDEW2I0n0hlCfi9+I0XuLzvNduaVqpqDjsYgmp/zknE/FyIuNndzk+3jTm/TR0ysILzuGm0faifQ5i7vF+P+2mPJKMmy/NWv1ciPJmYGSRRR5NnlyQ6Wcfa4bBe7I+FiWKNl2uz2UND90IOSWZydDgW8RQyfnc3vxnps8/Iz99Kh/PLNc7KvKKXQn63X1XX0RodMB/HCSZVy75ytjgXMNbdM2+R5iGpYjbQMIRGRvlTAtma3/4ybUt9ccRpy7od2/Orsdz6ODQcN+5PnPONNwiDObU6r7OqP6kPKwnJOvWV6aY5Zxne/YkebjJ+5RcZZ6jdq/ap8ghuO2w9g///u6dUyoyb7ZCRBf/RzUmUmcp2MZ3VDm3zqmhmyaOvQyhgfKQgKDTFP//orMvGiL3le/g+nHi0iIp/7UHI2MrvhYn867ejCNC4lTNNhancmewdjcsfbtRnPu7XVWvdnTHm6plPEEMHpNAWF0uvTxn57FfEQEWnY3yNjx1XIFp/rFpGM9O2InimUbLPWcrePz1RMWlTTnV9jB9ZP57/QHd9fPbEqo2i1tp2Ovqh8577FUtfSbXq8ULJlyc2rbTYFLv1sOahMskLu/iPfmZwV0VgrKdvhIIjDRSk6xX42aVzW6WV6UMiluL62Hrtjhz7UyvJ/r+3y44F52+SSyZlB8MJ/tqljVZb1/mjiUjn77oWe1ljq3JVoqnOd7fc9aeH2YjSnKEZixpCf31Z9KsAQkmt7uPjarXP1epNWxfz8jJsKugt+oGdQvnXPImnY7z5z46QFyWNW0EPU3Qym+r3W4ZtaqYl4AB+S9na1Y/rGPZ2uQRAvR8MZG/bJ755Zk3W5oL9z2Vafre+1YkeyJMjDC7cXrJSAatnfyB1BoWHulGOOkPpbz5N3HzxaRESm/+XrGctccfan5NLTjyl204pqX4d9loVblsw1r9eY/j+6PP1ziSiKbNrbKc8tbzQNqfITYLAePLNlCl3xUrV88475IiLyyhr/Q4jm17aYAkPaATRmCRa5vQfTxWxGTSH7v7Pxm6nTNxiXseMq9Nmg7HzttrkZj6lq8i7H+t0dpmLKhewUZFvXLyev9LyuXFu1tK5V7p+71fPyhQqKqaLqxfAH48ZhlNqFvP12grgbXoo+qN3b2NfRL2sac5teWJskoNfDLHR2hw7rPvC0m3Pcb3e8XeuQpel9hTW7O2RNQ7vrMunsuYScNn6+aRivkZ/aR2HpR4bl7v5wsLejL2vmVSltbeqSl1Yl6wbubOuVseMqZNr6zCyAUmcKDQdd/VH57VOrAsm0VVVVdrf3yVWvpbNQzMGZ/D4/u2OC0/HKbaKPQptWs1c27u2UiQu8BaqDCLwMxJL9QO135MTpxklZJNmfDyJgZd3/37p3kVz0mPPIgUK2oNTZmF5356KtrQUrJcBxsnAICo0wR73nYNvH/2IJCn3k8HcUozm+KIriPwquJDNkHIcOZVmhceyrcWazhKrKufcskqteW28aPhZLqJ6j1ZV16enWY/GEbG9xv+tirCWT651WY8qnNVNI47ZPjMtalzIXmrZfx8Y9nfK3KWtNGVV2y65pcL6Qbu9L1li5d45z4MPugsDpZLnN4QJy7c523xdqxvfV0RuVseMq9HRb37Rgis+X/eyR5TJ+5hbPywcSQDGsM1vmoPHzjyfUgswgEWS2otN32+7x0++cLz98cKnr+pw6Rtpv3K1DrX0/jYecCbO2yPVvbjR1drsHYvKZ62a4tkOktLOFffu+xVK5fb/rMsbV7WjtkcdtZuAJk10HeuUVjzXAQpRgGzhVVNnS1CVtAc0OdPItc+V79+dXCydIZ05YKH9/OTnD6ObUMEe770kpr3WG+lBFzcurd8msjU3y4LzMYvz5Mv5m7bp9+X58dscExwkzTH2qPDds0D0Qk4p12YctuQnifKzdjL3dZgSARlEU23PavNpm6Y0m+4i5lk9wU+jzqNfA1Tl3L5TJS+oLum0nTv2SUowUCSLoOFIRFIKIZGaphGkIWD4q1u2VY66e7njhn+19fuqaGXqAodywk4xD0foMRV793HX42aPL9b/vnLVFunze2Xxg3raMOyXReEJ+/8xqx9cYT1Z6TaG4eUiG9hb2dvRlZBGt3JEuMGx9r6bhYw7ZP394drW8WrVbGg0FweN68EPV2/CGw/T1IiLlqTs8Az7HNCdU+w7VT1LT3ls7dd9/YIm85bMzZOxgaMMHH7KZGUrn8nWxPhVUQkOhOjDGQKXxd5Vt9cbnb562SY6/flbOd/k37ulMBYH9vadsdxuNnH7idg8Xor6aazF8LW3asO/vmbNVHl+yw5BSLbKvw1zDqLmz33Y2pEIf9Qu9Puu+CKJDX0gXTKyUy1+qzjiOdvRGM+pKeT13WAPVb1TvkclLwh0cs3PWhIVy1oQFjs/nmzlVl+UmS1hoGciDNrNzBTsbp1r0Oj+Lt7bK2HEVpuHbQdP6Ofme5/a098nYcRWydFur/pifGx658PN6U//LkJ3rlE3p1ZWvrpc/PrdGNu3tzHkdQVxPaGc8t12kqulbgdrN2qbOfvnl5JX60LYgAgpeV6mfo7Ms57WNm/d12ZbJ6I/G5cpX10mrpdh2PvzcHAuaNtogLFm/QxlBoRFo5dVnZDxmzW4J6/Shfn/0b61LBhd+5xAo8dIRf61qt4iIlJfZ/1yMHf54QjV1Zpen7nxn6+Aagy1Wdq+cuKBOP/j//eV1snRbq9w5s1bW7erIqIVkZByqlS40bRk+llBlf/eAnHzLXLll+mbTczM2pNf99LIGc6FpH+nLxv1ht6yxE2Edd6wt77fQnXE7Xr9GTp1Xp8/Trmh2rqfIYp1bnd5LfzSZnj12XIXcPdt75pGIyG+eWiU3vLVRRLx8F9J/a7/Xbpc6Ok52tPbIt+5dJLdN3+y7Y3LbjM3ZF0rJpzOUS6fFbr3Nnf3SNxg3BX78tOfLN8+RC2yKhRd6CJPx+PqmS6DXq2/du8hx/Xa8vB8/WZfNXf2+AohNqZnatGa29QzKt+9bJMddP1O+dqt5iKu141+zu0P63WaVS7n0+Sr5z5sbPbcpDLR93trtnCk0XG7+ZjsujC5zvskR5AXWQwvq5Av/nql/R/Px8MI6U7DEyRvVyb6UW3+n0LRjY777cmVqxs3nV6Z//3brNB5z/GyyrWcw43iVUFXp6I3K66k+qIjzOSRus90XV+2U08bPl2VZMjDdaNOP51N0PJDgvb4f/K3bWlg6Hg8uUyjb+cfrd9JLUM1tWxXr9srzK3bKLdO893OycTr3luK4TaZQ4RAUGoGOPDRZDPZ7x3/I9PgdP/qCnP7p94lIOhvD6LGLTwy+cS5m1OzzfYcpW72amj3ZZwS5NlVbqNyh6E/UcFKxHrxvrNgkIuYZKey4HdJeXr1LPzE7+ccr6+S+udtk7U73mhzGAJA+JX08syNyIJWaO6+22XFdD1sKn5rTl+3fkd3QOrtFjUFJ6/AabR/7ndbSmL1k1zq7T7epc8D2ItDpHG0KCmnbyvGEpd3j0l6erU6KqqryWpX/WlPW5mntvnNm+o7T3bO3ypamLlm3K/39uqlio+lzqli/1xTAeyw1tCdbxyfXznpr94B09Udl14FeWVnfpk+Vvm5XR6BDcZw7Q943qopzUcQ7Z9aahm7Yxee/fPMc+ckjy/TviH1NIff2bNiTeee3UPtNv1Od+n9r96D8+fmqAqzX/P9snUFPd6h9BOp+/cQq+fvL63zXJ9Ha8da6PVKz2/6Ou/F7tb97QL5932L5R2qIkZE1iO/mrXV7hmytouFQJ+Kh+XWypz35XbEbvqyqajpTqMhBIW3q9MlL6mVpXfaAjtGJN86WSQvSGbA3T9tsynx2ogUDi/nJKnqmUGHWZ/w9mc/3zsX+s9m0t1O+eMMseWmV+fytqiKXTamSy6as1fu+xlX+981z5O8vVSe3Zfj6aNtduzPZv9Vee+GkSjnpptme2lQIekAukNkkUv+4rFpRFH25PQ4zfgZZaDr7cpkLXjipUr/m0Hg5j7kF3rRnCnIuSK0i15IcQShlIfPhhqDQCLX1pnNlwoXHmx674MSjZNJFX5K/nXms/O3MYzNec4jNzGXFtK+z33XKeTvRLB3om31EzsscgkLGO3zWE4x2MP/qrZmFj/34w7Pusw5o29GyM7ItF0+o0tVvP6baT6qvOVPIGBRyf11VY7t+x+apyvqM9bm9Xmuf3/PAV2+d4zuV+vkVjfL3l9dl1L5w2kd+M4XcigJqu9Pr51FZt1/+OqXa07JGGQWJU/8esMzgdtaEhfLd+5fo/39k0Q6ZXrMv650hbf1Oi+XaiTjxxtly2vj58vXb58kFEyv1314kYr/P2noGHTuGfjh3hpL/aj+JF1dmK4Bpv6L7UkXQtf3i9PlX72x3nXXDPJzAtSnp5Qp0uaZ/5ql/ozlOX5t1O1l+G146/Ad81LXRZtWzfiZtPYNyz+zMGmfaUk7t6BmI6UMMjavUjs3GIKzGT2H+Pz1XJW9vcM4cFUkOP9vg4eZIIfj5qQ+Hfv5tMzY7ZimLmN+jXVBIe/qpyvq867poegZissSQ1TNxQZ387BFzQCfb76q1eyAji9gL/ZyoHRfiiYIOabET0beZ3xfK7hib7fji9TuszSa7yJJtlVBV/YaiXdbgvs5+eSlVi8pUm8+hXSt2tJlmBg2atseCCLxYZ801Mt6o1fbLtuZu6bIZLlmMQtOObBZbsaNNnrZMouJl/xV7dIdjTaFSBIWGwQ2EsCAoNEKNKouYplXXlJdF5NLTj5F3HpQZAHIKioRZIY8VTmm7A4aTtfViwesF/WqXwsoiyewHN9pMRdloJ1JtmI6I6BcNWqcpuYj/HWd8q9k6YJe/VC3/fCV5F/wBDwUgH5pfJ2PHVcjDC+tyPgH0DMblycp6EXGerckpM2rFjjZZ3dAm+1MdWKc2mC+uUndF8/wOen2/1qnLvXaCndbvZbiPlw6VUzvqW3tkVX2bfTFNj9+/1u5Bff9qFzZlEcV2m1+6cVbewVmR5P7q7I9KzW7zbzJh6aT+45XMLA+NlyNpTA9+ugUOU5lCtsPHDMt52J6/Bd394dnVqdVZImUFlvWizEM/2c/FrfWiVnPt6zUywWWI5dkTFsoD87ZlPP5dQzFk43vRPnu7c662nPXcMqPGPmiw64B7IPTS56vkvHvT7ejojcrONvvM1IFYPGs2aqEE1dHf3tItD87P/Cy86h6Iyd9eXCsdvd7q8WgTUdhlksQTano4dOqizjz8KPn3dVM3yB+fyz4dtRd/e3Gt/PzR5a7Zy9bfVSEyDB6aXyerUv0c7bhwzNXT5cQbZ9sGPLyaX9ss11uGT0bjCbluao00dfbrw+SfX+F92KfRnvY+eXhhXUb2o0j281/+NYXSf2vfH8fhYz7q+BVbEDWFtHXafTefX9Go/2182m4InF0tr3x573t5W5/dELe+wbhc+3qNHujyW04hX44Z0yWoPKJ9F4beFWr4EBSCrf/68GEZj/kJCn3mg+8qZHNydtg7RhVsXU4HQVOmUMJ8OVusqPkYj0Gh++Ymi1NPMkwlmjEMTE0XfFZEZH2WgJTGPKNY9uX9XFxoNV9unrY5rw7GKIe6UBqnKeN/98xqOf+hSrlgUrIOy/HXz7Rd7hLD6/OuKZT61+399kfjss+hc59QRQ9iuW7HoS6Cl0LeipK9To7W/HbLRdSp4+fLjyZW2m6/emf6O1exbm/WrBujskjEtM+2NSfvwvr5KfYOxuSSySukcX/mxfG59yySc+9eJN++b7H5As5lfS+saDTVhVDFefiYRhsm5CUt3G3ogiL2M7C4rc+rmRv2yRdvmJXx+NsbmuQXj6+QjTZD1Aop27EgqOOvttY7Z9bKiyt32hbtFkl3Une398kdb9dmZBwaiyEbzy/aZ283jDseTxaP/eRV00zB/d89s8Z2GJDTsGcnZ929QL5++zzb5/79xgb5/gNLHING2fipqRVUUOinjyyT22fU5lxk+enKBnl1zW55cEE6sLS64YCMHVchG/d0ZvQT9ItXm19XQlUzLm5jWc6j/dG49ORQiP/ix1fI5CU7ZEtTcihRr8s6rL8rU73APM6/2oQfqiqmbKV8LswvmbxSHrcUWl+8rVWeqmyQq19bbzoyxuIJ02yxdqLxhIwdVyHPpLI1fvXESrl52mbb2kvZdkW+F8j2NYuyL1vMzInmrn7HGVa1c1wQQaGETZDOjvH5Hz64VK5+fb3peS/Z4/WtPRkZpZMW1MmtDjcUvL5drze/7M5jTy+rl6eXNegTmQQR3HITqkLTDtucUbPPc/AeSQSFYOuo9xws/5OqL6QZZemgvvtg54DLtz7/Af3vDx12UGEb50O2DoAfTgd6Y22bjM5UkXLgDxrl7aes3UHZaDOThHZSSajmGRvsUm41zZ3poIPxAnlPR58+VOfCSemCtsYT8GAsYTpgR+OqvOxh+uZfPWEfuPHCeoGkBQy82p66iOuPegiYpP5VVVV+8+RKuW+OzfASVeTVNfbvWR8+5nKS/dUTK+W/b5lj+1wskZCZG+07a0Z2X9FCdjCyz9CS+ZhxyMUfn1vjmnVjVR5RTOu8+HH/35c5m5plfm2L3Pa2fadPmznKuJ/SQZhM415dL5dNWSvPLW+0edaeVuvLegyxK2SqKMlhUMa7hcYZOax1w5z4HUZ087RNjtOKL9zSotdUC+o46LVe1cr6Nhk7riLvLBfts9Xez31zt8k/XlmXNcCnudtmiJkmnlBlybZWSSRU/fNyyhTanDp+W7P5rIFXEZGyLIFwq6ZO50CyVg/pQG8wU8kb+f3KVDUekNeqdmW9S987kDxfF/LaRcu2XbS1xddFUSyhpjMMUy8z/lbtVnXO3Qvlc/9623cbF2xpMRUkd/vOZgaF0v8vRKBVFZF7DedDr8GTebXNsmRbq1Q1HpB/Ta1x/Ky1x+MJVc8UEhG54qVqOe4/9jd0NNokB+NTNfW0jKqI3fAxm+HiRm7fhYFYXH7ycKXrTTe746ZTIMF8U07Vl85Xto/7Z48sl18/uSpjhkWnthVK3PK7cWL8juxu75Ml28xFt7s8TGpx6vj5cqZlpsRbpm+WiQvss9y9HgM8ZwrZLKjtbu33uMCmtIbX4P225i7f2YBOn2m29x7EDGF2X7097X3yu2dWy5+eT2ZYqqpK7SEPCArB0ec+ZM72+fQHD5V/nvNpufWH/yUiIh8/4hDH1xrvdv30yx8NpoFFFIsn5H6b9H8Rc6aQ9YC4vbUn57uqfuzOMkTACy3QEU+opgP+qHLnw4Sx+LGxs/jDB5fqQ3VWOMw0sq+zX45zyLhxU2/I3vjBg0tclsykZwqlTkxtPf6Dhl4zp5bWaTPPicze1Cx3zrIfXvK3F+3rAKmqKns7+uSHDy7Nug2RzJOt29CZeZvTw+TszuFRHzNyZK8p5L6A01S3uZ7AI4ri6a6pqqoyeckO6ba5Y+51y4OxhNwze6tsbeoy7YdCDLdwqim0eV86kFmdqjmjiMgJN8ySPxmGmJgu5Dzuy189scpXG+ttMqnsBDUOizrEAAAgAElEQVR1fLZgl/Ydmp8aFrpkW6vE4om8Px9r/YZCjKyeunaP/PzR5fLCyp16sLG8LHPFsURCDxZ5DfY58bsf1qeGTFo/ztUNbZ7WpS3i5aLJbZn+aDyjVtIPHlwqf51S7fm7nm05VVXlwfnbfM3OpYq/YFY8oernTe39Gusg2u0Dr7+5fFgDP8Zs1IJc4FvW77V4+i8nr5SfP7pcLphYKU9WNjiep4yrN54XX1+bDKK6vQfrM1oWoN1nke0847anNu3tkmXb2+Saqeaiwr95Mn0Mth1a7ZgplH2ZIGxPFbG2O8brNYUCzBSy7nfrsTnbpjfs6dQzLFVV1es7WbnNlJjRNo9BzmzHwalrd0vPQMx2/1mDg3+3mZTgJw8vk5auAbniJedakyt2tMkZdy2UZ3zcsBJxbrv12GE9flpfdvuMzTIYS8j09XvlP29u8NUGfZva8DHDj127LtOuv+6Zs1U+cdU016Gqa3e2m/rGIxFBITj68/8cI9d/73P6SXVUWUR+f+on5cITj5JrzvuMPP3rr8grvz9Zll91esZrs91BGWpaXIbhDBiyRm6s2JRx58EpFb+QmgtYPFBVzWN0vQ4/sE71KZJOFw9KVaO/O/+jLBdZuVwcfsdQB8TNHW8n7zTmWrw3nlBl5obsmT4iYjs73b/f2ODYOfylIdvKenJv743KugLWDcm2i3/uMGvN3TaZVUu2tcrYcRV6po6dsoj5Pdnd4RVJ3zm/8a2Nkuto9LaeQZkwe4v8/NHlvoMw2baoDfPQOln7OvrlpoqNcu496WnZF201DxcyZoYZ97ufGau8iMUTstWh82zHaX88t7xRxo6ryHkoj3G1dS3dphmRjNvVhtclEqocffV0ueGtTTltT+t0Zl4A5X+S27g3GXBp7R7Qzyl2mUKJRDpQH83IIstc77Wv18gsh4xBt4xA6wxrezvSvznjd33mhn1y/kOV8twK7xcVXn4fxqCwtaN+y7RN8n9Pr5ZV9Zk3HLxmsnT3x1wzx2qbuuT2GbX6rHkDsbjrcUdvt49zSsJwA0Z7uzGbIrnZ/ObJlabjQr6sAfmrDbMhWb/7uQTvra/w+31ItyXh+ryI/fE/Gk/IjJp9stQwhK2la0CWbmuVm6dtsiyrmv41Nt7Y7slL6kXEnIWdy1Ca2YahWMYhdk5BELttWReprNtv+v36kd597u/FrdBxoW8KHOgZtP08RCR1Pk/z0sfT6hE+s7xRzpqwUMaOq5C5m731vezkU2haU9V4QP7ywlr51xsbcg6qtXYP6KUXRERerdptyuha3XBAz+Q39vmeW94oaxrt65xqLXFqkvGtr93ZLl+5eY7rCIAH59fJ62t3y++fXaP/hvzysr+14aBumWHff2CJqW88EhEUgqPR5RH5xcljZe21Z8mKq9OBn0hEkd98/RPyzjHl8qWPvcdUt6f2xnOk5j9nZx0XX8ohZbk4+RbnArVeOophZxx28nRlg36xoCje7/LYZQSdcdcCmyWL40s3zMqYsUW/y1rAPkq2Tkeud+3iqurYmfr3GxtM05afctu8jFlcpqzaKbM2us881B+NZ0wVvnZnu6fphUWy18Vp6uyX9r7MO2xOqebGoSvT15s/u7HjKvQLT7fC7GURxVNQWrtjZDf8yeslvvbaWEI1HecK2QnWLnb+9NwaeWTRDttl7C7ujRcRT1U2ZDyfj3PvWSRdPuqaOO0PrRaIU12sbIydwZ88vExumb7ZVG/FGiDQ9tMTS+33o1fWDJ1CZApps/0d9o5R+pBku4B8XFX1odxuQzaMLnuhyvZxt+KkX75pjukY023oTBuPedtTQ4LrPdTmSF9QeMkUSv/9yydWmmZNakjd/bUbHu4U/9QCytr39rIpVfL9B5Y41l3TPmPtIuKyF9bK126dK3GbDRg/JT+BgLixplBq75g+U4+rmr2pWc+4fGX1LttaaFZugWLr79U0mYblu+8WhBuMJWyDBW9V7zWdF63BTdt1GdajLR2NpV8XS6jS0jUgXf1R0zF//pbMYTXReEJ+98xq03nupJtmy88eXW66gDUOb7fLyrPrG726Jl077oF52+TEG+2ngNcnCbB9Nul2w/Dl9PHcfln74WNJb63bK2fdtdBlS/b6BuP6jbdsH5F1/3T2Rz1NluBFV39ULn+xWjp6o9LWMygn3DBLxus33ZLDny6cWCk9AzFZtj3dD1UUf8Md1xtmezR+jn4Z369TvSXrcomEajqutqeObc1dAwXNtDr66ulSlQr4PF1Zb7vMVa+t1zPUY/GEbTZ1U2e/fq41zupqfE+1+5LHpOXbzUP2rPwUyZ6+fm9GDT/bTCrL566NEsinqP1IQFAIWR128Ch536HOQZwxhuFFY8rL5J1jyqXDcCGoqiI/POHD+v+vOOtYue9nX3Td5rmf/4C89edT8mh18QyHoNCx10zX/56yaqd+IlNEkVkuJzWjXKaoDdL+nkHXGVsSCVXeKsA0v6+vde88bHAptrvKJbgxeUm9LN6a2aEVEXliaX3GY1e/VpPx2DzDOHPrSbK5sz/rzEReuHVXvnLzHLnosRUZj/9lytqs63VLR3cLxJVFIqYLDqdMIY3dmrTH3IqxiqRrq4wui5jalHV6WB8d1SV1+2Xq2t22HTONbc2B1Db6owmZujb7THJ+bC1ABuDlL1abCs/mwvgd0YIWpkwLQ10lEXEsnr6yPj386fo3N8rPHllmu5y2HusFdSGyYbVMy4NGRfR22tYUiquGAIK3Hec0SYSXYvIr69syinUav95aECNbEX8R+2LKTqwXkgtsLu7tOF0EWjMS16QudvssFwl3zqyVyZbCxSIi02uSAfZsFzBOF3B2zTJlCqVWax6O7rop2/Vd/lJ1xrBqu+OlW128zDpm6b+jlu++2wXrsddMt62jtaK+TTbvS58X7WZXsrK7mDO2JZZIyEk3zZbTxqdvRNXu67KdRdPL76a9N2oa3m4XDMwW7Ji8pD7jZo3Gy0dr3GT6e5I9UyihqvLc8kbTjGtuQfxtzd36EFsjY9u1fdbU2a//Bp5f0ah/R698db38cnL6XD/bkJ1o3d9tPYOSSKgSjSc81bF7cmm9vLJmlzy2eLu09STbNCeVPaiqqtw6vVZW1LeZMquSz+U+dC2fOJZxk79+0nlItrZce19UPnHVNLny1XQRbG0fjy5Tcr7JpIp90PHR1M0lY9/I6Rx2+UvV8nmbGmYXPbZCLpxUKet2tZtmddWaet+crfLPV9br7ci2Hc3MDc43Mjft7ZTfP7smo69r/R32R+P6UGeNdn6yHu+zeX5Foxx79XTX/tdwQlAIebPLFvj9N4/W/zb+YD/wroPkj6cdLV/62OGu63zof78k7zt0TOEaCV/2tifv3Nc2dZlmKhsudh7olRdX7ZSnl+WfQXH/3NynN85mnk3xwFxZOzk/mlgp+aZM7W3vy2n4gDWDy07WwIqDN6v3mDqCjW29pqGNA7G4rN/VIffOsf/cGvf3yqWp7Kls+39/qs7AYDyhv0aksDUU3qzeI395wT2IZtdp9FbjxXmZIOpAGLf7iqHAutNnPXZchSzemjmjlmbhlhY9K0IP/MQNWQ2W9/f44mRn2PrWLphYKTNq9snOtl55fMkOU60uO9b9nS3w6EVUz85U9MCA7exjaroQtTVDzGmoarkhYHPd1Bq58tVk/Qkvd2gvmFgp339wiZw5IZ1pYHzdYKot5Q5BoSkrG+WCicm7ztrH4Wm4kGrdx+m/3fa23++t9Sdw39xt5oLMluX1oI3NZlTVXyAnmWGYyhRK/WucuCKhqlKZ5bto17b9luxHPxeVF06slB8/bA6Kug2NzfU4YbzIsgaaNLsO9Orv/7YZtfrj2mdiPG5o77G1e0D/TDsdhopYjzdeMu609Vtnm/XCPpMh+a/XQ0d6mGHqhYZVjh1XoU+CkVwmme3hZFtzlyngc8ZdC0wzp9rRZsr7ys1zTL8PzYwN+xzPl8bvT0vXgHzxhllyz5ytctv0zXLevYtle0u3PFVZL+NeWSc3VWSuWzvGRBwyJ+fZBLRERJ5d3iArbYaYOjHO4uk25X02bnUM7f6vZdq8YJhlVfuOji6PFHxGr4NSsxVbr916BmKm/f+vqTX6DSW77/CGPZ1Su888jFybmMaphqYdYzP+39OrHX+P2j6x1n3S9o+WuX3KbfMy+kyNqezSnoGY9A7GbMtdaA70DOolGf71xgYZjCdsA2PDEUEhFMwpRx+h//3R9x4sky85SUREvvCRw/Tz19/P/lTGgaj+1vNs1/fug0cH0k5k92pV7qmzQ8HK+gNS66MmihvjtNJhZr2AbGzr9X0n2uqW6Zszao8USn80nhE80pp73VT3goTWTtSp4+fpf0+cv13On7hUr/1g7W994455YseuY6ZdfLX1DJouQLwU6vYbR3AbqmfXwensy35nyy27L1uadT79VOuQvWdcgrOzs2QqfuOOeTJ2XIX0pvaBscabNWDpdnE8eWm9qf7bla+uk7HjKkxZMnqmkOXztX40Y8dVyDd81pLTOryxuKoP17G9CEokZMLsLam/vX0IxqDVU5UNegaB17um1mmbBw2BNz1TyCEb6Z+vrJeV9cmMyGx1UYyscQLjPtZ+C08va5C6FnPWWrvPmdGc6io57dts2VVuF5GqqpqG2e1o7dGzrrTNGTN4VBHbDA4jY8FtY0DJyEtmjNbuFfVtGZ+3Mbhq/A0NxhKmmdDOuXuh5ymgjbvXaV+fcts8+Wkqa+/FVekLZq0Nxvdl9x6dPgvjcLULJ1Z6Gi6tbXP2pib9O3/A43t9cN42UVVVFm1t0S82vfx2jVnosVRWmTbDoDVbcYbhe5AtkHHGXQvlxBtny7/f2KAPw7LbtvF4GE+oOQ+9+fZ9i+W6qTUSiyf0AMjbG/bpdWtmbmyS66ZukBdW7pRHFu2Q71rqNq7YkQwMjiqLZPRZ+qMJwxBMsy1N3fLMMn8FlDXaMcprQDUaT+i/G6f9v2FPp/zmyZV6UF1btd3i2jk9kUhOra5Z3ZAMchmHTjpRxL6foQ1Lth6yH1643TRE/UnDsHOnz97aCruh4D0DMWlOFZy2e6/Wxy59oUo/ltW39ugZxdrnbD1vWQ/hxoBnQhXTcLPewbh84d8z5YQbnCe5+cYd8+SU25Lf/WFQEtcXgkIoiNobz5Enf/Vl02Onffp9suzK0+X0z7xfvvzx94iIyNHve6dpGWvhX6PR5RGZ+ddvyDvHlMuJhsyir37yvRnLGmseAV7kWtRuqLp9Rmbn7x6XKbK9KsQQPDut3ZnD/6KpzpRdLRGjKYYLCBFzraIJs7eYMh1mb2qSzizrE7EvUNhqU+BdUbIXdr526gaZ73BX1Sn241bYudcmKOSlYOJTlfWOzy3MMlynJY/i9l+y1Nkw3iG1GlMe8XW31nghs2lvpxx//Uy5z0M2n7UmmhY42d3eJzW7O2TsuArZ2Za8oIklzDOYzd6UeeHe6HPWSe3i47qpNfp3rTyiyOOLd8jrhiD99pYeWZeaAdGa8eC0m7RO8jWvp7MHnlnWIGsthfq91nYwBi60dt85a4s8snC7qKoqFz++Qp5cWm9a3zfvmKcPJXIKUtTs7pA7U1OBZ2YKpX8Yc1NDR+bXtsjpd5rr1n3zjvme3oPG6T0b960xuKgHkVLN6R6I6UPLbpux2Xa4kuaxxTvk1PHp9v3xuTUZs48ZL77cgmc9AzEZjCVMQ92MAau/v1StF8t1ChYZ7evsd8z6rGpsF1VVpXpnu6kgrTW4u3lflxx3/UzZ2dbrOTgkkn0IvqqqtkEUY3FzY3aBdtHoFEg31pFaUd/mOEOqkbGu0t6Ofnl59S45/yHnmUGN7py1RRr298pFj62Qb6UKgmvfrz3tfa7DuLXZJg/0DMq4V9bJ5amZpKz7w1hLzetMWU8srXecVXeJJUPz+RWNeQ1DfqqyQX7w4FL53gOZM8ZaAw7rLDO8anWCyiKKr/ozfhm/Ltru9ZqxfOGkSjlt/Hxp6RpwvNn21ylrZfamZj3TRTuH2G2jMlWHZ8aGfXKXIevmmteTN8S0GwNuksPHMn8DHzviYKnZ3WGaybSqsd11wgenoNA/LLOeXTJ5Zcb5enrNPvnyzXMc123tz01bv09+kSo7cOr4+XptUu13PWAZ+mr8LVi33djWK5+9Lh247hmISSyhug6fNfb1jN+JQswqG3blQa5cUZRzROQeESkTkUdVVb01yO2hdMaUl9k+/oFUQemfnHSUnPqpI+WDh71Df67q2jOlzBIUuvOC4+Tsz39A//+x7z9Uav5ztgzGEjJt/V753vEfEkVR5F9Ta+TJygYZVabIkn/+j6nm0S0//C+5/s2N+gHku8d9SN5w6aiNVJ848hBTyjGGt8dt6mRUrM8/oGPtwAXJbbYkI79DHr2s125M+aOLM/fphj2dWQMqTn72yDLHC3u3O5bGoVh+9EcTsnz7fvnchw+TlZYLo98/61yPS0Tkd8+szmmbfk1auF22t/b4zgARSaai52trc1fGLIexuPki1elCxU8mnhbwjCVUuT41g87czc16AERj/B1vtqTub9jTmVE4XnPGXQtMs0FOmLVFz3RTFJFHF203ZWO4MU4zbLyguWnaJrkpNXvTgi0t8qSh9lmDofixsdP9vQeWyORLTpItTV3y26dWSVd/TH5zyicygrD3zN4q3zv+w2LHWBDbjtsQV6fPTjsmbNzbKTMNBfv1C5LUKq94sdoUAHQLcC621Dxp743KxlTNueauAZlRs8+UeamqznNXfs5mOIPxuPfS6l3y0updUnvjOXL89bP0x61ZQJoz7lwgv/n6Jxzb3tkXy7igd5rRyu+Mq7+cvNKUMf6bJ1eZbiA6HZ8fmp+ebXCT4bfwp+eSvwGnbJyfP+JtIgUjY8CtYX+vTM1SS9BKu+DWMkq199TUOaDPVOrGejzWZqfU9BSg5kk0npC7Z2+RY99/aEZQe3rNPj346YVdPM5a50Xjdq29rTn9ud4aYN1K6zHCOiRJJNkH2NHSI//1kcP0x1q6BuS1ql36OaJy+3492G79+mkBYO391qTqKdmd250yfTftNdeotAv6aJwKve9s65Nv32fOxtra3C0npW7e2+lPff+9BEbcgpJ23wu77//yHW0ZQ/+07ClrgMp4Ey5bX26O4QZOe++g3DVri8QSqu2NRi0rS9M1EJN3HTQqY7nhJLCgkKIoZSLygIicKSK7RGSloihvqKqaOWAUw56iKKaAkIjI4Yekh4dd9+3PyvbWbjn/Sx+xff3o8oh831Cs+l/f+Zz845xPyyFj0l/h35/6SXlofp385KSj5Kdf/qjeUbz1/P+S0eURqazbL2OPOFiWbMsco3/dtz8r63d3yGuWYVMnjT1cT323Pva+Q8foU8E/dvGJrgXljC756lg5+ZPvlf/zcMHy9WOOyJh22qtzP/8B15P4wxedWJTZwca+9+D0rF9AHpyya8JGK7DoV7ZaNkH4xyvrZH/3YKgLKTpNqV4MdvWcZm1ssp0ZLB97PM7AZpxdx2rigjrH57ZZioMb685EFEVurNhkfYmjVfUH5Csff6+8/11jXDMutzsEIIzTdlfvbJcv3jDL9Pw3x8/Th8kY17WtuUsO9zGsfOy4ChldFpEXf3ey4zIDsYTE4gnp6IuaZlI1Bot+90z6gtw4o2NHbzSjALbbxAKHjM7schs/M2ug9VdPrJJ3jErfcMtW18Iu8G83u6KdnsG43DPHOXP0Bw9lZnj84EFvmTJe9EfjMqY8Ivt7BmX2pibHqdmdXPx45oQGThlSfmZO1BiHrNTv7/FdR8wYFO0bjJsyjzTWC36v7RGRjH5qLjr7ovLAPOdjiBf3z90q42d6rycjIrbfOzU1++oZPmZNSyTUnMsCrKhvMwUs5m5ullunbzYdN7Xf35//J10z9aSbzFmvxtqC1kCDdhN2IBaXrU1drvvabdj0efcucnknZnalIOZtth+Sur3FeQKJKSsa5bIzjpUel1o8mpNvsc8KuvzFavnk+w7J+nrNn59L78uZG/bpE7No333tmGEsM+Ani9wYLLdz/kOVpv//6KGlMvOv3/TW+CFKCSodSlGUk0Xk36qqnp36/5UiIqqq3uL0mhNPPFFdtcrbhTWQzdqd7VIeUeTzH05H9XsGYqY7bEe/753y3eM+JJeefozE4gn564vV8mb1Hnnrz6cki4t96DCZXrNXVtUfkJkb90lT54B+R6tvMC6fuW6GiIgsu/J0uX3GZv0A/NjFJ8ro8ohp5qXjj3q3rN3ZLvW3niexeEKOvnq6nPnZ98uvT/m4PLJwu8zZ3CwTfnycfPQ9B8vrVXvkCx85TC448Sj51RMrM+4WX3r6MXLmZ94va3cekGtT9VWsAaTJl5wku9v75JrXa+TjRxwio8oU/e7S3848Vi49/RhZuq1VH0//neM+JDvbemXtTvNdcSe//frH5ZFFO+SXXxurXxic9qkjTcUGjz/q3fL6H78mIiKTFtR5nqHsjM+8T0746OGe7qANd2d85v1Z66oMZ+8+eFTGBSIAFMIR7xztebgNICLy2Q++6/+3d/dRctX1Hcc/33vnaTebpyVLBJaYIAEMIkGB8KTQaANWjg9UC4WjtNJiq6i1eizY03paPVaPWqSe2nM4QKVWoR4skioVH9BKfSApEpCAgQjyTIjkabNPM3Pvt3/c39zMbnZNILsZNvf9OmfPzP3NvTO/+5v7vTP7nfv7/cYkNjH1Vi1bqKe3j0x6dRF2ueSMJbp2gquWD1SRSb2zqpPO7DddLjljif7m3GX79TWni5nd5e4n7lY+jUmht0k6x93/JCy/Q9IKd79s3HqXSrpUkhYtWvTqRx/d99mAgD1xd922/hmtWvaSMYN5jjQS3fnIFp15VN9u27QuL21fv95M9cTWIR3R17Pb+lL2K2jf7KoOndelkUaioXqi3nCF1FPbhtU7q5LPBDCZJAwwWClNPATY9qGGPnnrA/roG1+uainSf9/3tF7/8oWaHS5zvO/J7Try4B7VyrE+/70HtWnHqP7hvOPy7bcO1vXA0zt0WhgofOOzA7rq+xu14ZkdenDTTl3xhmO0Y6ShU49YoK5KpFe/dNclpk9sHdLBs2u6+7Gt+vi37teNl56qtY9s0ZXfe1CP/GZQ17zzRK04YtcYULf/cpPe/eW7dNGKl+pDq47SVd97SI9tGVL//G4dtbBHUWQ6aXGv+ud3KTbT1Xc8rF//ZlBbBusaGGlq8YJu3bDmca1Y0qtrLj5Rn/vOg/rlMzv0mqV9OmlxrzZsGtDbX92vi69bo0opypNkX/3TFVp2yBxddM2det/KpfrfjZvHDED4yv652jnSVO+sij589tG6IMy+Uo5NjcR13qsOU19PVQvn1HThikW6+kcPa6SR6JkdI/rc24/X41uG9YXbH9JPH35OWwfr+a8p5594uM48uk+bB0b1sdVZ8q5WjvThVUfrE996QK9ZukD987u18dkBrf31Vr3r9CV6Zf9cnbBonn61eafOPOpgxZHps7dt0FfXPKY5tZL++o3LNFRv6vDebp33xZ8osl2XKb9v5ZH6wu0bdeX5x+uMI/v0jbufzLtztHz694/T396yXqPNVH2zq/nYMKsvO12fuW2DPn/+cq17fJvueXybbl73pF7ZP2/MLzEnLJqnZuL65FuP04ObBnTHQ5v1jXVP6Yi+WVowq6o14y75PWphT56MvHDFIp28uFf//rNH8199JnP2sQv1nrOO1LGHztHvfO6H+Xgu47Xvw2SWHTJHC2ZXx3TrOuWIXm0dbOS/LPbP78rHdrhwxSL91z1PTTiOUMublx+qW9Y9pX/9o5Py8XtOe9lB+dU+l772CM3tKuuGNY+pJ1zROL67z952bX3dMQfr1JcdpJvvflJm0n1P7v4PyftXHqkb1z6uwdGmBuuJjj10jlYte8mYsQf653fpTccfqp8+/NxuXaKej5XHHLxborrdWUf37dNVXbMqcR5D5djUP7970m4ve8Ns4i4KN7/ntEmvdjjusLkv6B+S6UxiHjK3pqfDFUUL51THjJU13uG9XZPGzFSbUytNOtNTy8lLevdq7JaJvOKwORMe8weySima1vFTZqLuSjzh+GkAZp4FPfs/sbMvfnrFyt16vMxUL9qkUDuuFAIwldz9t87ahBeHqXyfmkk66fTYaeoyy2YHmyjJmqSueJJuQc0kVRyZzOwF17eZpIrMlLqPqWMjSfPuSGb2W/fh+Wp9xr/Q9vW2cRDaE+JJ6jKNLWtvl/Ht1apDa5yPeNx22XTenm8zvg7u2TAucWRKU99tZq4kdfkE27u76kk6Zty79u1bdW4ff2SyY6B9n5tJmm9XKUW7/Wjg7mokrnJsaqZZ/Vv3y3t4b9PU8+nK48hUirLXGX9MJKmrmaYqRVFe5/b3oL3d28vTNBurJnVXbBbaJ9qt3dylkWaiainO27aeZLP9RGZjftBw96wrVuqqlaLsdtwPHq12bx0brRmVIrM8cVeOLT/WhupNNRLPu3UNjjZVik2VOBqzL61xJKohppM029fIds3alLVDNt5FT7Wk7kqs2Ezbhhsqx6aeakmNJNvHnaNNxVH2OqXINFhvalalpCgyjTQSjTbTfJKMainWYL2pchSpkaaaHRLA9SRVJbxfZtl2cWSKzNRIUtXKsbYPN9RTLaneTMN5Kc3brHWMjDYTlaNIURhkN/WsXVvjatSTVF3lWKPNVN1h21acjDYTJakr9SzROtxIVCvF+TmwHJtGGqm6KrGS1LVjuKFyKVJ3OVY9nKvi8D70VEsaaaSqlbP3drSZalYlVupZvUcaiWbXyhoYaWhWtaRmaMtybHlbjDaz81xrDJVKnLXZcD1Rd6WkyJQfCztHm5pdK8vdNVjPuomU4yg/jgfDe5S6y2QqxVnbDtab6irHSkP81cIx0Uw9f70dwxusuXUAAAtnSURBVE0dNKuiZoifchxpqJ6ouxLLlP3IUoqyerReO0ldpchUiqM8Pls/GpVj02gzDfsb5V2f2mNrtJndDow2s9uRpiIzzesqyyzrxliJo7zdU/dQhyx2IzNF4TwQ2a7JBWrlOD/Opazew41EXeU4P/arpSj/zGskad5mlbBfAyMN1cqxdow01NdTVerZ1RhJiK3WMVuOI0Vhn9yz52smnndxbKSpRhphfxupquUsfhJ3DdcTDTcSLZxdkys7ZtrPia34KMfZ8TVUb6oSR6qW4nBuUt6GrTZvHXuRWRj8Pzv2Wz8qVEuRLLz3rXqkqdRVifPvAkP1ROU4O3+2Podd2bhxI41Eibvmd1fysXla7+FIaOPREJO93RV5aP/BeqKeakkDIw2V4+y4bXVlrDezdqw3U5ViU7UUj/nsSTz7HOuulBRHpuF6okop0lC9mT9X6+NpsJ6oEV4/MsuH02idS1rn7ax9wrk2ytpxdrWUfy5l56XsnJC65+MpVUvxrnPVUEOza6W8zWrlWM8NjmpOraxaOdbgaFPdlVg7hpuaVY0VmWXHeKT8uKmUIu0YaeRdL3uqpfx9bp17U8/idMtQXd2V7LiqlWLtrDcVm6m7EmvzwKiq5VhzaiW5S0ONRLVSNjPd4GhT3dVYpixWtg3X1V0uqRRn57LB0aZq5VhxZPkxPziand/MTLMqcX58DdaTvP26yvGEs4HOVJ1ICtF9DAAAAAAAoMMmSwpN55T0ayUtNbMlZlaRdIGk1dP4egAAAAAAANhL0zb7mLs3zewySbcpm5L+OndfP12vBwAAAAAAgL03bUkhSXL3WyXdOp2vAQAAAAAAgOdvOruPAQAAAAAA4EWKpBAAAAAAAEABkRQCAAAAAAAoIJJCAAAAAAAABURSCAAAAAAAoIBICgEAAAAAABQQSSEAAAAAAIACIikEAAAAAABQQCSFAAAAAAAACoikEAAAAAAAQAGRFAIAAAAAACggkkIAAAAAAAAFRFIIAAAAAACggEgKAQAAAAAAFBBJIQAAAAAAgAIiKQQAAAAAAFBAJIUAAAAAAAAKiKQQAAAAAABAAZEUAgAAAAAAKCCSQgAAAAAAAAVEUggAAAAAAKCASAoBAAAAAAAUEEkhAAAAAACAAiIpBAAAAAAAUEAkhQAAAAAAAAqIpBAAAAAAAEABkRQCAAAAAAAoIJJCAAAAAAAABURSCAAAAAAAoIBICgEAAAAAABQQSSEAAAAAAIACIikEAAAAAABQQCSFAAAAAAAACoikEAAAAAAAQAGRFAIAAAAAACggkkIAAAAAAAAFRFIIAAAAAACggEgKAQAAAAAAFBBJIQAAAAAAgAIiKQQAAAAAAFBAJIUAAAAAAAAKiKQQAAAAAABAAZEUAgAAAAAAKCCSQgAAAAAAAAVk7t7pOuTMbLOkRztdjymwQNJvOl0JoOCIQ6CziEGg84hDoLOIQbyYvNTd+8YXvqiSQgcKM/s/dz+x0/UAiow4BDqLGAQ6jzgEOosYxExA9zEAAAAAAIACIikEAAAAAABQQCSFpsfVna4AAOIQ6DBiEOg84hDoLGIQL3qMKQQAAAAAAFBAXCkEAAAAAABQQCSFppiZnWNmG8xso5ld3un6AAcKM7vOzJ41s/vaynrN7Ltm9lC4nR/Kzcz+KcThvWb2qrZtLg7rP2RmF3diX4CZyMwON7MfmNn9ZrbezD4QyolDYD8xs5qZrTGze0Ic/l0oX2Jmd4Z4+w8zq4TyaljeGB5f3PZcV4TyDWZ2dmf2CJiZzCw2s7vN7JthmRjEjEVSaAqZWSzpnyW9QdIySX9oZss6WyvggPElSeeMK7tc0vfdfamk74dlKYvBpeHvUkn/ImX/vEr6mKQVkk6W9LHWP7AA9qgp6UPuvkzSKZLeGz7jiENg/xmVtNLdj5e0XNI5ZnaKpE9LutLdj5S0VdIlYf1LJG0N5VeG9RRi9wJJxyr7bP1i+B4LYO98QNIDbcvEIGYskkJT62RJG939YXevS7pR0ps7XCfggODuP5K0ZVzxmyVdH+5fL+ktbeX/5pmfSZpnZodIOlvSd919i7tvlfRd7Z5oAjABd3/a3X8e7g8o+zJ8mIhDYL8J8bQzLJbDn0taKemmUD4+DlvxeZOk15mZhfIb3X3U3R+RtFHZ91gAe2Bm/ZLeKOmasGwiBjGDkRSaWodJerxt+YlQBmB6LHT3p8P9ZyQtDPcni0ViFJgC4fL3EyTdKeIQ2K9Ct5V1kp5VllT9laRt7t4Mq7THVB5v4fHtkg4ScQjsi89L+oikNCwfJGIQMxhJIQAHBM+mUmQ6RWCamVmPpK9L+gt339H+GHEITD93T9x9uaR+ZVcWHNPhKgGFYWbnSnrW3e/qdF2AqUJSaGo9KenwtuX+UAZgemwK3VEUbp8N5ZPFIjEK7AMzKytLCH3F3f8zFBOHQAe4+zZJP5B0qrLumaXwUHtM5fEWHp8r6TkRh8ALdbqkN5nZr5UNFbJS0lUiBjGDkRSaWmslLQ2jz1eUDR62usN1Ag5kqyW1Zi66WNItbeXvDLMfnSJpe+jecpukVWY2PwxsuyqUAdiDMAbCtZIecPd/bHuIOAT2EzPrM7N54X6XpN9VNr7XDyS9Law2Pg5b8fk2SbeHK/pWS7ogzIy0RNmA8Gv2z14AM5e7X+Hu/e6+WNn/ere7+0UiBjGDlfa8CvaWuzfN7DJlX25jSde5+/oOVws4IJjZDZLOkrTAzJ5QNnvRpyR9zcwukfSopD8Iq98q6feUDdo3JOmPJcndt5jZx5UlcCXp7919/ODVACZ2uqR3SPpFGM9Ekj4q4hDYnw6RdH2YpSiS9DV3/6aZ3S/pRjP7hKS7lSVwFW6/bGYblU3WcIEkuft6M/uapPuVzSz4XndP9vO+AAeSvxIxiBnKskQlAAAAAAAAioTuYwAAAAAAAAVEUggAAAAAAKCASAoBAAAAAAAUEEkhAAAAAACAAiIpBAAAAAAAUEAkhQAAwAHLzHaG28VmduEUP/dHxy3/ZCqfHwAAYLqRFAIAAEWwWNLzSgqZWWkPq4xJCrn7ac+zTgAAAB1FUggAABTBpyS9xszWmdkHzSw2s8+Y2Vozu9fM3i1JZnaWmd1hZqsl3R/KvmFmd5nZejO7NJR9SlJXeL6vhLLWVUkWnvs+M/uFmZ3f9tw/NLObzOyXZvYVM7PW85nZ/aEun93vrQMAAAppT7+AAQAAHAgul/Rhdz9XkkJyZ7u7n2RmVUk/NrPvhHVfJekV7v5IWH6Xu28xsy5Ja83s6+5+uZld5u7LJ3it8yQtl3S8pAVhmx+Fx06QdKykpyT9WNLpZvaApLdKOsbd3czmTfneAwAATIArhQAAQBGtkvROM1sn6U5JB0laGh5b05YQkqT3m9k9kn4m6fC29SZzhqQb3D1x902S/kfSSW3P/YS7p5LWKevWtl3SiKRrzew8SUP7vHcAAAB7gaQQAAAoIpP0PndfHv6WuHvrSqHBfCWzsyS9XtKp7n68pLsl1fbhdUfb7ieSSu7elHSypJsknSvp2/vw/AAAAHuNpBAAACiCAUmz25Zvk/TnZlaWJDM7ysxmTbDdXElb3X3IzI6RdErbY43W9uPcIen8MG5Rn6TXSlozWcXMrEfSXHe/VdIHlXU7AwAAmHaMKQQAAIrgXklJ6Ab2JUlXKeu69fMw2PNmSW+ZYLtvS/qzMO7PBmVdyFqulnSvmf3c3S9qK79Z0qmS7pHkkj7i7s+EpNJEZku6xcxqyq5g+ssXtosAAADPj7l7p+sAAAAAAACA/YzuYwAAAAAAAAVEUggAAAAAAKCASAoBAAAAAAAUEEkhAAAAAACAAiIpBAAAAAAAUEAkhQAAAAAAAAqIpBAAAAAAAEABkRQCAAAAAAAooP8H8538b4NInxUAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 1440x1440 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}